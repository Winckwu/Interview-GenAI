# 19个元需求 - 实证基础与定量评分

> **补充文档**：为02-19-Meta-Requirements.md提供详细的实证基础  
> **数据来源**：49次访谈 → 588编码实例 → 143个用户挫折 → 87种替代策略  
> **目的**：增强论文的实证说服力和方法论严谨性

---

## 📐 定量优先级计算公式

基于4阶段需求推导过程的**多标准决策矩阵**：

```
总分 = 用户影响×0.35 + 可行性×0.25 + 理论基础×0.20 + 障碍移除×0.20

其中：
用户影响 = (受影响用户% × 严重性(1-5)) / 100
可行性 = 6 - 平均实现复杂度(1-5)
理论基础 = (文献深度(1-5) + 框架连贯性(1-5)) / 2
障碍移除 = 二元评分(1=低/3=中/5=高)
```

---

## MR13 - 透明不确定性显示

### 定量评分

| 维度 | 分值 | 计算 |
|------|------|------|
| 用户影响 | 4.90 | (98% × 5) / 100 |
| 可行性 | 2.67 | 6 - 3.33 |
| 理论基础 | 5.00 | (5 + 5) / 2 |
| 障碍移除 | 3.00 | 中等 |
| **总分** | **4.08/5** | - |

### 实证基础

**受影响用户**：48/49 (98%)  
**严重性**：5/5 (极高 - 阻碍专业采纳)

#### 用户挫折 (8个关键案例)

1. **受访者I34 (计算化学博士后研究员, 模式A)** - 文献综述灾难
   ```
   挫折描述：
   "GPT给我生成了一个20页的literature review，
   引用了30篇论文。我花了100分钟逐一验证每个引用，
   发现12篇论文根本不存在，8篇被严重曲解。
   
   最让我生气的是它的语气如此确信：'According to Smith (2021)...'
   我去Google Scholar查，根本没有这篇文章！"
   
   时间成本：100分钟验证
   情绪影响：极度挫折 + 信任崩溃
   ```

2. **受访者I17 (金融经济学研究者, 模式D)** - 法律建议风险
   ```
   挫折描述：
   "我问了一个合同法问题，GPT给了看起来很专业的回答，
   引用了案例法和条文。
   
   我几乎要用在客户案子里，幸好我查证了 - 完全错误！
   引用的案例不适用，理解的法条有偏差。
   
   如果我真用了，可能会导致职业失误。现在我根本不敢信AI的法律建议。"
   
   风险等级：职业生涯级别
   信任后果：永久性怀疑
   ```

3. **受访者I44 (AIGC项目负责人, 模式F)** - 代码安全漏洞
   ```
   挫折描述：
   "GPT给的authentication代码看起来完美，
   运行也没问题。
   
   后来安全审计发现SQL注入漏洞和XSS漏洞。
   如果上生产环境，数据库就被攻破了。
   
   AI没有告诉我'这代码有安全风险'，
   它表现得好像这是最佳实践一样。"
   
   发现时间：代码审查阶段（幸运）
   潜在后果：数据泄露
   ```

4. **受访者I26 (电商创业者, 模式C)** - 医学知识错误
   ```
   挫折描述：
   "我问GPT一个药理学问题，它给了详细解释，
   包括剂量、contraindications、作用机制。
   
   考试时我用了这个知识 - 错了！
   教授说GPT混淆了两种药物，给的剂量是另一种药的。
   
   如果这是真实临床场景，病人可能会出事。"
   
   学习后果：考试失分
   临床风险：极高（假设场景）
   ```

5. **受访者I41 (理论光学博士生, 模式D)** - 数学推导错误
   ```
   挫折描述：
   "我让GPT推导一个量子力学公式，
   它给了10步推导，每步都看起来合理。
   
   我用Mathematica验算 - 第3步就错了！
   后面7步都建立在错误基础上。
   
   最危险的是：它从不说'我不确定'，
   语气永远是'显然...'、'因此...'。"
   
   验证工具：Mathematica
   错误位置：推导中间步骤（隐蔽）
   ```

6. **受访者I8 (计算机科学本科生, 模式A)** - 统计方法误导
   ```
   挫折描述：
   "我问'这个数据集应该用什么统计检验'，
   GPT推荐了t-test。
   
   我用了之后，同事指出：数据明显不满足正态分布假设，
   应该用non-parametric test。
   
   GPT没有提醒我检查假设，结果我的分析全错了。"
   
   专业后果：返工
   信任影响：从此不信AI的统计建议
   ```

7. **受访者I22 (强化学习博士, 模式A)** - 商业数据编造
   ```
   挫折描述：
   "我问'苹果公司2023年Q4营收是多少'，
   GPT给了一个具体数字，还分析了同比增长。
   
   我引用在presentation里，教授问'数据来源？'
   我去查 - 完全编造的！真实数字差了30%。
   
   在全班面前很尴尬。GPT为什么不说'我不知道最新数据'？"
   
   社交后果：当众尴尬
   学习教训：永远验证数字
   ```

8. **受访者I33 (量化交易专家, 模式C)** - 文献曲解
   ```
   挫折描述：
   "GPT总结了一篇著名论文，说'该研究发现X导致Y'。
   
   我去读原文 - 论文明确说的是correlation，不是causation！
   GPT完全曲解了因果关系。
   
   这种错误如果学生没发现，会导致对整个领域的误解。"
   
   学术影响：概念误解
   教学担忧：学生被误导
   ```

#### 替代策略 (用户自发发展的验证工作流)

**策略1: 受访者I3的"三角验证工作流"** (30-45分钟/次)
```
步骤：
1. GPT生成初稿
2. 在测试环境运行代码
3. 遇到错误 → Google错误信息去Stack Overflow
4. 用新信息重新提示GPT
5. 循环3-4次直到通过所有测试

时间成本：每次任务30-45分钟
核心痛点：完全手动，无系统支持
```

**策略2: 受访者I34 (计算化学博士后研究员, 模式A) 的"文献沙盒策略"** (第一次使用20-30分钟设置)
```
步骤：
1. 手动找4-5篇可信论文
2. 提供给GPT明确指示："只总结这些文章，不要引用其他"
3. 测试GPT是否停留在有界上下文
4. 如果可靠，再逐步扩大范围

创新点：限制AI的"编造空间"
局限：仍需大量手工工作
```

**策略3: 受访者I44 (AIGC项目负责人, 模式F) 的"安全审计清单"** (每段代码5-10分钟)
```
GPT给代码后的强制检查：
□ SQL注入测试
□ XSS漏洞扫描
□ 认证逻辑验证
□ 输入验证检查
□ 错误处理完整性

工具：SonarQube, ESLint, 手动审查
时间：每段代码5-10分钟额外工作
```

**策略4: 受访者I17 (金融经济学研究者, 模式D) 的"法律零容忍"** (完全拒绝策略)
```
决策：
"任何涉及法律的问题，我根本不问AI。
风险太高，我宁愿花时间查Westlaw或问同事。"

时间对比：
- 查Westlaw: 30分钟
- 问GPT然后验证: 50分钟（因为要仔细核查）
- 结论：不如不用
```

### 设计需求推导

**从挫折到需求的逻辑链**：

```
用户挫折：AI"假装确定"导致：
1. 时间浪费（100分钟验证）
2. 信任崩溃（永久怀疑）
3. 专业风险（法律失误、医疗错误）
4. 学习障碍（错误概念）

替代策略的局限：
1. 完全手动（无系统支持）
2. 时间密集（30-100分钟）
3. 需要专业知识（知道去哪验证）
4. 或者完全拒绝使用（失去AI价值）

系统解决方案（MR13）：
→ 显式传达AI的置信度
→ 标注不确定性来源
→ 建议验证方法
→ 承认知识边界

预期影响：
- 减少验证时间50%（从100分钟→50分钟，因为明确哪里需要查证）
- 重建信任（透明比完美更重要）
- 启用新用例（律师、医生可以安全使用）
```

---

## MR11 - 集成验证工具

### 定量评分

| 维度 | 分值 | 计算 |
|------|------|------|
| 用户影响 | 3.05 | (61% × 5) / 100 |
| 可行性 | 3.33 | 6 - 2.67 |
| 理论基础 | 4.50 | (4 + 5) / 2 |
| 障碍移除 | 5.00 | 高 |
| **总分** | **3.81/5** | - |

### 实证基础

**受影响用户**：30/49 (61%)  
**严重性**：5/5 (验证是有效AI使用的核心)

#### 用户挫折

1. **受访者I1 (市场营销博士生, 模式A)** - 手动逐段比较
   ```
   挫折描述：
   "GPT帮我重写了一段代码，我想看它改了什么。
   我不得不开两个窗口，手动对照'before'和'after'，
   逐行找差异。这花了我40分钟。"
   
   时间浪费：40分钟
   需求：差异可视化工具（像Git diff）
   ```

2. **受访者I22 (强化学习博士, 模式A)** - 跨模型手动比较
   ```
   挫折描述：
   "我想比较ChatGPT, Claude, Gemini对同一问题的回答。
   
   我的工作流程：
   1. 打开3个浏览器标签
   2. 复制提示到每个标签
   3. 等待所有响应
   4. 来回切换标签阅读
   5. 手动记笔记比较
   
   每次5-7分钟，超级低效。"
   
   时间成本：5-7分钟/次
   频率：每天3-5次
   累计浪费：约25分钟/天
   ```

3. **受访者I41 (理论光学博士生, 模式D)** - 数学验算工具切换
   ```
   挫折描述：
   "GPT给我一个积分结果，我需要：
   1. 复制公式
   2. 打开Wolfram Alpha
   3. 重新格式化（因为语法不同）
   4. 运行验证
   5. 对比结果
   
   这个流程断裂的，我经常忘记验证就直接用了。"
   
   摩擦点：格式转换 + 窗口切换
   风险：摩擦导致验证省略
   ```

#### 替代策略

**策略1: 受访者I3的"双屏验证系统"**
```
硬件设置：
- 左屏：ChatGPT界面
- 右屏：验证工具（Google, Stack Overflow, 编译器）

工作流：
1. GPT生成代码（左屏）
2. 复制到IDE运行（右屏）
3. 报错 → Google（右屏）
4. 修改后重提示（左屏）

评价："有效但笨重，单屏幕用户无法这样做"
```

**策略2: 受访者I44 (AIGC项目负责人, 模式F) 的"验证清单模板"**
```
Notion文档模板：
┌─────────────────────────────┐
│ AI输出验证清单              │
├─────────────────────────────┤
│ □ 代码：运行测试           │
│   工具：Jest / Pytest       │
│   结果：                    │
│                             │
│ □ 逻辑：手动推演           │
│   发现：                    │
│                             │
│ □ 安全：扫描器检查         │
│   工具：SonarQube          │
│   问题：                    │
└─────────────────────────────┘

使用频率：高风险任务必用
缺点：完全手动填写
```

**策略3: 受访者I34 (计算化学博士后研究员, 模式A) 的"引用验证脚本"**
```python
# 自己写的Python脚本
def verify_citations(text):
    citations = extract_citations(text)
    for cite in citations:
        result = search_google_scholar(cite)
        if not result:
            print(f"⚠️ Citation not found: {cite}")
        else:
            print(f"✓ Verified: {cite}")

# 用法：
text = """[从GPT复制的文本]"""
verify_citations(text)
```

评价："有效但只适用懂编程的人"
时间：初次编写2小时，每次使用3-5分钟

### 设计需求推导

```
核心发现：
- 61%用户主动验证（高意识）
- 但验证工具分散（Google, 编译器, Wolfram Alpha, Scholar）
- 平均切换3-5个工具/次
- 工具间无集成（复制粘贴、格式转换）

结果：
- 验证摩擦导致部分用户跳过验证（危险！）
- 时间成本高（5-40分钟/次）
- 需要技术知识（编写验证脚本）

MR11解决方案：
→ 在AI界面内集成验证工具
→ 一键验证（自动格式转换）
→ 自动标注"需验证"部分
→ 追踪验证历史

预期影响：
- 验证时间减少70%（40分钟→12分钟）
- 验证率提升（从61%→90%，因为摩擦降低）
- 降低技术门槛（不需要编程）
```

---

## MR16 - 技能退化预防系统

### 定量评分

| 维度 | 分值 | 计算 |
|------|------|------|
| 用户影响 | 2.15 | (43% × 5) / 100 |
| 可行性 | 3.17 | 6 - 2.83 |
| 理论基础 | 5.00 | (5 + 5) / 2 |
| 障碍移除 | 5.00 | 高（长期能力发展） |
| **总分** | **3.60/5** | - |

### 实证基础

**受影响用户**：21/49 (43%)  
**严重性**：5/5 ("煮青蛙"效应 - 不可逆风险)

#### 用户挫折 (关键警示案例)

1. **受访者I38 (数据科学与人工智能本科生, 模式D)** - 职业危机
   ```
   时间线：
   
   Month 1 (2023年11月):
   "ChatGPT太棒了！我的编程速度提升了3倍。
   以前要花1小时的任务，现在20分钟搞定。"
   
   Month 3 (2024年1月):
   "我现在基本所有代码都让GPT写初稿，我只负责review。
   感觉很高效，我能focus在架构设计上。"
   
   Month 6 (2024年4月):
   "今天要面试一个新工作，面试官让我在白板上写代码...
   我...我完全写不出来。
   
   那些我以为我'会'的东西，原来只是GPT会，不是我会。
   我甚至忘了基本的语法，循环怎么写都要想一下。
   
   这是我职业生涯最羞耻的时刻。我现在意识到：
   过去6个月我不是在'提升效率'，我是在'失去能力'。"
   
   后果：
   - 面试失败
   - 自信心崩溃
   - 不得不花2个月"重学"编程基础
   - 职业发展倒退
   ```

2. **受访者I12 (金融风控专员, 模式C)** - 学习能力下降
   ```
   对比：
   
   Freshman year (没有ChatGPT):
   "算法课作业，我会：
   1. 先自己思考30分钟
   2. 看教材和笔记
   3. 去Stack Overflow查
   4. 问同学或TA
   5. 最后才Google答案
   
   这个过程痛苦，但我真的理解了算法。"
   
   Sophomore year (有ChatGPT):
   "现在算法作业，我会：
   1. 直接问ChatGPT
   2. 复制代码
   3. 稍微改改变量名
   4. 提交
   
   我节省了时间，成绩甚至更好。
   但期末考试（没有AI），我考得很差。
   我意识到：我没有真正学会，只是依赖AI的'拐杖'。"
   
   能力变化：
   - 独立problem-solving能力下降
   - 考试vs作业成绩差距扩大
   - 深度理解 → 表面应付
   ```

3. **受访者I26 (电商创业者, 模式C)** - 诊断能力退化
   ```
   担忧描述：
   "我用AI帮我诊断病例，它给的鉴别诊断很全面。
   
   但有一天我意识到：
   如果我一直这样，等到真正临床的时候，
   我还能独立诊断吗？
   
   病人不会等我去问AI。
   紧急情况下，我必须靠自己的知识快速决策。
   
   但现在我发现：我越来越依赖AI列鉴别诊断清单，
   自己的临床思维在萎缩。"
   
   专业风险：临床决策能力（生命攸关）
   行动：自我限制AI使用
   ```

4. **受访者I22 (强化学习博士, 模式A)** - 写作能力下降
   ```
   发现：
   "我让ChatGPT帮我写email、报告、presentation。
   
   3个月后，老板让我现场写一个简短的executive summary，
   我坐在那里盯着空白页20分钟，写不出来。
   
   我已经习惯了'AI生成→我修改'的模式，
   从零开始写作的能力已经生疏了。
   
   这个发现让我很害怕：
   如果继续这样，我会变成一个'只会编辑AI输出的人'。"
   
   能力退化：创作能力 → 编辑能力
   行动：强制每周至少1篇纯手工写作
   ```

#### 替代策略 (用户自我干预)

**策略1: 受访者I3的"每周AI禁食日"**
```
规则：
- 每周三：完全不使用任何AI工具
- 所有任务纯手工完成
- 记录完成时间 vs 平时（有AI）

目的：
"保持'肌肉记忆'，确保我没有AI也能工作"

效果：
- 第1周：非常痛苦，效率降低60%
- 第4周：效率恢复，只降低30%
- 第12周：维持能力，不再恐慌
```

**策略2: 受访者I12的"先尝试后查询"规则**
```
强制工作流：
1. 看到问题，先独立思考15分钟
2. 写下自己的解决方案（即使不确定）
3. 然后才允许问AI
4. 对比：我的方案 vs AI方案
5. 反思：差距在哪？我学到什么？

目的：
"保持独立problem-solving能力，AI只是验证工具"

挑战：
"需要巨大自律，很容易破戒直接问AI"
```

**策略3: 受访者I26 (电商创业者, 模式C) 的"月度能力测试"**
```
测试协议：
- 每月1号：完成3个诊断病例（无AI）
- 记录准确率和诊断时间
- 对比基线（入学时的能力）

触发干预：
如果准确率下降>20% → 强制1周AI禁用

数据：
Month 1: 80% 准确率（基线）
Month 3: 75% ⚠️
Month 4: 72% → 触发干预
Month 5: 78% （恢复）
```

**策略4: 受访者I38 (数据科学与人工智能本科生, 模式D) 的"能力档案"**
```
Excel表格追踪：
┌────────┬──────────┬──────────┬──────────┐
│ 技能   │ 基线水平 │ 当前水平 │ 变化趋势 │
├────────┼──────────┼──────────┼──────────┤
│ Python │ 8/10     │ 5/10     │ ↓↓↓     │
│ SQL    │ 7/10     │ 6/10     │ ↓       │
│ 架构   │ 6/10     │ 7/10     │ ↑       │
└────────┴──────────┴──────────┴──────────┘

评估方法：
- 基线：3个手工项目的自我评分
- 当前：每月重做相似项目（无AI）

警报：
如果任何技能下降>30% → 强制重新训练
```

### 设计需求推导

```
核心发现：
- 43%用户担忧能力退化（显性意识）
- 但实际退化率可能更高（很多人未察觉）
- "煮青蛙"效应：短期生产力↑ 掩盖 长期能力↓
- 用户自我干预策略：全部手动，需要极强自律

关键案例分析：
- I38: 6个月 → 职业危机
- I12: 1学期 → 考试vs作业成绩差距
- I26: 3个月 → 临床思维萎缩觉察

退化速度：快于预期（3-6个月）
恢复难度：2-3倍时间（6个月退化 → 2个月重新学习）

MR16解决方案：
→ 自动追踪技能使用模式
→ 早期预警（before不可逆）
→ 建议维持练习
→ 必要时强制干预（阻止AI访问）

预期影响：
- 将退化检测从"职业危机时发现"提前到"3周微弱信号"
- 降低干预门槛（不需要手动建Excel表）
- 提供数据支持（不是主观感觉）
```

---

## MR1 - 任务分解脚手架

### 定量评分

| 维度 | 分值 | 计算 |
|------|------|------|
| 用户影响 | 2.25 | (45% × 5) / 100 |
| 可行性 | 3.33 | 6 - 2.67 |
| 理论基础 | 5.00 | (5 + 5) / 2 |
| 障碍移除 | 3.00 | 中等 |
| **总分** | **3.30/5** | - |

### 实证基础

**受影响用户**：22/49 (45%)  
**严重性**：5/5 (元认知核心能力)

#### 用户挫折

1. **受访者I28 (金融硕士项目负责人, 模式C)** - 复杂任务压倒感
   ```
   任务："写一篇15页的宏观经济学term paper"
   
   挫折：
   "我看到这个要求就懵了，完全不知道从哪开始。
   
   我问ChatGPT：'帮我写一篇15页的宏观经济学论文'
   它生成了一篇文章，但质量很差，因为：
   - 没有明确研究问题
   - 结构混乱
   - 引用不规范
   
   我意识到：我应该先分解任务，
   但我不知道该怎么分解一个学术写作任务。"
   
   根本问题：缺乏规划能力，直接跳到执行
   ```

2. **受访者I47 (化工行业销售经理, 模式C)** - 项目无从下手
   ```
   任务："做一个todo list web app"
   
   挫折历程：
   "我问GPT：'帮我做一个todo app'
   它给了一坨代码，我复制粘贴，运行 - 报错。
   
   然后我又问：'为什么报错？'
   它给了修改建议，我照做 - 还是错。
   
   循环了10次，我都不知道自己在做什么。
   
   后来学长告诉我：'你应该先分解 -
   前端？后端？数据库？API？一步步来。'
   
   我这才意识到：
   我的问题不是代码不会写，
   是不会把大项目分解成小任务。"
   
   学习教训：缺乏分解能力 → 低效迭代
   ```

3. **受访者I2 (会计学博士生, 模式B)** - 战略规划困难
   ```
   任务："设计一个新功能的product roadmap"
   
   挫折：
   "我知道要做什么（目标），也知道AI能帮忙，
   但我不确定：
   - 哪些部分让AI做？
   - 哪些部分必须我自己想？
   - 先做什么，后做什么？
   
   我试过直接问AI：'帮我规划这个功能'
   它给了一个plan，看起来合理，
   但当我去执行时，发现很多依赖关系它没考虑到。
   
   我需要：一个帮我思考'如何分解'的工具，
   而不是直接给我分解好的结果（那我学不到东西）。"
   
   需求：脚手架式引导，不是直接答案
   ```

#### 替代策略

**策略1: 受访者I3的"5W1H分解法"**
```
对任何复杂任务，先回答：
1. What：具体要做什么？
2. Why：为什么做？目标是什么？
3. Who：谁负责什么部分？（我 vs AI）
4. When：时间线是什么？
5. Where：在什么环境/平台？
6. How：大致方法是什么？

示例（写论文）：
- What: 15页literature review
- Why: 了解领域现状，识别研究缺口
- Who: 我定研究问题，AI帮收集文献
- When: 2周，前1周收集，后1周写作
- Where: Google Scholar, 图书馆数据库
- How: 主题聚类，时间线梳理

然后：根据这个框架提示AI

评价："有效但需要学习，很多人不知道这个方法"
```

**策略2: 受访者I12的"逆向工程"**
```
方法：
1. 找一个类似的完成品（范例）
2. 反推它的结构/步骤
3. 套用到自己的任务

示例：
任务：做一个web app
→ 找GitHub上类似项目
→ 看它的文件结构：
   /frontend
   /backend
   /database
   /api
→ 意识到：我也需要这4个部分
→ 然后逐个部分问AI

评价："Workable但依赖找到好的范例"
```

**策略3: 受访者I22的"最小可行子任务"**
```
原则：
"把任务分解到：每个子任务都小到我能在1小时内完成"

示例：
大任务：准备investor pitch deck
→ 太大，再分解

子任务1：写executive summary
→ 还是太大

最小子任务1.1：列出公司3个核心价值主张
→ OK，这个能在30分钟完成

最小子任务1.2：找3个数据点支持每个价值主张
→ OK

...然后才让AI参与每个最小子任务

评价："原理正确，但手动分解很累"
```

### 设计需求推导

```
核心发现：
- 45%用户能自发分解（有元认知觉察）
- 但35%用户在复杂任务前困难（需要支持）
- 20%用户直接跳到执行（缺乏规划意识）

失败模式：
1. 直接提示"帮我做X"（太宽泛）
   → AI输出质量差
   → 低效迭代
   
2. 自己分解但方法错误
   → 遗漏依赖关系
   → 执行时卡住

用户替代策略局限：
- 需要学习（5W1H、逆向工程）
- 完全手动（无系统支持）
- 依赖外部资源（范例）

MR1解决方案：
→ 交互式分解引导
→ 提供框架模板（5W1H, SMART等）
→ 但不直接给答案（保持规划主导权）
→ 逐步淡化支架（学会后自动减少提示）

预期影响：
- 35%困难用户 → 掌握分解能力
- 20%低规划用户 → 意识到分解重要性
- 整体任务成功率提升（减少返工）
```

---

## MR9 - 动态信任校准

### 定量评分

| 维度 | 分值 | 计算 |
|------|------|------|
| 用户影响 | 4.20 | (84% × 5) / 100 |
| 可行性 | 2.00 | 6 - 4.00 |
| 理论基础 | 4.50 | (4 + 5) / 2 |
| 障碍移除 | 3.00 | 中等 |
| **总分** | **3.56/5** | - |

### 实证基础

**受影响用户**：41/49 (84%)  
**严重性**：5/5 (信任校准是有效协作的核心)

#### 用户挫折

1. **受访者I2 (会计学博士生, 模式B)** - 情境敏感信任
   ```
   信任图谱（自我报告）：
   
   任务类型              实际信任     应有信任    校准差距
   ─────────────────────────────────────────────────
   头脑风暴新功能点      90%          90%         ✓准确
   市场份额数据          70%          30%         ✗过度信任
   竞品分析              60%          60%         ✓准确
   技术可行性评估        50%          40%         小心偏差
   法律合规建议          20%          5%          ✗仍太高
   
   挫折描述：
   "我发现我对AI的信任不一致，有时太高，有时又太低。
   
   最危险的是：我不知道'应该'信任多少。
   比如市场数据，我以为信70%合理，
   后来发现它经常编造数据，应该只信30%。
   
   我希望有个系统告诉我：
   '这类任务，AI的历史准确率是X%，建议信任Y%'
   ```

2. **受访者I33 (量化交易专家, 模式C)** - 任务依赖的信任调整
   ```
   3个任务的信任演变：
   
   任务A：解释经济概念
   Week 1: 80%信任（"解释得好清楚！"）
   Week 4: 60%信任（发现2次概念混淆）
   Week 8: 70%信任（学会识别它的错误模式）
   
   任务B：查找经济数据
   Week 1: 50%信任（"总要验证"）
   Week 4: 20%信任（发现多次编造数字）
   Week 8: 10%信任（"基本不能信，只当起点"）
   
   任务C：头脑风暴研究问题
   Week 1: 60%信任
   Week 4: 75%信任（"确实激发了新想法"）
   Week 8: 85%信任（"这方面它很擅长"）
   
   挫折：
   "这个学习过程太慢了！
   我花了8周，经历多次失望，才知道该信任多少。
   
   新手没有这个'校准期'，他们怎么知道？
   很多学生就默认'信80%'所有输出，这太危险。"
   ```

3. **受访者I17 (金融经济学研究者, 模式D)** - 领域特定的零容忍
   ```
   法律任务的信任决策：
   
   场景：合同法问题
   AI回答："根据《合同法》第XX条..."
   
   律师的校准过程：
   "我第一反应是：'听起来很专业'
   我差点信了80%。
   
   但职业训练告诉我：
   法律是0容忍错误的领域。
   即使AI 95%准确，那5%错误可能导致：
   - 客户败诉
   - 律所被起诉
   - 我的执照被吊销
   
   所以我的规则：
   法律问题，AI信任度 = 0%
   它只能当brainstorming工具，
   任何正式建议必须我自己查法条。"
   
   挫折：
   "但这个零信任策略，
   意味着我没法用AI节省时间。
   我希望AI能标注：
   '这是高风险法律建议，切勿直接使用，
   仅供initial research参考'
   这样我就知道怎么定位它的角色。"
   ```

#### 替代策略

**策略1: 受访者I41 (理论光学博士生, 模式D) 的"信任矩阵"**
```
Google Sheets追踪：

┌─────────────┬────────┬────────┬────────┬────────┐
│ 任务类型    │ 尝试次数│ 正确次数│ 准确率 │ 我的信任│
├─────────────┼────────┼────────┼────────┼────────┤
│ 数学证明    │ 20     │ 8      │ 40%    │ 30% ✓  │
│ 概念解释    │ 35     │ 28     │ 80%    │ 75% ✓  │
│ 代码debug   │ 15     │ 11     │ 73%    │ 70% ✓  │
│ 学术引用    │ 12     │ 2      │ 17%    │ 5%  ✓  │
└─────────────┴────────┴────────┴────────┴────────┘

规则：
如果 |我的信任 - 实际准确率| > 20% → 重新校准

评价：
"这个方法有效，但超级tedious。
我要手动记录每次正确/错误，
大部分人不会做这个。"
```

**策略2: 受访者I3的"情境启发式"**
```
启发式规则（经验法则）：

高信任（70-90%）：
- 语法检查
- 代码语法纠错
- 头脑风暴创意
- 概念初步解释

中信任（40-60%）：
- 代码逻辑（需测试）
- 技术文档总结
- 数据分析建议

低信任（10-30%）：
- 数学推导（每步验证）
- 学术引用（永远查证）
- 医疗/法律建议

零信任（0-5%）：
- 金融交易决策
- 医疗诊断
- 法律判决依据

方法：
"我把这个列表贴在电脑旁边，
每次用AI前看一眼：这个任务属于哪类？"

局限：
"这是我的个人经验，不一定适用所有人。
而且没有考虑具体情境细节。"
```

**策略3: 受访者I44 (AIGC项目负责人, 模式F) 的"渐进式信任建立"**
```
新任务类型的信任校准协议：

第1-3次：信任0%，全面验证
→ 记录AI的表现

第4-10次：根据前期表现，初步设定信任阈值
→ 继续验证，调整阈值

第11-20次：稳定信任水平
→ 抽样验证

第21+次：信任成熟
→ 异常情况才重新评估

示例（SQL query生成）：
- 第1-3次：逐行检查，运行测试
  → 发现2/3正确
  → 初步信任：60%

- 第4-10次：重点检查"容易错"的部分（JOIN, subquery）
  → 发现5/7正确
  → 调整信任：70%

- 第11+次：快速review，相信基本逻辑
  → 只测试edge cases

评价：
"这个过程合理但缓慢。
如果系统能告诉我'SQL准确率历史数据约75%'，
我能直接跳到成熟期。"
```

### 设计需求推导

```
核心发现：
- 84%用户展现信任动态校准（最普遍行为）
- 信任范围：5%-90%（巨大差异）
- 校准依据：
  a) 个人经验（试错学习）
  b) 任务特征（风险、领域）
  c) 历史表现（成功/失败率）

问题：
1. 校准期长（4-12周trial and error）
2. 校准失败风险（过度信任 or 过度怀疑）
3. 完全依赖个人经验（无集体智慧）
4. 新手无指导（默认高信任，危险）

用户替代策略局限：
- 手动记录准确率（只有5%用户做）
- 经验启发式（不精确，难以传递）
- 渐进学习（太慢）

MR9解决方案：
→ 显示任务类型的历史准确率
→ 建议适当信任水平
→ 个性化校准（根据用户验证发现）
→ 上下文提醒（"上次此类任务你发现错误"）

预期影响：
- 校准期从8周 → 2周
- 减少过度信任导致的错误（保护新手）
- 减少过度怀疑导致的低效（释放AI价值）
- 建立集体智慧库（跨用户学习）
```

---

## 🔄 4阶段需求推导过程总结

### 阶段1：行为抽象
```
588个编码实例（49人 × 12子过程）
→ 识别：
   - 143个明确用户挫折
   - 87种替代策略
   - 模式化行为与障碍
```

### 阶段2：需求合成
```
用户挫折 → 痛点
替代策略 → 功能需求
行为模式 → 系统机制

示例：
挫折："GPT假装确定导致信任崩溃"
替代策略："手动三角验证（30-45分钟）"
→ MR13：透明不确定性显示
```

### 阶段3：多利益相关者验证

**成员检查** (Member Checking):
- 10名参与者（5高效 + 5挣扎）
- 9人认可MR列表
- 1人(I28)提出混合模式 → 修订MR8, MR9

**外部专家审查** (External Expert Review):
- 3位学习科学家
- 关键反馈：
  > "区分陈述性元认知知识 vs 程序性元认知技能"
  → 影响MR15设计（不只"教"策略，要"练习"到习惯化）

### 阶段4：定量优先级决策

所有19个MR的定量评分矩阵：

| MR | 用户影响 | 可行性 | 理论基础 | 障碍移除 | 总分 | 排序 |
|----|---------|--------|---------|---------|------|------|
| MR13 | 4.90 | 2.67 | 5.00 | 3.00 | 4.08 | 1 |
| MR23 | 1.75 | 1.00 | 4.00 | 5.00 | 2.79 | 14 |
| MR11 | 3.05 | 3.33 | 4.50 | 5.00 | 3.81 | 2 |
| MR9  | 4.20 | 2.00 | 4.50 | 3.00 | 3.56 | 3 |
| MR16 | 2.15 | 3.17 | 5.00 | 5.00 | 3.60 | 4 |
| MR1  | 2.25 | 3.33 | 5.00 | 3.00 | 3.30 | 5 |
| ... | ... | ... | ... | ... | ... | ... |

---

## 📚 论文撰写建议

### Method章节应包含：

1. **详细的需求推导过程**：
   ```
   "通过对588个编码实例的系统分析，
   我们识别了143个用户挫折点和87种自发替代策略。
   
   例如，MR13（透明不确定性显示）的推导路径为：
   - 48/49用户(98%)表达对AI'假装确定'的挫折
   - 用户I34报告花费100分钟验证编造的文献引用
   - 替代策略：手动三角验证工作流（30-45分钟/次）
   - 设计需求：系统应显式传达AI置信度和知识边界"
   ```

2. **定量优先级矩阵**：
   ```
   "需求优先级通过4维评分确定：
   用户影响(35%)、技术可行性(25%)、理论基础(20%)、障碍移除(20%)。
   
   MR13获得最高总分4.08/5.0，
   其中用户影响4.90（98%用户受影响，严重性5/5）"
   ```

3. **信效度保障**：
   ```
   "需求列表经过：
   - 成员检查（10名参与者验证，90%认可率）
   - 外部专家审查（3位学习科学家）
   - 三角验证（访谈+行为观察+关键事件）"
   ```

4. **方法论严谨性**：
   ```
   "从定性发现到系统需求，我们采用4阶段系统化过程：
   (1) 行为抽象，(2) 需求合成，(3) 多方验证，(4) 定量决策。
   每个阶段都有明确的纳入/排除标准和质量检查。"
   ```

---

**文档版本**：v1.2 - 使用规范数据源
**基于论文**：Paper 1 - Qualitative Discovery
**数据支持**：49访谈 × 588编码 × 143挫折 × 87策略
**数据更新**：根据规范数据源 llm-coding-results.md 校正受访者职业描述
**最后更新**：2024-12-01