# MLæœåŠ¡è®¾è®¡ - Pattern Recognition Service

> **æŠ€æœ¯æ ˆ**ï¼šPython 3.10+ + FastAPI + scikit-learn + XGBoost  
> **ç›®æ ‡**ï¼šå®æ—¶åˆ†ç±»ç”¨æˆ·çš„å…ƒè®¤çŸ¥ä½¿ç”¨æ¨¡å¼ï¼ˆPattern A-Fï¼‰  
> **å‡†ç¡®ç‡ç›®æ ‡**ï¼š>70%ï¼ˆè€ƒè™‘åˆ°49ä¸ªæ ·æœ¬çš„é™åˆ¶ï¼‰

---

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡

**è¾“å…¥**ï¼š12ç»´å…ƒè®¤çŸ¥ç‰¹å¾å‘é‡  
**è¾“å‡º**ï¼šPattern (A-F) + ç½®ä¿¡åº¦

**ç¤ºä¾‹**ï¼š
```python
# è¾“å…¥
features = {
    "prompt_specificity": 12.5,
    "verification_rate": 0.75,
    "iteration_frequency": 3.2,
    # ... å…±12ä¸ªç‰¹å¾
}

# è¾“å‡º
{
    "pattern": "A",
    "confidence": 0.87,
    "all_probabilities": {
        "A": 0.87,
        "B": 0.05,
        "C": 0.04,
        "D": 0.02,
        "E": 0.01,
        "F": 0.01
    }
}
```

---

## ğŸ“Š 12ç»´ç‰¹å¾å®šä¹‰

### **ç‰¹å¾1: prompt_specificityï¼ˆæç¤ºè¯å…·ä½“æ€§ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šå¹³å‡promptè¯æ•°  
**å…¬å¼**ï¼š`sum(word_count per prompt) / total_prompts`  
**å–å€¼èŒƒå›´**ï¼š0-50ï¼ˆé€šå¸¸5-20ï¼‰  
**Patternå…³è”**ï¼š
- Pattern A, Dï¼šé€šå¸¸ >12 è¯ï¼ˆè¯¦ç»†å…·ä½“ï¼‰
- Pattern Fï¼šé€šå¸¸ <8 è¯ï¼ˆè¿‡äºç®€çŸ­ï¼‰

**Pythonå®ç°**ï¼š
```python
def calc_prompt_specificity(interactions):
    if not interactions:
        return 0
    word_counts = [len(i.user_prompt.split()) for i in interactions]
    return sum(word_counts) / len(word_counts)
```

---

### **ç‰¹å¾2: verification_rateï¼ˆéªŒè¯ç‡ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šè¢«æ ‡è®°ä¸º"å·²éªŒè¯"çš„äº¤äº’å æ¯”  
**å…¬å¼**ï¼š`verified_interactions / total_interactions`  
**å–å€¼èŒƒå›´**ï¼š0-1  
**Patternå…³è”**ï¼š
- Pattern A, Dï¼š>0.7ï¼ˆé«˜éªŒè¯ï¼‰
- Pattern Cï¼š0.4-0.7ï¼ˆåŠ¨æ€ï¼‰
- Pattern Fï¼š<0.1ï¼ˆå‡ ä¹ä¸éªŒè¯ï¼‰

**Pythonå®ç°**ï¼š
```python
def calc_verification_rate(interactions):
    if not interactions:
        return 0
    verified = sum(1 for i in interactions if i.was_verified)
    return verified / len(interactions)
```

---

### **ç‰¹å¾3: iteration_frequencyï¼ˆè¿­ä»£é¢‘ç‡ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šæ¯å°æ—¶å¹³å‡è¿­ä»£æ¬¡æ•°  
**å®šä¹‰**ï¼šè¿ç»­ä¿®æ”¹åŒä¸€ä»»åŠ¡ç®—ä½œè¿­ä»£  
**å–å€¼èŒƒå›´**ï¼š0-20  
**Patternå…³è”**ï¼š
- Pattern Bï¼š>5ï¼ˆé¢‘ç¹è¿­ä»£ï¼‰
- Pattern Aï¼š2-4ï¼ˆé€‚åº¦è¿­ä»£ï¼‰
- Pattern Fï¼š<1ï¼ˆå¾ˆå°‘è¿­ä»£ï¼‰

**Pythonå®ç°**ï¼š
```python
def calc_iteration_frequency(interactions, session_duration_hours):
    if session_duration_hours == 0:
        return 0
    # æ£€æµ‹è¿ç»­çš„ç›¸ä¼¼promptï¼ˆç¼–è¾‘è·ç¦»<30%ï¼‰
    iterations = 0
    for i in range(1, len(interactions)):
        similarity = calc_similarity(
            interactions[i].user_prompt,
            interactions[i-1].user_prompt
        )
        if similarity > 0.7:  # 70%ç›¸ä¼¼åº¦ç®—è¿­ä»£
            iterations += 1
    return iterations / session_duration_hours
```

---

### **ç‰¹å¾4: modification_rateï¼ˆä¿®æ”¹ç‡ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šAIè¾“å‡ºè¢«ä¿®æ”¹çš„å æ¯”  
**å…¬å¼**ï¼š`modified_interactions / total_interactions`  
**å–å€¼èŒƒå›´**ï¼š0-1  
**Patternå…³è”**ï¼š
- Pattern Aï¼š>0.6ï¼ˆé¢‘ç¹ä¿®æ”¹AIè¾“å‡ºï¼‰
- Pattern Fï¼š<0.1ï¼ˆç›´æ¥æ¥å—ï¼‰

---

### **ç‰¹å¾5: task_decomposition_scoreï¼ˆä»»åŠ¡åˆ†è§£å¾—åˆ†ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šæ£€æµ‹ç”¨æˆ·æ˜¯å¦å°†ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡  
**å¯å‘å¼è§„åˆ™**ï¼š
- æç¤ºè¯åŒ…å«"ç¬¬ä¸€æ­¥""ç„¶å""æœ€å"ç­‰è¯ â†’ +0.2
- å¤šä¸ªç›¸å…³ä½†ç‹¬ç«‹çš„promptåºåˆ— â†’ +0.3
- æç¤ºè¯åŒ…å«æ˜ç¡®çš„å­ä»»åŠ¡åˆ—è¡¨ â†’ +0.5

**å–å€¼èŒƒå›´**ï¼š0-1  
**Patternå…³è”**ï¼š
- Pattern Aï¼š>0.7ï¼ˆæ˜æ˜¾åˆ†è§£ï¼‰
- Pattern B, Cï¼š0.3-0.6
- Pattern Fï¼š<0.2

---

### **ç‰¹å¾6: reflection_depthï¼ˆåæ€æ·±åº¦ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šæ£€æµ‹å…ƒè®¤çŸ¥åæ€è¯­è¨€  
**å…³é”®è¯æƒé‡**ï¼š
- "æˆ‘ç†è§£äº†"ã€"è¿™è®©æˆ‘æ„è¯†åˆ°" â†’ +0.15
- "å¦‚æœæˆ‘..."ã€"ä¸ºä»€ä¹ˆ" â†’ +0.1
- "æˆ‘çš„ç­–ç•¥æ˜¯" â†’ +0.2

**å–å€¼èŒƒå›´**ï¼š0-1  
**Patternå…³è”**ï¼š
- Pattern Eï¼š>0.6ï¼ˆé«˜åæ€ï¼‰
- Pattern Fï¼š<0.1

---

### **ç‰¹å¾7: cross_model_usageï¼ˆè·¨æ¨¡å‹ä½¿ç”¨ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šä½¿ç”¨ä¸åŒAIæ¨¡å‹çš„å æ¯”  
**å…¬å¼**ï¼š`unique_models_used / possible_models`  
**å–å€¼èŒƒå›´**ï¼š0-1  
**Patternå…³è”**ï¼š
- Pattern B, Cï¼š>0.3ï¼ˆå®éªŒå¤šä¸ªæ¨¡å‹ï¼‰
- Pattern Fï¼š0ï¼ˆåªç”¨ä¸€ä¸ªæ¨¡å‹ï¼‰

---

### **ç‰¹å¾8: independent_attempt_rateï¼ˆç‹¬ç«‹å°è¯•ç‡ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šç”¨æˆ·åœ¨æŸ¥è¯¢AIå‰è‡ªå·±å°è¯•çš„è¯æ®  
**å¯å‘å¼**ï¼š
- æç¤ºè¯åŒ…å«"æˆ‘è¯•è¿‡..."ã€"æˆ‘å°è¯•äº†" â†’ +1
- æè¿°å…·ä½“å°è¯•è¿‡çš„æ–¹æ³• â†’ +1

**å–å€¼èŒƒå›´**ï¼š0-10ï¼ˆä¼šè¯ä¸­ç‹¬ç«‹å°è¯•æ¬¡æ•°ï¼‰  
**Patternå…³è”**ï¼š
- Pattern A, Eï¼š>3
- Pattern Fï¼š0

---

### **ç‰¹å¾9: error_awarenessï¼ˆé”™è¯¯è§‰å¯Ÿï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šç”¨æˆ·å‘ç°å¹¶æŠ¥å‘ŠAIé”™è¯¯çš„é¢‘ç‡  
**å…¬å¼**ï¼š`rejected_interactions / total_interactions`  
**å–å€¼èŒƒå›´**ï¼š0-1  
**Patternå…³è”**ï¼š
- Pattern Dï¼š>0.3ï¼ˆé«˜æ•æ„Ÿåº¦ï¼‰
- Pattern Fï¼š0ï¼ˆæœªæ„è¯†åˆ°é”™è¯¯ï¼‰

---

### **ç‰¹å¾10: strategy_diversityï¼ˆç­–ç•¥å¤šæ ·æ€§ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šç”¨æˆ·ä½¿ç”¨ä¸åŒåä½œç­–ç•¥çš„æ•°é‡  
**ç­–ç•¥ç±»å‹**ï¼š
- ä»»åŠ¡åˆ†è§£
- è¿­ä»£ä¼˜åŒ–
- éªŒè¯æ£€æŸ¥
- è·¨æ¨¡å‹æ¯”è¾ƒ
- åæ€æ€§æé—®

**å…¬å¼**ï¼š`strategies_used / 5`  
**å–å€¼èŒƒå›´**ï¼š0-1  
**Patternå…³è”**ï¼š
- Pattern Cï¼š>0.6ï¼ˆçµæ´»åˆ‡æ¢ï¼‰
- Pattern Fï¼š<0.2

---

### **ç‰¹å¾11: trust_calibration_accuracyï¼ˆä¿¡ä»»æ ¡å‡†å‡†ç¡®æ€§ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šç”¨æˆ·å¯¹ä¸åŒä»»åŠ¡ç±»å‹çš„ä¿¡ä»»æ°´å¹³æ˜¯å¦åˆç†  
**ç†æƒ³æ ¡å‡†**ï¼š
- é«˜é£é™©ä»»åŠ¡ï¼ˆåŒ»ç–—ã€æ³•å¾‹ï¼‰ï¼šä½ä¿¡ä»»ï¼ˆé«˜éªŒè¯ï¼‰
- ä½é£é™©ä»»åŠ¡ï¼ˆå¤´è„‘é£æš´ï¼‰ï¼šé«˜ä¿¡ä»»ï¼ˆä½éªŒè¯ï¼‰

**æµ‹é‡**ï¼šè§‚å¯ŸéªŒè¯è¡Œä¸ºä¸ä»»åŠ¡é‡è¦æ€§çš„ç›¸å…³æ€§  
**å–å€¼èŒƒå›´**ï¼š0-1ï¼ˆ1=å®Œç¾æ ¡å‡†ï¼‰  
**Patternå…³è”**ï¼š
- Pattern C, Dï¼š>0.7
- Pattern Fï¼š<0.3ï¼ˆç›²ç›®ä¿¡ä»»æˆ–ç›²ç›®ä¸ä¿¡ï¼‰

---

### **ç‰¹å¾12: time_before_ai_queryï¼ˆAIæŸ¥è¯¢å‰æ€è€ƒæ—¶é—´ï¼‰**
**è®¡ç®—æ–¹æ³•**ï¼šç”¨æˆ·å¹³å‡åœ¨ç¬¬ä¸€æ¬¡AIæŸ¥è¯¢å‰çš„æ—¶é—´  
**å•ä½**ï¼šåˆ†é’Ÿ  
**å–å€¼èŒƒå›´**ï¼š0-30  
**Patternå…³è”**ï¼š
- Pattern A, Eï¼š>5åˆ†é’Ÿï¼ˆå…ˆè‡ªå·±æ€è€ƒï¼‰
- Pattern Fï¼š<1åˆ†é’Ÿï¼ˆç«‹å³æ±‚åŠ©AIï¼‰

---

## ğŸ¤– MLæ¨¡å‹æ¶æ„

### **Ensembleæ¨¡å‹è®¾è®¡**

```python
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

class PatternClassifier:
    def __init__(self):
        # ä¸‰ä¸ªåŸºç¡€æ¨¡å‹
        self.rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            random_state=42
        )
        
        self.svm = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,  # å¯ç”¨æ¦‚ç‡è¾“å‡º
            random_state=42
        )
        
        self.xgb = XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        
        # Soft voting ensemble
        self.ensemble = VotingClassifier(
            estimators=[
                ('rf', self.rf),
                ('svm', self.svm),
                ('xgb', self.xgb)
            ],
            voting='soft',  # ä½¿ç”¨æ¦‚ç‡å¹³å‡
            weights=[0.4, 0.3, 0.3]  # Random Forestæƒé‡ç¨é«˜
        )
        
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
    
    def train(self, X, y):
        # 1. æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X)
        
        # 2. ç¼–ç æ ‡ç­¾ (A-F â†’ 0-5)
        y_encoded = self.label_encoder.fit_transform(y)
        
        # 3. è®­ç»ƒensemble
        self.ensemble.fit(X_scaled, y_encoded)
        
        # 4. äº¤å‰éªŒè¯
        scores = cross_val_score(
            self.ensemble, 
            X_scaled, 
            y_encoded, 
            cv=5,
            scoring='f1_macro'
        )
        print(f"Cross-validation F1: {scores.mean():.3f} (+/- {scores.std():.3f})")
    
    def predict(self, features):
        # 1. æ ‡å‡†åŒ–
        X = self.scaler.transform([features])
        
        # 2. é¢„æµ‹æ¦‚ç‡
        probas = self.ensemble.predict_proba(X)[0]
        
        # 3. è·å–æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
        predicted_idx = np.argmax(probas)
        predicted_pattern = self.label_encoder.inverse_transform([predicted_idx])[0]
        confidence = probas[predicted_idx]
        
        # 4. å¦‚æœç½®ä¿¡åº¦<60%ï¼Œfallbackåˆ°è§„åˆ™å¼•æ“
        if confidence < 0.6:
            return self.rule_based_fallback(features)
        
        return {
            'pattern': predicted_pattern,
            'confidence': float(confidence),
            'all_probabilities': {
                label: float(prob) 
                for label, prob in zip(self.label_encoder.classes_, probas)
            }
        }
    
    def rule_based_fallback(self, features):
        """
        è§„åˆ™å¼•æ“ï¼šå½“MLä¸ç¡®å®šæ—¶ä½¿ç”¨
        """
        # Rule 1: é«˜éªŒè¯ç‡ + é«˜åˆ†è§£ â†’ Pattern A
        if features[1] > 0.7 and features[4] > 0.7:
            return {'pattern': 'A', 'confidence': 0.75}
        
        # Rule 2: é«˜è¿­ä»£ + è·¨æ¨¡å‹ â†’ Pattern B
        if features[2] > 5 and features[6] > 0.3:
            return {'pattern': 'B', 'confidence': 0.70}
        
        # Rule 3: é«˜ç­–ç•¥å¤šæ ·æ€§ â†’ Pattern C
        if features[9] > 0.6:
            return {'pattern': 'C', 'confidence': 0.72}
        
        # Rule 4: é«˜é”™è¯¯è§‰å¯Ÿ + é«˜éªŒè¯ â†’ Pattern D
        if features[8] > 0.3 and features[1] > 0.9:
            return {'pattern': 'D', 'confidence': 0.70}
        
        # Rule 5: é«˜åæ€æ·±åº¦ â†’ Pattern E
        if features[5] > 0.6:
            return {'pattern': 'E', 'confidence': 0.68}
        
        # Rule 6: ä½éªŒè¯ + ä½åˆ†è§£ + ä½ç‹¬ç«‹å°è¯• â†’ Pattern F
        if features[1] < 0.1 and features[4] < 0.2 and features[7] == 0:
            return {'pattern': 'F', 'confidence': 0.65}
        
        # é»˜è®¤ï¼šPattern Cï¼ˆæœ€å¸¸è§ï¼‰
        return {'pattern': 'C', 'confidence': 0.50}
```

---

## ğŸš€ FastAPIæœåŠ¡å®ç°

```python
# ml-service/main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, List
import numpy as np
from model import PatternClassifier

app = FastAPI(title="MCA Pattern Recognition Service")

# å…¨å±€æ¨¡å‹å®ä¾‹
classifier = PatternClassifier()

# å¯åŠ¨æ—¶åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
@app.on_event("startup")
async def load_model():
    try:
        classifier.load('./models/pattern_classifier.pkl')
        print("âœ… Model loaded successfully")
    except FileNotFoundError:
        print("âš ï¸ No trained model found. Using rule-based engine only.")

class FeatureVector(BaseModel):
    prompt_specificity: float
    verification_rate: float
    iteration_frequency: float
    modification_rate: float
    task_decomposition_score: float
    reflection_depth: float
    cross_model_usage: float
    independent_attempt_rate: float
    error_awareness: float
    strategy_diversity: float
    trust_calibration_accuracy: float
    time_before_ai_query: float

class PredictionResponse(BaseModel):
    pattern: str
    confidence: float
    all_probabilities: Dict[str, float]
    method: str  # 'ml_ensemble' or 'rule_based'

@app.post("/predict", response_model=PredictionResponse)
async def predict_pattern(features: FeatureVector):
    """
    ä¸»é¢„æµ‹endpoint
    """
    try:
        # è½¬æ¢ä¸ºnumpy array
        feature_array = np.array([
            features.prompt_specificity,
            features.verification_rate,
            features.iteration_frequency,
            features.modification_rate,
            features.task_decomposition_score,
            features.reflection_depth,
            features.cross_model_usage,
            features.independent_attempt_rate,
            features.error_awareness,
            features.strategy_diversity,
            features.trust_calibration_accuracy,
            features.time_before_ai_query
        ])
        
        # é¢„æµ‹
        result = classifier.predict(feature_array)
        result['method'] = 'ml_ensemble' if result['confidence'] >= 0.6 else 'rule_based'
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": classifier.is_loaded()}

@app.post("/retrain")
async def retrain_model(training_data: TrainingData):
    """
    å…è®¸ç³»ç»Ÿå®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹
    """
    try:
        X = np.array(training_data.features)
        y = np.array(training_data.labels)
        
        classifier.train(X, y)
        classifier.save('./models/pattern_classifier.pkl')
        
        return {"status": "success", "samples": len(y)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
ml-service/
â”œâ”€â”€ main.py                 # FastAPI app
â”œâ”€â”€ model.py                # PatternClassifierç±»
â”œâ”€â”€ features.py             # ç‰¹å¾æå–å‡½æ•°
â”œâ”€â”€ train.py                # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ requirements.txt        # Pythonä¾èµ–
â”œâ”€â”€ models/                 # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
â”‚   â”œâ”€â”€ pattern_classifier.pkl
â”‚   â””â”€â”€ scaler.pkl
â””â”€â”€ data/                   # è®­ç»ƒæ•°æ®
    â””â”€â”€ training_data.csv
```

---

## ğŸ“ è®­ç»ƒæ•°æ®å‡†å¤‡

### **ä»49ä¸ªè®¿è°ˆæå–ç‰¹å¾**

ä½ éœ€è¦æ‰‹åŠ¨ä»è®¿è°ˆä¸­æå–ç‰¹å¾ï¼Œåˆ›å»º`training_data.csv`ï¼š

```csv
prompt_specificity,verification_rate,iteration_frequency,modification_rate,task_decomposition_score,reflection_depth,cross_model_usage,independent_attempt_rate,error_awareness,strategy_diversity,trust_calibration_accuracy,time_before_ai_query,pattern_label
12.5,0.75,3.2,0.65,0.80,0.45,0.00,3,0.20,0.70,0.80,5.5,A
8.3,0.15,7.8,0.55,0.30,0.25,0.67,1,0.10,0.60,0.55,2.1,B
15.2,0.68,2.5,0.72,0.55,0.35,0.50,4,0.25,0.85,0.90,4.2,C
18.5,0.92,1.8,0.45,0.60,0.40,0.33,5,0.42,0.65,0.95,7.3,D
11.0,0.85,4.2,0.68,0.75,0.88,0.00,6,0.30,0.55,0.75,8.5,E
5.2,0.05,0.5,0.08,0.15,0.10,0.00,0,0.02,0.20,0.25,0.5,F
...ï¼ˆå…±49è¡Œï¼Œæ¯ä¸ªè®¿è°ˆä¸€è¡Œï¼‰
```

### **æ•°æ®æå–æŒ‡å—**

å¯¹äºæ¯ä¸ªè®¿è°ˆå‚ä¸è€…ï¼ˆI1-I49ï¼‰ï¼Œé˜…è¯»è®¿è°ˆè®°å½•å¹¶ä¼°è®¡12ä¸ªç‰¹å¾ï¼š

1. **prompt_specificity**: è®¿è°ˆä¸­æè¿°çš„å¹³å‡promptè¯æ•°
2. **verification_rate**: ä»–ä»¬è¯´éªŒè¯äº†å¤šå°‘æ¯”ä¾‹çš„AIè¾“å‡ºï¼Ÿ
3. **iteration_frequency**: æè¿°äº†å¤šå°‘æ¬¡è¿­ä»£è¡Œä¸ºï¼Ÿ
4. **modification_rate**: å¤šå¸¸ä¿®æ”¹AIè¾“å‡ºï¼Ÿ
5. **task_decomposition_score**: æ˜¯å¦å±•ç°åˆ†è§£ç­–ç•¥ï¼Ÿ(0=æ— , 1=æ˜æ˜¾)
6. **reflection_depth**: è®¿è°ˆä¸­åæ€è¯­è¨€çš„ä¸°å¯Œç¨‹åº¦ (0-1)
7. **cross_model_usage**: æ˜¯å¦ä½¿ç”¨å¤šä¸ªæ¨¡å‹ï¼Ÿ(0=å•ä¸€, 1=å…¨éƒ¨)
8. **independent_attempt_rate**: æè¿°äº†å¤šå°‘æ¬¡å…ˆè‡ªå·±å°è¯•ï¼Ÿ
9. **error_awareness**: å¤šå¸¸å‘ç°AIé”™è¯¯ï¼Ÿ(0-1)
10. **strategy_diversity**: ä½¿ç”¨äº†å‡ ç§ä¸åŒç­–ç•¥ï¼Ÿ(0-1)
11. **trust_calibration_accuracy**: ä¿¡ä»»æ˜¯å¦éšä»»åŠ¡ç±»å‹åˆç†å˜åŒ–ï¼Ÿ(0-1)
12. **time_before_ai_query**: æè¿°çš„å¹³å‡"å…ˆæ€è€ƒ"æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
13. **pattern_label**: Pattern A-Fï¼ˆæ ¹æ®ä½ è®ºæ–‡ä¸­çš„åˆ†ç±»ï¼‰

---

## ğŸ”¬ æ¨¡å‹è¯„ä¼°

```python
# train.py
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(classifier, X_test, y_test):
    # é¢„æµ‹
    y_pred = [classifier.predict(x)['pattern'] for x in X_test]
    
    # åˆ†ç±»æŠ¥å‘Š
    print(classification_report(y_test, y_pred))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred, labels=['A', 'B', 'C', 'D', 'E', 'F'])
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', 
                xticklabels=['A', 'B', 'C', 'D', 'E', 'F'],
                yticklabels=['A', 'B', 'C', 'D', 'E', 'F'])
    plt.title('Pattern Classification Confusion Matrix')
    plt.ylabel('True Pattern')
    plt.xlabel('Predicted Pattern')
    plt.savefig('confusion_matrix.png')
    
    # ç‰¹å¾é‡è¦æ€§ï¼ˆRandom Forestï¼‰
    importances = classifier.rf.feature_importances_
    feature_names = [
        'prompt_specificity', 'verification_rate', 'iteration_frequency',
        'modification_rate', 'task_decomposition_score', 'reflection_depth',
        'cross_model_usage', 'independent_attempt_rate', 'error_awareness',
        'strategy_diversity', 'trust_calibration_accuracy', 'time_before_ai_query'
    ]
    
    plt.figure(figsize=(12, 6))
    plt.bar(feature_names, importances)
    plt.xticks(rotation=45, ha='right')
    plt.title('Feature Importance')
    plt.tight_layout()
    plt.savefig('feature_importance.png')
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### **1. æ¨¡å‹ç¼“å­˜**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_predict(features_tuple):
    features = np.array(features_tuple)
    return classifier.predict(features)
```

### **2. æ‰¹é‡é¢„æµ‹**
```python
@app.post("/predict_batch")
async def predict_batch(feature_list: List[FeatureVector]):
    results = []
    for features in feature_list:
        result = classifier.predict(features.to_array())
        results.append(result)
    return results
```

### **3. å¼‚æ­¥å¤„ç†**
```python
import asyncio

@app.post("/predict_async")
async def predict_async(features: FeatureVector):
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None, 
        classifier.predict, 
        features.to_array()
    )
    return result
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### **é—®é¢˜1ï¼šå‡†ç¡®ç‡<50%**
**å¯èƒ½åŸå› **ï¼š
- è®­ç»ƒæ•°æ®æ ‡æ³¨é”™è¯¯
- ç‰¹å¾æå–ä¸å‡†ç¡®
- ç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é‡æ–°æ£€æŸ¥è®­ç»ƒæ•°æ®
- ä½¿ç”¨SMOTEè¿‡é‡‡æ ·å°‘æ•°ç±»
- è°ƒæ•´class_weightå‚æ•°

### **é—®é¢˜2ï¼šç½®ä¿¡åº¦æ€»æ˜¯å¾ˆä½**
**å¯èƒ½åŸå› **ï¼š
- æ¨¡å‹è¿‡äºè°¨æ…
- ç±»åˆ«è¾¹ç•Œæ¨¡ç³Š

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é™ä½fallbacké˜ˆå€¼ï¼ˆä»0.6é™åˆ°0.5ï¼‰
- å¢å¼ºè§„åˆ™å¼•æ“

### **é—®é¢˜3ï¼šæ€»æ˜¯é¢„æµ‹Pattern C**
**å¯èƒ½åŸå› **ï¼š
- Pattern Cæ ·æœ¬å æ¯”è¿‡é«˜ï¼ˆ33%ï¼‰
- æ¨¡å‹æ¬ æ‹Ÿåˆ

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å¹³è¡¡è®­ç»ƒæ•°æ®
- å¢åŠ æ¨¡å‹å¤æ‚åº¦

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æœ€åæ›´æ–°**ï¼š2024-11-15  
**ä¸‹ä¸€æ­¥**ï¼šå¼€å§‹è®­ç»ƒæ•°æ®æå–ï¼Œç„¶åè®­ç»ƒæ¨¡å‹ï¼
