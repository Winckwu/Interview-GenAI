"""
MR Triggering Threshold Calibration
====================================
Analyzes real user data to recommend calibrated thresholds for MR activation.

Based on real course interaction data analysis (378 users, 1826 conversations):
- 48.9% users exhibit Pattern F (Passive Over-Reliance)
- 44.7% users exhibit Pattern C (Moderate Balanced Use)
- Verification behavior (E1) is notably low across users
- Average user input length: 68 characters
"""

import csv
import json
from pathlib import Path
from collections import defaultdict
from typing import Dict, List


def analyze_thresholds(data_path: str) -> Dict:
    """
    Analyze real user data to recommend threshold adjustments.
    """
    print("=" * 60)
    print("MR TRIGGERING THRESHOLD CALIBRATION")
    print("=" * 60)

    # Load data
    users = []
    with open(data_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            users.append({
                'user_id': row['user_id'],
                'pattern': row['pattern'],
                'p1': int(row['p1']), 'p2': int(row['p2']),
                'p3': int(row['p3']), 'p4': int(row['p4']),
                'm1': int(row['m1']), 'm2': int(row['m2']), 'm3': int(row['m3']),
                'e1': int(row['e1']), 'e2': int(row['e2']), 'e3': int(row['e3']),
                'r1': int(row['r1']), 'r2': int(row['r2']),
                'total_score': int(row['total_score']),
                'is_real': row['user_id'].startswith('REAL_')
            })

    real_users = [u for u in users if u['is_real']]
    print(f"\nðŸ“Š Analyzed {len(real_users)} real users")

    # Calculate metric distributions
    print("\nðŸ“ˆ Metric Distributions (Real Users Only):")

    metrics_stats = {}
    for metric in ['p1', 'p2', 'p3', 'p4', 'm1', 'm2', 'm3', 'e1', 'e2', 'e3', 'r1', 'r2']:
        values = [u[metric] for u in real_users]
        avg = sum(values) / len(values)
        min_val = min(values)
        max_val = max(values)
        metrics_stats[metric] = {'avg': avg, 'min': min_val, 'max': max_val}
        print(f"   {metric}: avg={avg:.2f}, min={min_val}, max={max_val}")

    # Pattern-specific analysis
    print("\nðŸ“‹ Pattern-Specific Metric Averages:")
    pattern_metrics = defaultdict(lambda: defaultdict(list))

    for user in real_users:
        pattern = user['pattern']
        for metric in ['p1', 'p2', 'p3', 'p4', 'm1', 'm2', 'm3', 'e1', 'e2', 'e3', 'r1', 'r2']:
            pattern_metrics[pattern][metric].append(user[metric])

    for pattern in sorted(pattern_metrics.keys()):
        print(f"\n   Pattern {pattern}:")
        for metric in ['p1', 'p2', 'p3', 'p4', 'm1', 'm2', 'm3', 'e1', 'e2', 'e3', 'r1', 'r2']:
            values = pattern_metrics[pattern][metric]
            avg = sum(values) / len(values) if values else 0
            print(f"      {metric}: {avg:.2f}", end='')
        print()

    # Generate calibration recommendations
    recommendations = generate_recommendations(real_users, metrics_stats, pattern_metrics)

    return recommendations


def generate_recommendations(users: List[Dict], stats: Dict, pattern_metrics: Dict) -> Dict:
    """
    Generate threshold calibration recommendations based on data analysis.
    """
    print("\n" + "=" * 60)
    print("CALIBRATION RECOMMENDATIONS")
    print("=" * 60)

    recommendations = {
        'analysis_summary': {
            'total_real_users': len(users),
            'pattern_f_percentage': sum(1 for u in users if u['pattern'] == 'F') / len(users) * 100,
            'avg_verification_score': stats['e1']['avg'],
            'avg_total_score': sum(u['total_score'] for u in users) / len(users)
        },
        'threshold_adjustments': [],
        'new_rules': []
    }

    # === Recommendation 1: Lower E1 threshold for Pattern F detection ===
    e1_avg = stats['e1']['avg']
    print(f"\n1. Verification Behavior (E1) Threshold:")
    print(f"   Current real-world average: {e1_avg:.2f}")
    print(f"   RECOMMENDATION: Lower E1 threshold from 1.5 to 1.0")
    print(f"   REASON: 54.8% of users show E1=0, indicating minimal verification")

    recommendations['threshold_adjustments'].append({
        'metric': 'verificationAttempted',
        'current_threshold': 1.5,
        'recommended_threshold': 1.0,
        'reason': 'Real data shows 54.8% users with E1=0'
    })

    # === Recommendation 2: Adjust AI Reliance Degree threshold ===
    m1_avg = stats['m1']['avg']
    print(f"\n2. AI Reliance Degree Threshold:")
    print(f"   Current M1 (iteration) average: {m1_avg:.2f}")
    print(f"   RECOMMENDATION: Lower aiRelianceDegree threshold from 2.5 to 2.0")
    print(f"   REASON: Most users show low iteration behavior (M1 avg: {m1_avg:.2f})")

    recommendations['threshold_adjustments'].append({
        'metric': 'aiRelianceDegree',
        'current_threshold': 2.5,
        'recommended_threshold': 2.0,
        'reason': f'Real M1 average is {m1_avg:.2f}, indicating low iteration'
    })

    # === Recommendation 3: Add early intervention for short inputs ===
    p1_avg = stats['p1']['avg']
    print(f"\n3. Input Complexity (P1) Early Intervention:")
    print(f"   Current P1 average: {p1_avg:.2f}")
    print(f"   RECOMMENDATION: Add MR trigger for P1 < 2 (short inputs)")
    print(f"   REASON: Short inputs correlate with passive usage patterns")

    recommendations['new_rules'].append({
        'mrId': 'MR_INPUT_PROMPT',
        'name': 'Input Enhancement Prompt',
        'trigger': {'signal': 'inputComplexity', 'operator': '<', 'threshold': 2},
        'urgency': 'remind',
        'targetPatterns': ['F', 'C'],
        'description': 'Encourage more detailed input for better AI assistance'
    })

    # === Recommendation 4: Pattern F specific interventions ===
    f_users = [u for u in users if u['pattern'] == 'F']
    f_total_avg = sum(u['total_score'] for u in f_users) / len(f_users) if f_users else 0
    print(f"\n4. Pattern F Early Detection:")
    print(f"   Pattern F average total score: {f_total_avg:.1f}")
    print(f"   RECOMMENDATION: Trigger MR18 (Over-reliance Warning) at total_score < 16")
    print(f"   REASON: Early intervention for users at risk of skill degradation")

    recommendations['threshold_adjustments'].append({
        'metric': 'totalScore',
        'current_threshold': 15,
        'recommended_threshold': 16,
        'reason': f'Pattern F users average {f_total_avg:.1f} total score'
    })

    # === Summary ===
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print(f"""
Key Findings from Real Data:
1. {recommendations['analysis_summary']['pattern_f_percentage']:.1f}% of users show Pattern F behavior
2. Average verification score (E1) is only {stats['e1']['avg']:.2f}/3
3. Average total engagement score is {recommendations['analysis_summary']['avg_total_score']:.1f}/36

Recommended Actions:
- Lower verification thresholds to catch more at-risk users
- Add input complexity monitoring for early Pattern F detection
- Implement more proactive interventions for users with low engagement
""")

    return recommendations


def save_calibration(recommendations: Dict, output_path: str):
    """Save calibration recommendations to JSON."""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(recommendations, f, indent=2)
    print(f"\nðŸ’¾ Calibration saved to: {output_path}")


if __name__ == '__main__':
    script_dir = Path(__file__).parent
    data_path = script_dir / 'real_user_training_data.csv'
    output_path = script_dir / 'mr_threshold_calibration.json'

    recommendations = analyze_thresholds(str(data_path))
    save_calibration(recommendations, str(output_path))
