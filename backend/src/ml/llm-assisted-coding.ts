/**
 * LLM-Assisted Interview Coding
 *
 * Uses Claude's semantic understanding to code interviews according to
 * the 12-subprocess metacognitive framework with evidence-based scoring.
 *
 * Follows constructivist grounded theory approach with:
 * - Three-level evidence strength (âœ“âœ“âœ“=3, âœ“âœ“=2, âœ“=1, âœ—=0)
 * - High-risk task dominance principle
 * - Mixed pattern recognition
 */

import * as fs from 'fs';
import * as path from 'path';

type EvidenceStrength = 0 | 1 | 2 | 3;
type Pattern = 'A' | 'B' | 'C' | 'D' | 'E' | 'F';
type Confidence = 'high' | 'moderate' | 'low';

interface SubprocessCoding {
  subprocess: string;
  score: EvidenceStrength;
  rationale: string;
  evidence: string[]; // Direct quotes
}

interface InterviewAnalysis {
  interviewId: string;
  participant: {
    background: string;
    aiUsageFrequency: string;
  };
  highRiskScenarios: string[];
  lowRiskScenarios: string[];
  subprocessCoding: SubprocessCoding[];
  totalScore: number;
  primaryPattern: Pattern;
  secondaryPattern?: Pattern;
  confidence: Confidence;
  isMixed: boolean;
  analysisSummary: string;
}

const SUBPROCESS_DEFINITIONS = {
  P1: {
    name: 'Task Decomposition',
    description: 'å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡æˆ–æ­¥éª¤',
    strongEvidence: 'æ˜ç¡®æè¿°åˆ†æ®µã€åˆ†å—ã€é€æ­¥å¤„ç†çš„ä¹ æƒ¯æ€§è¡Œä¸ºï¼Œå¤šæ¬¡æåŠï¼Œè·¨æƒ…å¢ƒä¸€è‡´',
    moderateEvidence: 'æè¿°äº†å…·ä½“çš„åˆ†è§£è¡Œä¸ºï¼Œè‡³å°‘ä¸€æ¬¡æ¸…æ™°ç¤ºä¾‹',
    weakEvidence: 'æåŠåˆ†è§£æ¦‚å¿µä½†æ— å…·ä½“è¡Œä¸ºæè¿°ï¼Œæˆ–ä»…åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹ä½¿ç”¨',
  },
  P2: {
    name: 'Goal Setting',
    description: 'æ˜ç¡®è®¾å®šä»»åŠ¡ç›®æ ‡å’ŒæœŸæœ›ç»“æœ',
    strongEvidence: 'è¯¦ç»†æè¿°å¦‚ä½•è®¾å®šæ˜ç¡®ã€å…·ä½“çš„ç›®æ ‡ï¼ŒåŒ…å«è¯„åˆ¤æ ‡å‡†',
    moderateEvidence: 'æåŠè®¾å®šç›®æ ‡æˆ–è¦æ±‚ï¼Œæœ‰ä¸€å®šå…·ä½“æ€§',
    weakEvidence: 'æ¨¡ç³ŠæåŠç›®æ ‡æˆ–æœŸæœ›',
  },
  P3: {
    name: 'Strategy Selection',
    description: 'é€‰æ‹©å’Œç»„åˆå¤šç§å·¥å…·æˆ–æ–¹æ³•',
    strongEvidence: 'ä½¿ç”¨3+ç§å·¥å…·ï¼Œæˆ–æ ¹æ®ä»»åŠ¡ç±»å‹ç³»ç»Ÿæ€§è°ƒæ•´ç­–ç•¥',
    moderateEvidence: 'ä½¿ç”¨2ç§å·¥å…·ï¼Œæˆ–æœ‰ä»»åŠ¡é€‚åº”æ€§è€ƒè™‘',
    weakEvidence: 'ä»…ä½¿ç”¨1ç§å·¥å…·ï¼Œç­–ç•¥å•ä¸€',
  },
  P4: {
    name: 'Resource Planning',
    description: 'æå‰å‡†å¤‡ã€è‡ªå·±å…ˆåšåŸºç¡€å·¥ä½œ',
    strongEvidence: 'æ˜ç¡®çš„"è‡ªå·±å…ˆåšï¼ŒAIè¾…åŠ©"åŸåˆ™ï¼Œå¤šæ¬¡å¼ºè°ƒ',
    moderateEvidence: 'æè¿°äº†æŸäº›å‡†å¤‡è¡Œä¸º',
    weakEvidence: 'å¶å°”æåŠå‡†å¤‡',
  },
  M1: {
    name: 'Progress Monitoring',
    description: 'æŒç»­è·Ÿè¸ªä»»åŠ¡è¿›å±•',
    strongEvidence: 'æè¿°æ¯ä¸€æ­¥éƒ½æ£€æŸ¥ã€æŒç»­ç›‘æ§çš„ä¹ æƒ¯',
    moderateEvidence: 'æåŠé˜¶æ®µæ€§æ£€æŸ¥',
    weakEvidence: 'æ¨¡ç³ŠæåŠè¿›åº¦å…³æ³¨',
  },
  M2: {
    name: 'Quality Checking',
    description: 'éªŒè¯AIè¾“å‡ºè´¨é‡',
    strongEvidence: 'ç³»ç»ŸåŒ–éªŒè¯æµç¨‹ï¼ˆå¦‚äº¤å‰éªŒè¯ã€é€å¥å¯¹æ¯”ï¼‰ï¼Œ"æ¯æ¬¡éƒ½"ã€"æ€»æ˜¯"ç­‰ç»å¯¹é¢‘ç‡è¯',
    moderateEvidence: '"ç»å¸¸"ã€"ä¼š"è¿›è¡ŒéªŒè¯ï¼Œæœ‰å…·ä½“æ–¹æ³•æè¿°',
    weakEvidence: 'å¶å°”éªŒè¯ï¼Œæˆ–ä»…æ„è¯†åˆ°åº”è¯¥éªŒè¯ä½†æœªå®æ–½',
  },
  M3: {
    name: 'Context Monitoring',
    description: 'æ ¹æ®æƒ…å¢ƒè°ƒæ•´ä½¿ç”¨æ–¹å¼',
    strongEvidence: 'è¯¦ç»†æè¿°ä¸åŒæƒ…å¢ƒä¸‹çš„ä¸åŒç­–ç•¥ï¼Œæœ‰å¤šä¸ªå¯¹æ¯”ç¤ºä¾‹',
    moderateEvidence: 'æåŠæƒ…å¢ƒå·®å¼‚ï¼Œè‡³å°‘ä¸€ä¸ªå¯¹æ¯”',
    weakEvidence: 'æ¨¡ç³Šæ„è¯†åˆ°æƒ…å¢ƒé‡è¦æ€§',
  },
  E1: {
    name: 'Result Evaluation',
    description: 'è¯„ä¼°è¾“å‡ºç»“æœè´¨é‡',
    strongEvidence: 'æœ‰æ˜ç¡®çš„è¯„ä»·æ ‡å‡†å’Œç³»ç»ŸåŒ–è¯„ä¼°æµç¨‹',
    moderateEvidence: 'æè¿°äº†è¯„ä»·è¡Œä¸ºå’Œåˆ¤æ–­ä¾æ®',
    weakEvidence: 'ç®€å•çš„æ»¡æ„/ä¸æ»¡æ„åˆ¤æ–­',
  },
  E2: {
    name: 'Learning Reflection',
    description: 'æ€»ç»“ç»éªŒã€åæ€å­¦ä¹ ',
    strongEvidence: 'æ˜ç¡®æè¿°ä»ä½¿ç”¨ä¸­æ€»ç»“è§„å¾‹ã€è°ƒæ•´ç†è§£',
    moderateEvidence: 'æåŠå­¦ä¹ æˆ–ç»éªŒç§¯ç´¯',
    weakEvidence: 'æ¨¡ç³Šçš„å­¦ä¹ æ„è¯†',
  },
  E3: {
    name: 'Capability Judgment',
    description: 'åˆ¤æ–­AIèƒ½åŠ›è¾¹ç•Œ',
    strongEvidence: '"è‡ªå·±å…ˆåš"åŸåˆ™ï¼Œæ˜ç¡®AIå±€é™æ€§ï¼Œä»»åŠ¡-èƒ½åŠ›åŒ¹é…æ„è¯†',
    moderateEvidence: 'æåŠAIä¸æ“…é•¿æŸäº›ä»»åŠ¡ï¼Œæˆ–é€‰æ‹©æ€§ä¿¡ä»»',
    weakEvidence: 'æ¨¡ç³Šçš„èƒ½åŠ›æ„è¯†',
  },
  R1: {
    name: 'Strategy Adjustment',
    description: 'è¿­ä»£è°ƒæ•´ã€å¤šè½®ä¼˜åŒ–',
    strongEvidence: 'è¯¦ç»†æè¿°å¤šè½®è°ƒæ•´æµç¨‹ï¼ˆ3+è½®ï¼‰ï¼Œæˆ–ä¸»åŠ¨åˆ‡æ¢æ–¹æ³•',
    moderateEvidence: 'æåŠè°ƒæ•´æˆ–ä¼˜åŒ–ï¼Œ2è½®å·¦å³',
    weakEvidence: 'å¶å°”è°ƒæ•´',
  },
  R2: {
    name: 'Trust Calibration',
    description: 'æ ¹æ®ä»»åŠ¡è°ƒæ•´ä¿¡ä»»æ°´å¹³',
    strongEvidence: 'æ˜ç¡®åŒºåˆ†ä¸åŒä»»åŠ¡ç±»å‹çš„ä¿¡ä»»ç¨‹åº¦ï¼Œæœ‰å…·ä½“ç™¾åˆ†æ¯”æˆ–å¯¹æ¯”',
    moderateEvidence: 'æåŠä¿¡ä»»å·®å¼‚æˆ–æ ¡å‡†',
    weakEvidence: 'ç®€å•çš„ä¿¡/ä¸ä¿¡åˆ¤æ–­',
  },
};

const PATTERN_DEFINITIONS = {
  A: {
    name: 'Strategic Decomposition & Control',
    description: 'æˆ˜ç•¥æ€§åˆ†è§£ä¸æ§åˆ¶',
    necessaryConditions: 'P1â‰¥2 AND M2â‰¥2 AND E3â‰¥2 AND totalâ‰¥24',
    sufficientConditions: 'P1+P4+M2+E3â‰¥10 for high confidence',
    behaviorSignature: 'ç³»ç»Ÿåˆ†è§£ã€ä¸¥æ ¼éªŒè¯ã€ç‹¬ç«‹ä¼˜å…ˆ',
  },
  B: {
    name: 'Iterative Optimization',
    description: 'è¿­ä»£ä¼˜åŒ–ä¸æ ¡å‡†',
    necessaryConditions: 'R1â‰¥2 AND totalâ‰¥20',
    sufficientConditions: 'R1=3 for high confidence',
    behaviorSignature: 'å¤šè½®è°ƒæ•´ã€æŒç»­ä¼˜åŒ–',
  },
  C: {
    name: 'Context-Sensitive Adaptation',
    description: 'æƒ…å¢ƒæ•æ„Ÿçš„é€‚é…',
    necessaryConditions: 'P3â‰¥2 AND R2â‰¥2 AND totalâ‰¥22',
    sufficientConditions: 'P3+M3+R2â‰¥7 for high confidence',
    behaviorSignature: 'å¤šå·¥å…·é€‰æ‹©ã€ä»»åŠ¡é€‚é…ã€ä¿¡ä»»æ ¡å‡†',
  },
  D: {
    name: 'Deep Verification',
    description: 'æ·±åº¦æ ¸éªŒä¸æ‰¹åˆ¤æ€§ä»‹å…¥',
    necessaryConditions: 'M2=3 AND E1â‰¥2 AND totalâ‰¥20',
    sufficientConditions: 'Cross-validation or systematic verification',
    behaviorSignature: 'äº¤å‰éªŒè¯ã€æ·±åº¦æ£€æŸ¥ã€æ‰¹åˆ¤æ€§è¯„ä¼°',
  },
  E: {
    name: 'Learning-Oriented',
    description: 'æ•™å­¦åŒ–åæ€ä¸è‡ªæˆ‘ç›‘æ§',
    necessaryConditions: 'E1+E2+E3â‰¥6 AND totalâ‰¥20',
    sufficientConditions: 'All E scores â‰¥2',
    behaviorSignature: 'åæ€å­¦ä¹ ã€èƒ½åŠ›åˆ¤æ–­ã€æŒç»­æ”¹è¿›',
  },
  F: {
    name: 'Ineffective/Passive',
    description: 'æ— æ•ˆä¸è¢«åŠ¨ä½¿ç”¨',
    necessaryConditions: 'total<15 OR (P1=0 AND M2=0 AND E3=0)',
    sufficientConditions: 'â‰¥2 core subprocesses (P1,M2,E3) = 0',
    behaviorSignature: 'ç¼ºä¹è§„åˆ’ã€ä¸éªŒè¯ã€ç›²ç›®ä¿¡ä»»',
  },
};

/**
 * Analyze a single interview and generate coding
 */
function analyzeInterview(interviewId: string, content: string): InterviewAnalysis {
  console.log(`\nğŸ“‹ Analyzing ${interviewId}...`);
  console.log(`Content length: ${content.length} chars`);

  // This is a placeholder - in practice, you would:
  // 1. Use Claude API to analyze the interview
  // 2. Or perform manual analysis following the template

  // For now, return a template structure
  return {
    interviewId,
    participant: {
      background: 'To be analyzed',
      aiUsageFrequency: 'To be analyzed',
    },
    highRiskScenarios: [],
    lowRiskScenarios: [],
    subprocessCoding: Object.entries(SUBPROCESS_DEFINITIONS).map(([key, def]) => ({
      subprocess: `${key}: ${def.name}`,
      score: 0 as EvidenceStrength,
      rationale: 'To be analyzed',
      evidence: [],
    })),
    totalScore: 0,
    primaryPattern: 'F',
    confidence: 'low',
    isMixed: false,
    analysisSummary: 'Analysis pending',
  };
}

/**
 * Generate analysis prompt for Claude API
 */
function generateAnalysisPrompt(interviewId: string, content: string): string {
  return `You are an expert qualitative researcher analyzing an interview about AI usage patterns.

INTERVIEW: ${interviewId}

TRANSCRIPT:
${content}

TASK: Analyze this interview using the metacognitive framework with 12 subprocesses. Follow these steps:

1. IDENTIFY SCENARIOS
   - List 2-3 HIGH-RISK scenarios (academic papers, core code, research, critical decisions)
   - List 2-3 LOW-RISK scenarios (daily emails, simple queries, formatting)

2. CODE EACH SUBPROCESS (P1-P4, M1-M3, E1-E3, R1-R2)
   For each subprocess, assign a score 0-3:

   âœ“âœ“âœ“ (3 = STRONG): Multiple explicit statements + consistent behavior + cross-context evidence + meta-statements
   âœ“âœ“ (2 = MODERATE): Clear behavior description (at least once) OR explicit statement with concrete example
   âœ“ (1 = WEAK): Inferred from context OR brief mention without detail
   âœ— (0 = NONE): Not mentioned OR explicitly denied

   CRITICAL DISTINCTIONS:
   - "æˆ‘çŸ¥é“åº”è¯¥éªŒè¯" (aware) = 1, NOT 2
   - "æˆ‘æ¯æ¬¡éƒ½éªŒè¯" (always does) = 3
   - "æˆ‘ç»å¸¸éªŒè¯" (usually does) = 2
   - "æˆ‘æœ‰æ—¶éªŒè¯" (sometimes) = 1
   - "ä»ä¸éªŒè¯" (never) = 0

   FOCUS ON HIGH-RISK SCENARIOS for primary pattern determination.

3. SUBPROCESS DEFINITIONS:
   ${Object.entries(SUBPROCESS_DEFINITIONS).map(([key, def]) =>
     `${key} - ${def.name}: ${def.description}
     Strong(3): ${def.strongEvidence}
     Moderate(2): ${def.moderateEvidence}
     Weak(1): ${def.weakEvidence}`
   ).join('\n\n')}

4. DETERMINE PATTERN
   Primary pattern based on HIGH-RISK task behavior:
   ${Object.entries(PATTERN_DEFINITIONS).map(([key, def]) =>
     `Pattern ${key} - ${def.name}: ${def.necessaryConditions}`
   ).join('\n')}

5. OUTPUT FORMAT (JSON):
{
  "participant_background": "...",
  "ai_usage_frequency": "...",
  "high_risk_scenarios": ["scenario 1", "scenario 2"],
  "low_risk_scenarios": ["scenario 1", "scenario 2"],
  "subprocess_scores": {
    "P1": { "score": 0-3, "rationale": "...", "evidence": ["quote 1", "quote 2"] },
    "P2": { "score": 0-3, "rationale": "...", "evidence": ["..."] },
    ...all 12 subprocesses...
  },
  "total_score": 0-36,
  "primary_pattern": "A/B/C/D/E/F",
  "secondary_pattern": "optional if mixed",
  "confidence": "high/moderate/low",
  "is_mixed": true/false,
  "analysis_summary": "2-3 sentences explaining the primary pattern and key evidence"
}

Be rigorous in evidence evaluation. Prefer lower scores when uncertain.`;
}

/**
 * Process all interviews (placeholder for batch processing)
 */
function processAllInterviews(inputDir: string, outputDir: string): void {
  console.log('ğŸ”¬ LLM-Assisted Interview Coding');
  console.log('Using Claude for semantic analysis\n');

  const files = fs.readdirSync(inputDir)
    .filter(f => f.match(/^I\d{3}\.txt$/))
    .sort();

  console.log(`Found ${files.length} interviews\n`);
  console.log('INSTRUCTIONS:');
  console.log('1. This tool generates analysis prompts for Claude API');
  console.log('2. Copy each prompt to Claude or use API integration');
  console.log('3. Save responses as JSON in output directory\n');

  // Create output directory
  if (!fs.existsSync(outputDir)) {
    fs.mkdirSync(outputDir, { recursive: true });
  }

  // Generate prompts for first 5 interviews as examples
  for (let i = 0; i < Math.min(5, files.length); i++) {
    const file = files[i];
    const filePath = path.join(inputDir, file);
    const content = fs.readFileSync(filePath, 'utf-8');
    const interviewId = file.replace('.txt', '');

    const prompt = generateAnalysisPrompt(interviewId, content);
    const promptPath = path.join(outputDir, `${interviewId}_prompt.txt`);

    fs.writeFileSync(promptPath, prompt, 'utf-8');
    console.log(`âœ… Generated prompt: ${promptPath}`);
  }

  console.log(`\nğŸ“ Prompts saved to: ${outputDir}`);
  console.log('\nNEXT STEPS:');
  console.log('1. Review the generated prompts');
  console.log('2. Process through Claude (manually or via API)');
  console.log('3. Save responses as {interview_id}_analysis.json');
  console.log('4. Run aggregation script to compile results');
}

// Export for testing
export {
  analyzeInterview,
  generateAnalysisPrompt,
  SUBPROCESS_DEFINITIONS,
  PATTERN_DEFINITIONS,
};

// CLI
import { fileURLToPath } from 'url';
const __filename = fileURLToPath(import.meta.url);
const isMainModule = process.argv[1] === __filename || process.argv[1].endsWith('llm-assisted-coding.ts');

if (isMainModule) {
  const args = process.argv.slice(2);
  if (args.length < 2) {
    console.error('Usage: npx tsx llm-assisted-coding.ts <interviews-dir> <output-dir>');
    console.error('Example: npx tsx llm-assisted-coding.ts ./interviews-split ./llm-analysis');
    process.exit(1);
  }

  processAllInterviews(args[0], args[1]);
}
