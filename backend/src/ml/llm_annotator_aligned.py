#!/usr/bin/env python3
"""
LLM-based Conversation Annotator (Aligned Version)

Uses Claude/GPT as a judge to annotate conversations with
12-dimensional metacognitive subprocess scores.

KEY CHANGE: Dimensions are now aligned with theoretical framework from interviews.
- P4: Role Definition (was: Problem Decomposition)
- M3: Trust Calibration (was: Integration Effort)

Methodology: LLM-as-a-Judge with theory-aligned dimensions
References:
- Winne & Perry (2000) - Event measures of metacognition
- Azevedo et al. (2010) - Trace data inference
- Veenman et al. (2006) - Multi-method validation

Author: MCA Research Team
Date: 2024-12-03
Version: 2.0 (Aligned)
"""

import os
import sys
import json
import csv
import time
from typing import Dict, List, Optional, Tuple
from collections import defaultdict

# Try to import anthropic first, fall back to openai
try:
    import anthropic
    USE_CLAUDE = True
    client = anthropic.Anthropic(api_key=os.environ.get('ANTHROPIC_API_KEY'))
    MODEL = "claude-sonnet-4-20250514"
except ImportError:
    from openai import OpenAI
    USE_CLAUDE = False
    client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))
    MODEL = os.environ.get('AI_MODEL', 'gpt-4o')

# Configuration
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
INPUT_CSV = os.path.join(PROJECT_ROOT, 'docs/interviews/conv_history_active_users.csv')
OUTPUT_CSV = os.path.join(PROJECT_ROOT, 'backend/src/ml/llm_annotated_aligned_v2.csv')
OUTPUT_JSON = os.path.join(PROJECT_ROOT, 'backend/src/ml/llm_annotations_aligned_v2.json')
CHECKPOINT_JSON = os.path.join(PROJECT_ROOT, 'backend/src/ml/annotation_checkpoint_v2.json')

BATCH_SIZE = 10
MAX_RETRIES = 3
RETRY_DELAY = 5

# NEW ALIGNED ANNOTATION PROMPT
ANNOTATION_PROMPT_ALIGNED = """You are an expert in metacognitive assessment and educational psychology.

Your task is to analyze this user's conversation history with an AI tutor and score their metacognitive behaviors across 12 dimensions.

## IMPORTANT: Theory-Aligned Dimensions
These dimensions are derived from qualitative interview research (N=49) and aligned with metacognition theory (Zimmerman 2000, Nelson & Narens 1990).

## Scoring Scale (0-3)
- 0: No evidence of this behavior
- 1: Minimal/weak evidence
- 2: Moderate evidence
- 3: Strong/consistent evidence

## 12 Metacognitive Dimensions (ALIGNED VERSION):

### Planning (P1-P4) - Pre-action cognitive preparation:

- **P1 (Task Understanding & Analysis)**: Does the user demonstrate understanding of the task requirements?
  - 0: Vague, unclear requests ("å¸®æˆ‘å†™ä¸ªä¸œè¥¿")
  - 1: Basic task description ("å¸®æˆ‘å†™ä¸€ç¯‡AIçš„æ–‡ç« ")
  - 2: Clear requirements with constraints ("å†™2000å­—AIä¼¦ç†æ–‡ç« ï¼Œé¢å‘å¤§å­¦ç”Ÿ")
  - 3: Systematic analysis with key elements identified ("æ ¸å¿ƒè®ºç‚¹æ˜¯Xï¼Œéœ€è¦†ç›–éšç§ã€åè§ã€å°±ä¸šä¸‰æ–¹é¢ï¼Œè¯»è€…æ˜¯éæŠ€æœ¯èƒŒæ™¯...")
  - Keywords: éœ€æ±‚è¯´æ˜, èƒŒæ™¯ä»‹ç», çº¦æŸæ¡ä»¶, å—ä¼—å®šä¹‰, é¢„æœŸäº§å‡º

- **P2 (Goal Setting)**: Does the user set specific, measurable goals?
  - 0: No clear goal ("å¸®æˆ‘æ”¹è¿›ä¸€ä¸‹")
  - 1: Vague goal ("è®©å®ƒæ›´å¥½")
  - 2: Specific goal ("æŠŠå“åº”æ—¶é—´é™åˆ°200msä»¥ä¸‹")
  - 3: SMART goal - Specific, Measurable, Time-bound ("åœ¨ä¿æŒ95%å‡†ç¡®ç‡å‰æä¸‹ï¼Œå°†å“åº”æ—¶é—´ä»500msé™è‡³200msï¼Œä»Šå¤©å®Œæˆ")
  - Keywords: ç›®æ ‡æ˜¯, å¸Œæœ›è¾¾åˆ°, æˆåŠŸæ ‡å‡†, éªŒæ”¶æ¡ä»¶, å…·ä½“æŒ‡æ ‡

- **P3 (Strategy Selection & Planning)**: Does the user show strategic thinking?
  - 0: No strategy, direct questioning ("æ€ä¹ˆåšXï¼Ÿ")
  - 1: Implicit simple strategy ("å…ˆå¸®æˆ‘åˆ—ä¸ªå¤§çº²")
  - 2: Explicit multi-step plan ("æˆ‘æ‰“ç®—åˆ†ä¸‰æ­¥ï¼š1)ç†æ¸…éœ€æ±‚ 2)è®¾è®¡æ¶æ„ 3)å®ç°ä»£ç ")
  - 3: Systematic strategy with alternatives ("æˆ‘çš„è®¡åˆ’æ˜¯...ï¼Œå¦‚æœä¸è¡Œå¯ä»¥å°è¯•...ï¼Œä½ è§‰å¾—åˆç†å—ï¼Ÿ")
  - Keywords: è®¡åˆ’, æ­¥éª¤, å…ˆ...å†..., ç­–ç•¥, æ–¹æ³•, å¦‚æœ...å°±...

- **P4 (Role Definition)**: Does the user define AI's role and human-AI boundaries? [CRITICAL - NEW ALIGNMENT]
  - 0: No role definition (just asks questions directly)
  - 1: Implicit role expectation ("å¸®æˆ‘æ£€æŸ¥ä»£ç " implies AI as reviewer)
  - 2: Explicit role instruction ("ä½ æ˜¯èµ„æ·±Pythonå¼€å‘è€…ï¼Œè¯·ä»ä»£ç è´¨é‡è§’åº¦ç»™å»ºè®®")
  - 3: Systematic role + boundary definition ("ä½ ä½œä¸ºæŠ€æœ¯é¡¾é—®æä¾›å»ºè®®ï¼Œæœ€ç»ˆå†³ç­–ç”±æˆ‘æ¥åšã€‚è¯·ä¸è¦ç›´æ¥æ”¹ä»£ç ï¼Œåªç»™å»ºè®®")
  - Keywords: ä½ æ˜¯, ä½ çš„è§’è‰², ä½ è´Ÿè´£, æˆ‘è´Ÿè´£, è¯·ä¸è¦, è¾¹ç•Œ, åˆ†å·¥

### Monitoring (M1-M3) - Execution phase tracking:

- **M1 (Process Tracking)**: Does the user track progress and confirm milestones?
  - 0: No tracking, one-shot interaction (single Q&A then leaves)
  - 1: Occasional confirmation ("å¥½çš„ï¼Œç»§ç»­")
  - 2: Stage-wise checking ("ç¬¬ä¸€éƒ¨åˆ†å®Œæˆäº†ï¼Œè®©æˆ‘ç¡®è®¤ä¸€ä¸‹...å¥½çš„æ²¡é—®é¢˜ï¼Œè¿›å…¥ç¬¬äºŒéƒ¨åˆ†")
  - 3: Systematic progress tracking ("ç›®å‰å®Œæˆ3/5æ¨¡å—ï¼Œè¿›åº¦60%ã€‚ç¬¬ä¸‰ä¸ªæœ‰é—®é¢˜éœ€è¦å…ˆè§£å†³...")
  - Keywords: è¿›åº¦, å®Œæˆäº†, æ¥ä¸‹æ¥, ç›®å‰, é˜¶æ®µ, æ£€æŸ¥ç‚¹, å›é¡¾

- **M2 (Quality Checking)**: Does the user actively check and request modifications?
  - 0: Accepts everything without checking ("å¥½çš„è°¢è°¢" then ends)
  - 1: Occasional modification requests ("è¿™é‡Œæ”¹ä¸€ä¸‹")
  - 2: Actively points out issues ("ç¬¬äºŒæ®µé€»è¾‘æœ‰é—®é¢˜ï¼Œå› ä¸º...ï¼Œè¯·ä¿®æ­£")
  - 3: Systematic quality review ("è®©æˆ‘é€æ¡æ£€æŸ¥ï¼š1)å‡†ç¡®æ€§-æœ‰é”™è¯¯... 2)å®Œæ•´æ€§-ç¼ºå°‘X 3)é€»è¾‘æ€§-æœ‰æ¼æ´...")
  - Keywords: ä¸å¯¹, æœ‰é—®é¢˜, è¯·ä¿®æ”¹, æ£€æŸ¥, æ ¸å®, è¿™é‡Œé”™äº†, ä¸ºä»€ä¹ˆ

- **M3 (Trust Calibration)**: Does the user adjust trust based on context? [CRITICAL - NEW ALIGNMENT]
  - 0: Fixed trust (always trusts OR always distrusts)
  - 1: Occasional trust/doubt expression ("è¿™ä¸ªæˆ‘ä¸å¤ªç¡®å®šå¯¹ä¸å¯¹")
  - 2: Context-sensitive trust adjustment ("ä»£ç éƒ¨åˆ†æˆ‘æ¯”è¾ƒä¿¡ä»»ä½ ï¼Œä½†è¿™ä¸ªåŒ»å­¦å»ºè®®æˆ‘éœ€è¦å†æŸ¥è¯")
  - 3: Systematic trust calibration ("äº‹å®æ€§é—®é¢˜æˆ‘ä¼šäº¤å‰éªŒè¯ï¼Œåˆ›æ„å»ºè®®æˆ‘æ›´æ„¿æ„é‡‡çº³ï¼Œä¸“ä¸šåˆ¤æ–­æˆ‘æŒä¿ç•™æ€åº¦")
  - Keywords: ä¿¡ä»», ç›¸ä¿¡, æ€€ç–‘, ä¸ç¡®å®š, éœ€è¦éªŒè¯, è¿™ä¸ªé¢†åŸŸä½ æ“…é•¿å—

### Evaluation (E1-E3) - Quality judgment:

- **E1 (Quality Evaluation)**: Does the user evaluate output quality?
  - 0: No evaluation expressed
  - 1: Simple good/bad judgment ("ä¸é”™" / "ä¸è¡Œ")
  - 2: Reasoned evaluation ("æ–¹æ¡ˆä¸é”™å› ä¸ºè€ƒè™‘äº†Xå’ŒYï¼Œä½†Zè¿˜å¯ä»¥æ”¹è¿›")
  - 3: Multi-dimensional systematic assessment ("å‡†ç¡®æ€§8/10ï¼Œå®Œæ•´æ€§7/10ï¼Œå¯ç”¨æ€§9/10ï¼Œæ•´ä½“è¯„ä»·...")
  - Keywords: å¥½, ä¸å¥½, è¯„åˆ†, æ»¡æ„, è´¨é‡, è¾¾åˆ°é¢„æœŸ, ç¬¦åˆè¦æ±‚

- **E2 (Risk Assessment)**: Does the user consider potential risks and consequences?
  - 0: No risk awareness (just uses output directly)
  - 1: Implicit risk awareness ("è¿™ä¸ªåº”è¯¥æ²¡é—®é¢˜å§ï¼Ÿ")
  - 2: Explicit risk identification ("è¿™æ–¹æ¡ˆæœ‰é£é™©ï¼šå¦‚æœXå‘ç”Ÿï¼Œå¯èƒ½å¯¼è‡´Y")
  - 3: Systematic risk assessment ("è¯„ä¼°é£é™©ï¼šæŠ€æœ¯é£é™©-ä½å› ä¸º...ï¼›ä¸šåŠ¡é£é™©-ä¸­å› ä¸º...ï¼›ç¼“è§£æªæ–½æ˜¯...")
  - Keywords: é£é™©, é—®é¢˜, å¦‚æœå‡ºé”™, åæœ, å®‰å…¨å—, ä¼šä¸ä¼š, ä¸‡ä¸€

- **E3 (Capability Judgment)**: Does the user understand AI's capability boundaries?
  - 0: No boundary awareness (expects AI to perfectly answer everything)
  - 1: Vague capability awareness ("ä½ èƒ½åšè¿™ä¸ªå—ï¼Ÿ")
  - 2: Clear boundary understanding ("è¿™æ¶‰åŠæœ€æ–°æ•°æ®ï¼Œå¯èƒ½è¶…å‡ºä½ çš„çŸ¥è¯†èŒƒå›´")
  - 3: Systematic capability mapping ("ä½ æ“…é•¿ä»£ç é€»è¾‘ï¼Œä½†å®æ—¶æ•°æ®å’Œä¸“ä¸šåˆ¤æ–­éœ€è¦æˆ‘è¡¥å……ã€‚Aéƒ¨åˆ†äº¤ç»™ä½ ï¼ŒBéƒ¨åˆ†æˆ‘å¤„ç†")
  - Keywords: ä½ èƒ½, ä½ æ“…é•¿, ä½ çš„å±€é™, æˆ‘è‡ªå·±æ¥, è¿™ä¸ªä½ å¯èƒ½ä¸çŸ¥é“, çŸ¥è¯†æˆªæ­¢

### Regulation (R1-R2) - Adaptive adjustment:

- **R1 (Strategy Adjustment)**: Does the user adjust strategies based on feedback?
  - 0: No adjustment, repeats same approach
  - 1: Passive adjustment (after AI suggests) ("å¥½çš„ï¼ŒæŒ‰ä½ è¯´çš„æ–¹å¼é‡æ–°é—®")
  - 2: Active strategy change ("è¿™ä¸ªæ–¹æ³•æ•ˆæœä¸å¥½ï¼Œæˆ‘æ¢ä¸ªè§’åº¦é—®...")
  - 3: Systematic strategy optimization ("å‰ä¸¤æ¬¡é—®é¢˜æ˜¯æç¤ºè¯å¤ªæ¨¡ç³Šã€‚æˆ‘æ€»ç»“äº†æ›´æœ‰æ•ˆçš„æ¨¡å¼ï¼šå…ˆèƒŒæ™¯ï¼Œå†éœ€æ±‚ï¼Œæœ€åçº¦æŸ")
  - Keywords: æ¢ä¸ªæ–¹å¼, é‡æ–°å°è¯•, è°ƒæ•´, æ”¹å˜ç­–ç•¥, ä¸Šæ¬¡çš„é—®é¢˜æ˜¯, è¿™æ¬¡æˆ‘

- **R2 (Tool Switching)**: Does the user flexibly switch tools/methods?
  - 0: Single tool dependence (only uses this one AI)
  - 1: Awareness of alternatives ("ä¹Ÿè®¸æˆ‘åº”è¯¥å»GoogleæŸ¥ä¸€ä¸‹")
  - 2: Active tool switching ("ChatGPTç­”å¾—ä¸å¥½ï¼Œæˆ‘å»é—®Claudeè¯•è¯•" / "è®©æˆ‘ç”¨ä»£ç éªŒè¯è¿™ä¸ªç»“æœ")
  - 3: Systematic tool combination ("éªŒè¯æµç¨‹ï¼š1)AIåˆç¨¿ 2)Google Scholaræ ¸æŸ¥ 3)ä¸“ä¸šè®ºå›éªŒè¯ 4)è¿è¡Œæµ‹è¯•ä»£ç ")
  - Keywords: å…¶ä»–å·¥å…·, æ¢ä¸€ä¸ª, éªŒè¯, Google, æœç´¢, æµ‹è¯•, è¿è¡Œ, å¯¹æ¯”

## Pattern Classification Rules:
Based on scores, classify into one of these patterns:

- **Pattern A (Strategic Decomposition)**: High P scores (avg â‰¥ 2.5), especially P3 and P4
- **Pattern B (Iterative Refinement)**: High M scores (avg â‰¥ 2), especially M1 and M2
- **Pattern C (Moderate Balanced)**: Balanced moderate scores across all dimensions
- **Pattern D (Critical Evaluation)**: High E scores (avg â‰¥ 2.5), especially E1 and E2
- **Pattern E (Pedagogical Reflection)**: High R scores (avg â‰¥ 2.5), balanced high P and E
- **Pattern F (Passive Over-Reliance)**: CRITICAL - Low total score (â‰¤15) AND E1 â‰¤ 1

## CONVERSATION HISTORY TO ANALYZE:

{conversation_history}

## OUTPUT FORMAT:
Respond with ONLY a JSON object (no markdown, no explanation):
{{
  "scores": {{
    "P1": <0-3>,
    "P2": <0-3>,
    "P3": <0-3>,
    "P4": <0-3>,
    "M1": <0-3>,
    "M2": <0-3>,
    "M3": <0-3>,
    "E1": <0-3>,
    "E2": <0-3>,
    "E3": <0-3>,
    "R1": <0-3>,
    "R2": <0-3>
  }},
  "pattern": "<A|B|C|D|E|F>",
  "confidence": <0.0-1.0>,
  "reasoning": "<brief explanation of pattern assignment>"
}}
"""


def load_conversations(csv_path: str) -> Dict[str, List[Dict]]:
    """Load and group conversations by user"""
    print(f"ğŸ“¥ Loading conversations from {csv_path}...")

    user_conversations = defaultdict(list)

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            user_id = row.get('hash_id', '')
            if user_id:
                user_conversations[user_id].append({
                    'sender': row.get('sender', ''),
                    'text': row.get('text', ''),
                    'timestamp': row.get('timestamp', '')
                })

    print(f"âœ… Loaded conversations for {len(user_conversations)} users")
    return dict(user_conversations)


def format_conversation_for_annotation(messages: List[Dict], max_chars: int = 8000) -> str:
    """Format conversation for LLM annotation"""
    formatted = []
    total_chars = 0

    for msg in messages:
        sender = msg.get('sender', 'unknown')
        text = msg.get('text', '')

        if not text.strip():
            continue

        # Truncate long messages
        if len(text) > 500:
            text = text[:500] + "..."

        line = f"[{sender}]: {text}"

        if total_chars + len(line) > max_chars:
            formatted.append("... (conversation truncated)")
            break

        formatted.append(line)
        total_chars += len(line)

    return "\n".join(formatted)


def annotate_with_llm(conversation_text: str) -> Optional[Dict]:
    """Call LLM to annotate conversation"""
    prompt = ANNOTATION_PROMPT_ALIGNED.format(conversation_history=conversation_text)

    for attempt in range(MAX_RETRIES):
        try:
            if USE_CLAUDE:
                response = client.messages.create(
                    model=MODEL,
                    max_tokens=1000,
                    messages=[{"role": "user", "content": prompt}]
                )
                content = response.content[0].text
            else:
                response = client.chat.completions.create(
                    model=MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=1000,
                    temperature=0.1
                )
                content = response.choices[0].message.content

            # Parse JSON response
            content = content.strip()
            if content.startswith("```"):
                content = content.split("```")[1]
                if content.startswith("json"):
                    content = content[4:]

            result = json.loads(content)
            return result

        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON parse error (attempt {attempt+1}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)
        except Exception as e:
            print(f"âš ï¸ API error (attempt {attempt+1}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

    return None


def load_checkpoint() -> Dict:
    """Load annotation checkpoint"""
    if os.path.exists(CHECKPOINT_JSON):
        with open(CHECKPOINT_JSON, 'r') as f:
            return json.load(f)
    return {"completed_users": [], "annotations": {}}


def save_checkpoint(checkpoint: Dict):
    """Save annotation checkpoint"""
    with open(CHECKPOINT_JSON, 'w') as f:
        json.dump(checkpoint, f, indent=2)


def main():
    print("=" * 60)
    print("ğŸ”„ LLM Annotator (ALIGNED VERSION 2.0)")
    print("=" * 60)
    print("Key changes from v1.0:")
    print("  - P4: Problem Decomposition â†’ Role Definition")
    print("  - M3: Integration Effort â†’ Trust Calibration")
    print("=" * 60)

    # Load conversations
    conversations = load_conversations(INPUT_CSV)

    # Load checkpoint
    checkpoint = load_checkpoint()
    completed_users = set(checkpoint.get("completed_users", []))
    annotations = checkpoint.get("annotations", {})

    # Filter users to annotate
    users_to_annotate = [u for u in conversations.keys() if u not in completed_users]

    print(f"\nğŸ“Š Progress: {len(completed_users)}/{len(conversations)} users annotated")
    print(f"ğŸ“ Remaining: {len(users_to_annotate)} users")

    if not users_to_annotate:
        print("âœ… All users already annotated!")
    else:
        # Annotate remaining users
        for i, user_id in enumerate(users_to_annotate):
            print(f"\n[{i+1}/{len(users_to_annotate)}] Annotating user {user_id[:8]}...")

            # Format conversation
            conv_text = format_conversation_for_annotation(conversations[user_id])

            if len(conv_text) < 50:
                print(f"  âš ï¸ Conversation too short, skipping")
                annotations[user_id] = {
                    "scores": {"P1":0,"P2":0,"P3":0,"P4":0,"M1":0,"M2":0,"M3":0,"E1":0,"E2":0,"E3":0,"R1":0,"R2":0},
                    "pattern": "F",
                    "confidence": 0.5,
                    "reasoning": "Conversation too short to analyze"
                }
            else:
                # Call LLM
                result = annotate_with_llm(conv_text)

                if result:
                    annotations[user_id] = result
                    print(f"  âœ… Pattern: {result.get('pattern')} (conf: {result.get('confidence', 0):.2f})")
                else:
                    print(f"  âŒ Annotation failed")
                    annotations[user_id] = {
                        "scores": {"P1":0,"P2":0,"P3":0,"P4":0,"M1":0,"M2":0,"M3":0,"E1":0,"E2":0,"E3":0,"R1":0,"R2":0},
                        "pattern": "F",
                        "confidence": 0.5,
                        "reasoning": "Annotation failed"
                    }

            completed_users.add(user_id)

            # Save checkpoint every BATCH_SIZE users
            if (i + 1) % BATCH_SIZE == 0:
                checkpoint = {"completed_users": list(completed_users), "annotations": annotations}
                save_checkpoint(checkpoint)
                print(f"  ğŸ’¾ Checkpoint saved ({len(completed_users)} users)")

            # Rate limiting
            time.sleep(0.5)

    # Save final results
    print("\nğŸ’¾ Saving final results...")

    # Save detailed JSON
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(annotations, f, indent=2, ensure_ascii=False)

    # Save CSV for training
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['user_id', 'pattern', 'confidence',
                        'p1', 'p2', 'p3', 'p4', 'm1', 'm2', 'm3',
                        'e1', 'e2', 'e3', 'r1', 'r2', 'total_score',
                        'is_mixed_pattern', 'notes'])

        for user_id, ann in annotations.items():
            scores = ann.get('scores', {})
            total = sum(scores.values())
            writer.writerow([
                user_id,
                ann.get('pattern', 'F'),
                ann.get('confidence', 0.5),
                scores.get('P1', 0), scores.get('P2', 0), scores.get('P3', 0), scores.get('P4', 0),
                scores.get('M1', 0), scores.get('M2', 0), scores.get('M3', 0),
                scores.get('E1', 0), scores.get('E2', 0), scores.get('E3', 0),
                scores.get('R1', 0), scores.get('R2', 0),
                total,
                'false',
                f"LLM-annotated-aligned-v2 (confidence: {ann.get('confidence', 0.5)})"
            ])

    print(f"âœ… Saved {len(annotations)} annotations to {OUTPUT_CSV}")

    # Print distribution
    pattern_counts = defaultdict(int)
    for ann in annotations.values():
        pattern_counts[ann.get('pattern', 'F')] += 1

    print("\nğŸ“Š Pattern Distribution:")
    for pattern in sorted(pattern_counts.keys()):
        count = pattern_counts[pattern]
        pct = count / len(annotations) * 100
        print(f"  Pattern {pattern}: {count} ({pct:.1f}%)")


if __name__ == "__main__":
    main()
