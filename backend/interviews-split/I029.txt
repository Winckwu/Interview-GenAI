受访人29:
WEBVTT

1
00:00:00.000 --> 00:00:01.330
Norris: 继续来吧。

2
00:00:02.790 --> 00:00:07.780
Fan Yunjie: 你是很有话语权的人啊，看来谁都要找你聊聊天。

3
00:00:07.780 --> 00:00:13.740
Norris: 不是的，单纯就是开会，我讲了一遍，然后他们没记住，说，能再来一遍吗？我说，你赶紧开录像。

4
00:00:14.410 --> 00:00:17.590
Fan Yunjie: 不是你，你的下属这样子吗？

5
00:00:18.090 --> 00:00:19.180
Fan Yunjie: 哇，你，你说。

6
00:00:19.180 --> 00:00:24.950
Norris: 不算是下属吧？我，我们没有人比谁低的，就是你也知道创业公司嘛。

7
00:00:27.010 --> 00:00:33.520
Fan Yunjie: 那你那你现在的这个项目你方便跟我介绍一些吗？因为我看到官网其实。

8
00:00:33.520 --> 00:00:39.720
Norris: 可以啊，就是官网上面不是有一个比较模糊的小动画嘛，那个动画我也不知道为什么不会搞清的就是

9
00:00:41.610 --> 00:00:44.920
Norris: 简单来讲呢，就是因为我自己

10
00:00:44.920 --> 00:01:09.630
Norris: 发薪是我同时管理很多家公司，然后我自己的个人生活，所以我邮件就很混，邮件，很混的这个结果就是我每天早上的工作流是先去开每一个邮箱，然后挨个过，因为我很怕就是错过一些比较重要的，我就要把没有用的先删掉，或者是给它已读掉，然后有用的，我看它有用，

11
00:01:09.630 --> 00:01:13.840
Norris: 然后我再把它给变成未读或者是加一个flag，我就不知。

12
00:01:13.840 --> 00:01:14.370
Fan Yunjie: 对呀，你。

13
00:01:14.370 --> 00:01:27.680
Norris: 走这么一遍之后呢？我再回来去看我有什么事情要回，然后把能回的就回掉。然后如果有后续的需要我再做什么事情，准备什么文件的，我再一个一个做掉就是这个工作有很漫长。

14
00:01:27.700 --> 00:01:46.050
Norris: 然后我就幻想说，能不能有一个工具把它给统一一下。我当然尝试了非常多的就是那种邮件整合工具，像什么superhan，像什么这个notion male not刚出嘛，还有spark就是整合在一起，也并不解决这个问题，因为内容还是需要我自己去读的，

15
00:01:46.110 --> 00:01:48.270
Norris: 所以我就

16
00:01:48.360 --> 00:02:01.280
Norris: 开发了这么一套系统，让ai去把我所有的邮件挨个读一遍，不管是垃圾邮件啊，还是诈骗邮件啊，还是广告啊？还是钓鱼啊？或者是正式的邮件啊，都读一遍读一遍之后呢？他自然就知道

17
00:02:01.280 --> 00:02:09.500
Norris: 哪些是有用的，哪些是没用的，哪些与我的相关性更强？哪些与我的完全无关，那么哪一些还是

18
00:02:09.500 --> 00:02:12.780
Norris: 就是一个类别的，它们是一起的，

19
00:02:12.810 --> 00:02:16.230
Norris: 那都可以归类，就是他可以把这些工作都做掉，

20
00:02:16.380 --> 00:02:23.080
Norris: 那我有了这个工具之后，我就可以每天只看他给我总结出来的一些智能卡片就可以了。

21
00:02:24.270 --> 00:02:31.270
Fan Yunjie: Okay，那那你现在是。有有把任何的这个api接进来吗？还是说。

22
00:02:31.760 --> 00:02:38.060
Norris: 我只接了谷歌的api，就是我把邮件获取过来呀，因为我邮件是数据源头嘛。

23
00:02:38.620 --> 00:02:40.450
Fan Yunjie: 那用的是什么模型啊？

24
00:02:40.710 --> 00:02:47.080
Norris: 用的是jamline和这个jamai，还有一个hbt。

25
00:02:48.420 --> 00:02:50.920
Fan Yunjie: 那你觉得你有多大程度上相信

26
00:02:51.060 --> 00:02:52.060
Fan Yunjie: 就是

27
00:02:52.190 --> 00:02:55.710
Fan Yunjie: 他给你管理的这种这种导向。

28
00:02:59.890 --> 00:03:00.650
Norris: 目前。

29
00:03:00.650 --> 00:03:01.600
Fan Yunjie: 他有多大，不可能。

30
00:03:01.600 --> 00:03:10.670
Norris: 使用过程中要分情况，如果是过滤我的这种垃圾邮件，钓鱼邮件，它的确是做到了百分之百。

31
00:03:10.930 --> 00:03:24.010
Norris: 然后如果说是识别我的工作，它在归类方面还不是很准，但是它的确是能帮我把里边的日程。我的机票，我的酒店

32
00:03:24.190 --> 00:03:32.790
Norris: 就是具体的时间，地点都帮我提的比较准确，包括时区也会自动换算就是我感觉还是比较靠谱的吧。

33
00:03:33.340 --> 00:03:37.980
Fan Yunjie: 明白，所以一开始的话，就是因为自己觉得这个东西

34
00:03:38.160 --> 00:03:45.240
Fan Yunjie: 比较有比较大的这个可能性，解决你的你的这个效率。所以说，想要创立这样的团队。

35
00:03:45.490 --> 00:03:46.580
Norris: 是的。

36
00:03:47.190 --> 00:03:50.500
Fan Yunjie: 他是从大概多久之前的做这个事情啊。

37
00:03:50.500 --> 00:03:51.340
Norris: 一年。

38
00:03:52.190 --> 00:03:56.690
Fan Yunjie: 那现在的团队规模也没有啊，十个人。

39
00:03:57.840 --> 00:04:01.520
Fan Yunjie: 然后现在的这个状况就是大部分是推。

40
00:04:01.520 --> 00:04:04.550
Norris: 现在状况就是没有营收，然后，

41
00:04:04.840 --> 00:04:07.320
Norris: 然后也没有融资，然后。

42
00:04:08.200 --> 00:04:10.230
Norris: 就全靠我自己撑着。

43
00:04:11.750 --> 00:04:14.540
Fan Yunjie: 那说明你很有实力啊，我不能说。

44
00:04:14.790 --> 00:04:19.260
Norris: 我，我，我我还行吧，但是

45
00:04:19.380 --> 00:04:23.550
Norris: 就就是肯定没有把这个团队做很大嘛，

46
00:04:23.690 --> 00:04:26.100
Norris: 要不然有实力的话就做更大了。

47
00:04:27.160 --> 00:04:32.690
Fan Yunjie: 那你如果你认为的话就是他最核心的，或者说他

48
00:04:32.980 --> 00:04:35.680
Fan Yunjie: 最优势的是处理哪些任务啊？

49
00:04:36.080 --> 00:04:59.730
Norris: 最优势的，最优势的是处理日程类的，就是我们有一个功能，比如说你下我们的手机app，然后你们需要看到一个poster，或者是你朋友在给你微信上发说，我们下午三点去喝个茶，就是所有这些东西只要涉及到时间，地点的一截图丢给它，它自动就帮你生成具体的calendar entry同步到你的邮箱的

50
00:04:59.730 --> 00:05:02.720
Norris: 同步到你的手机的上，就是。

51
00:05:03.270 --> 00:05:16.780
Norris: 它会做到这种全自动化，那么第二个优点也是自动化，就是包括邮件里面提到的。assign给你的一些assignment的一些工作都会变成你的，

52
00:05:17.110 --> 00:05:31.620
Norris: 然后也可以就是查找相关的邮件，比如说abcde五封邮件acc是相关的，他就可以帮你把这三个邮件总和成一个小的notes，然后直接读这个notes就像读了三封邮件。

53
00:05:33.050 --> 00:05:39.130
Fan Yunjie: 现在这种我没有太了解这种有相对来说竞品吗？比较牛的。

54
00:05:39.740 --> 00:05:40.550
Norris: 没有。

55
00:05:41.630 --> 00:05:49.990
Fan Yunjie: 那他其实还是比较多的解决一些就like unique proposition这样的，其实它还不错的呀。

56
00:05:50.670 --> 00:05:59.720
Norris: 我们应该是比较有edge的吧，只是说没钱就是我现在没有钱去宣传它，所以就还处于一个steel smote。

57
00:06:00.600 --> 00:06:03.710
Fan Yunjie: 那你是从什么时候开始就是

58
00:06:03.920 --> 00:06:08.490
Fan Yunjie: 接触到ai，或者说是开始就involve in到ai的呢？

59
00:06:08.890 --> 00:06:12.360
Norris: Involve ai的话应该是charge发布就involve了吧。

60
00:06:12.780 --> 00:06:16.510
Fan Yunjie: 你的意思是就是充当一个用户吗？还是说。

61
00:06:16.790 --> 00:06:21.810
Norris: 他充当了一个用户，然后开始开发ai的话，这应该是我的第四个产品了。

62
00:06:22.360 --> 00:06:23.990
Fan Yunjie: 之前的话也是都是。

63
00:06:23.990 --> 00:06:25.490
Norris: 尝试了三次都失败了

64
00:06:25.600 --> 00:06:26.410
Norris: 啊，刚好。

65
00:06:26.410 --> 00:06:30.150
Fan Yunjie: 也都是在做这种类似plugin，或者说是

66
00:06:30.350 --> 00:06:35.860
Fan Yunjie: 解决一些人机交互问题的这一类的产品吗？

67
00:06:36.310 --> 00:06:44.970
Norris: 没有我之前做的比较大，就是比较成型的一个产品，也拿到了。Vc的一个产品是做分布式计算的，基础框架的。

68
00:06:45.160 --> 00:06:47.420
Fan Yunjie: 喔。我说底层的。

69
00:06:47.420 --> 00:06:48.750
Norris: 多底层的infer。

70
00:06:50.070 --> 00:06:57.990
Fan Yunjie: 那您现在还会经常用ai吗？写作还是说基本上已经就停留在就是团队产品的层面上了。

71
00:06:57.990 --> 00:07:06.150
Norris: 没有啊，我个人生活的话，离不开ai的，就每天我跟ai的交互的token，绝对这20块钱的plus物超所值。

72
00:07:06.150 --> 00:07:06.770
Fan Yunjie: 哈哈哈

73
00:07:07.860 --> 00:07:11.020
Fan Yunjie: okay，大部分你都用它做什么。

74
00:07:11.500 --> 00:07:12.730
Norris: 大部分写代码。

75
00:07:13.200 --> 00:07:15.100
Fan Yunjie: 大部分写代码，那你说。

76
00:07:15.100 --> 00:07:16.140
Norris: 部分查资料。

77
00:07:16.450 --> 00:07:18.880
Fan Yunjie: 啊？你学过prompt engineering吗？

78
00:07:18.880 --> 00:07:20.360
Norris: 学没有学过啊。

79
00:07:20.360 --> 00:07:23.810
Fan Yunjie: 没有学过okay，所以说你觉得

80
00:07:24.030 --> 00:07:29.980
Fan Yunjie: 多大程度之上你会依赖ai在你的日常生活，like a percentage。

81
00:07:30.800 --> 00:07:33.820
Norris: 30%.

82
00:07:34.640 --> 00:07:38.040
Fan Yunjie: 30%，那也还好啊，其实也没有很依赖，

83
00:07:38.240 --> 00:07:44.210
Fan Yunjie: 没有没有更多的依赖的原因是因为你觉得他还是有很大程度上不可信吗？还是说。

84
00:07:44.210 --> 00:07:48.410
Norris: 不是不是单纯单纯，就是他还没有cater for其他的expect。

85
00:07:51.020 --> 00:07:52.440
Fan Yunjie: 明白。

86
00:07:52.590 --> 00:08:00.580
Fan Yunjie: 所以我想知道一下你以你个人用户来说的话，你大概跟ai互动的一个一个flow是怎么样的。

87
00:08:01.260 --> 00:08:07.320
Norris: 基本就是我想到一个问题，如果我本来可以用谷歌查的，我会变成去问查gpt，

88
00:08:07.430 --> 00:08:13.190
Norris: 因为我不知道关键词是什么，所以我就用自然语言去替代了我的关键词。

89
00:08:15.750 --> 00:08:21.800
Fan Yunjie: 那你觉得这个会影响你对于你自己的这个产品未来迭代的很多判断吗？

90
00:08:22.480 --> 00:08:24.540
Norris: 跟我的产品关系不大。

91
00:08:24.540 --> 00:08:33.990
Fan Yunjie: 跟你产品关系不大，你们，你为什么现在还会有什么在写代码的问题啊，是你现在也在这个产品，或者说是技术，这里充当什么角色吗？

92
00:08:33.990 --> 00:08:39.620
Norris: 没有，我是还有personal project，然后没有人帮我写的。我自己在写。

93
00:08:39.620 --> 00:08:41.070
Fan Yunjie: 是这样。

94
00:08:41.510 --> 00:08:43.059
Fan Yunjie: 你这学什么专业的呀。

95
00:08:43.380 --> 00:08:44.320
Norris: 有土木的。

96
00:08:46.340 --> 00:08:51.870
Fan Yunjie: 你是在美国吗？还是说还是说有有有有尝试，不同的市场。

97
00:08:52.450 --> 00:08:58.530
Norris: 市场的话，我来美国之前就是在澳大利亚，然后。

98
00:08:58.990 --> 00:09:04.700
Norris: 在澳洲当时还没有ai吗？我来美国的时候是2021年疫情期间，

99
00:09:04.910 --> 00:09:16.380
Norris: 那个时候是做了一款游戏产品，然后本来是打向澳洲市场的，结果澳洲那边就是人太少了，没什么市场，正好美国这边爆了，我就我就搬家到美国来了。

100
00:09:16.840 --> 00:09:22.310
Fan Yunjie: 那那个，那个产品是是相关的吗？跟five还是纯gam。

101
00:09:22.560 --> 00:09:31.780
Norris: 没有纯game相关的一个游戏平台，我发给你。目前目前应该也是全世界除了中国以外最大的游戏陪玩平台吧。

102
00:09:32.140 --> 00:09:35.140
Fan Yunjie: 这么牛的我对游戏不是很了解。

103
00:09:35.140 --> 00:09:46.590
Norris: 也就是游戏社交。你可以理解为一个。擦边的平台，它虽然涉黄就是，但是很多人可能就是冲著这个裸露的。

104
00:09:46.590 --> 00:09:48.480
Fan Yunjie: 带点干部的属性，是吗？

105
00:09:49.210 --> 00:09:50.310
Norris: 有一点什么。

106
00:09:50.310 --> 00:09:53.390
Fan Yunjie: 就是有点赌博的属性在是吗？

107
00:09:53.390 --> 00:09:54.490
Norris: 没有没有完全没有。

108
00:09:55.800 --> 00:09:59.800
Fan Yunjie: 明白ok，我们回来这里吧，回来这里谈回到ai。

109
00:09:59.830 --> 00:10:18.490
Fan Yunjie: 所以说就是你觉得ai，因为你是经历过前期的，不是ai相关的这种。这种时代跨到ai之后，你觉得你个人在你自己的工作，或者说你自己的产品，你的思维会发生多大的改变呢？

110
00:10:19.200 --> 00:10:21.060
Norris: 没听这个问题。

111
00:10:21.470 --> 00:10:33.720
Fan Yunjie: 就是来你现在去思考一个project，或者说你现在要去开始去进行一个项目的时候，你在ai的这个时代之前跟ai时代之后，你会有很大的差别吗？

112
00:10:33.850 --> 00:10:35.400
Norris: 呵，差别太大了，

113
00:10:36.060 --> 00:10:45.810
Norris: 就基本上你一个产品立项，你肯定要做前期的这个调研工作吧。不管是市场调研还是产品可行性调研。

114
00:10:46.150 --> 00:10:52.840
Norris: 所有的这些工作现在全都可以用ai代替了，之前需要去做客户访谈，现在不需要了，因为你会asse

115
00:10:52.970 --> 00:10:56.430
Norris: ai在训练的过程中已经有了这些人的input。

116
00:10:58.210 --> 00:11:01.280
Fan Yunjie: 但是你又有多大程度上相信他呢？

117
00:11:11.100 --> 00:11:17.610
Norris: 我一直感觉这个相信不相信的问题没有那么重要，因为世界的真相都不是很重要。

118
00:11:17.610 --> 00:11:20.750
Fan Yunjie: Okay,

119
00:11:21.210 --> 00:11:24.880
Fan Yunjie: 对，这个就是很大的问题在于，这个差别啊，就是我们。

120
00:11:24.880 --> 00:11:25.220
Norris: 我们。

121
00:11:25.220 --> 00:11:41.380
Fan Yunjie: 我们在采访一些就是business level和这个的人对于这个事情的可信性，比方说我自己啊，我之前一直是做做做工作，做企业的，你也知道我之前做什么呢？

122
00:11:41.590 --> 00:11:42.790
Fan Yunjie: 你知道吗？

123
00:11:42.790 --> 00:11:44.110
Norris: 不知道啊，金融吗？

124
00:11:44.680 --> 00:12:02.660
Fan Yunjie: 我回头会跟你说吧，然后。然后等到我进到这个这个比较学术的领域之后，我就发现大家对于这可行性的问题很追究，很咬文嚼字，一定要觉得他是要追溯的，追溯那个根源的

125
00:12:02.660 --> 00:12:21.710
Fan Yunjie: 要去看啊，我，我这个东西输出来的这些文本，每一个字，它的这个应用来源，它本身的这个dui是不是真正的是是一个可信性，还是说ai在骗我？还是说我就是不小心的进入到了这种ai的环境当中，然后被它去去

126
00:12:21.710 --> 00:12:24.470
Fan Yunjie: 去牵著鼻子走，这样子所以。

127
00:12:24.470 --> 00:12:29.530
Norris: 我感觉看就是人对于世界的认知的差别了，学术上可能比较较真。

128
00:12:30.110 --> 00:12:38.360
Fan Yunjie: 但是你会有这种困扰吗？就比方说，ai也许会把你牵到一个正好相左的方向，然后你就跟著他走了。

129
00:12:38.860 --> 00:12:42.080
Norris: 我不会啊，就是ai说错了，我肯定知道啊，

130
00:12:43.060 --> 00:12:48.070
Norris: 我干这事儿，他如果说错了，那说明我问的问题就错了。

131
00:12:48.640 --> 00:12:53.500
Fan Yunjie: 那你是怎么去判断，或者说是靠什么判断。

132
00:12:54.060 --> 00:12:55.240
Norris: 靠经验。

133
00:12:55.510 --> 00:12:56.610
Fan Yunjie: 靠经验。

134
00:12:58.290 --> 00:12:59.380
Fan Yunjie: 我想要去。

135
00:12:59.380 --> 00:13:00.790
Norris: 没错了，我肯定是知道的。

136
00:13:01.450 --> 00:13:05.840
Fan Yunjie: 你是完全靠经验去知道他给你的答案是错的。

137
00:13:07.000 --> 00:13:09.670
Fan Yunjie: 那你还会去去跟他。

138
00:13:10.250 --> 00:13:10.920
Norris: 38.

139
00:13:10.920 --> 00:13:13.150
Fan Yunjie: 奶头，你会啊。

140
00:13:13.310 --> 00:13:14.500
Norris: Beatles它

141
00:13:14.780 --> 00:13:21.590
Norris: 你给我瞎掰什么东西呢？你刚才给我改过来，你刚才说这个完全是无稽之谈，我就会这么骂他。

142
00:13:21.590 --> 00:13:32.280
Fan Yunjie: 那你你要跟我讲一下这个过程，比方说你从头从零到一开始某一个项目，然后你大概跟跟他是怎么样的一个配合协作。

143
00:13:34.090 --> 00:13:46.210
Norris: 比如说我现在的personal project需要写代码嘛，那写代码的时候，我可能由于描述的不够全面，那他就会asse去脑补一些条件，

144
00:13:46.310 --> 00:14:04.690
Norris: 那这些条件加起来之后代码的方向就不是我的方向了，他可能多给了几个看起来非常全面的supporting的这个角度，但是我并不需要那个supporting的角度，反而是由于加了那个东西之后打掉了我未来的可能性，

145
00:14:05.560 --> 00:14:22.960
Norris: 所以我就会跟他说，你不要，就是我先看他这些回答，然后他的回答里边，比如说有四五点，然后后边两点呢？完全是optional的。但是他认为说建议怎么怎么样，建议怎么样，我说你不要给我建议你刚才说的这些东西，我告诉你，为什么你不需要给我建议？

146
00:14:24.290 --> 00:14:36.080
Fan Yunjie: 但是在这个基础之上，你会不会去去靠它完完全全的输出一个最优结果，还是说你就自己去optimize了。

147
00:14:42.280 --> 00:14:51.930
Norris: 你其实你们目前研究的课题就是这个问题啊，就是人机交互啊，这个东西必然是有人有机啊。

148
00:14:52.930 --> 00:14:57.110
Fan Yunjie: 但每一个人，他人机交互的程度不一样啊对吧，

149
00:14:57.320 --> 00:15:05.480
Fan Yunjie: 有些人他可能就是大概靠ai给他一轮输出，他就即使发现他是错的，他也不会去纠错，

150
00:15:05.910 --> 00:15:15.630
Fan Yunjie: 他就自己去把它优化完之后，可能就是后面的工作，他就是自己的了，但有一些人，可能，他就是会跟ai深度的去交互。

151
00:15:15.630 --> 00:15:18.130
Norris: 你没有为什么跟他，而不是自己去纠错吗？

152
00:15:18.450 --> 00:15:22.070
Fan Yunjie: 你会是白头，然后一直让他给到你最好的答复。

153
00:15:22.070 --> 00:15:26.180
Norris: 对，你知道为什么吗？就是最核心的问题是，他们，我不会写代码呀。

154
00:15:27.080 --> 00:15:29.420
Fan Yunjie: 呵，

155
00:15:29.760 --> 00:15:32.520
Fan Yunjie: 那但是你会不会觉得有点浪费时间？

156
00:15:32.760 --> 00:15:34.720
Fan Yunjie: 从效率上来想。

157
00:15:35.330 --> 00:15:39.140
Norris: 会啊，就是我今天刚teach了一个我改了50多遍的单码

158
00:15:40.670 --> 00:15:43.710
Norris: 50多遍。我花了我大概两三周的时间。

159
00:15:45.160 --> 00:15:57.060
Fan Yunjie: 那你都不会说，觉得我要通过有效性来讲，我就减少对于ai的依赖，然后更大程度之上就是我大概把工作留我这边多做一点，这样子。

160
00:15:57.250 --> 00:16:06.320
Norris: 不会的趋势不可逆，没有必要去探讨这个问题，这个趋势就是趋势，你如果不能跟ai共存的话，你就被淘汰。就这么简单。

161
00:16:08.150 --> 00:16:22.270
Fan Yunjie: 那你如果说你是带着经验来看你所在处理的工作流，那很多的，比方说现在的就那些大学生就还没有去到研究阶段的小朋友，

162
00:16:22.360 --> 00:16:29.360
Fan Yunjie: 他们对于很多事情的判断能力就很差，然后很大程度上还在依赖ai，这样子。

163
00:16:29.560 --> 00:16:30.090
Norris: 那么。

164
00:16:30.090 --> 00:16:34.620
Fan Yunjie: 会不会影响到他本身自己的思维或者主观的判断。

165
00:16:35.020 --> 00:16:39.950
Norris: 这么说吧，影响了，也就影响了这个人该怎么长，这是他的命数。

166
00:16:39.950 --> 00:16:40.930
Fan Yunjie: 哈哈哈哈

167
00:16:42.610 --> 00:16:45.260
Fan Yunjie: okay，不要这么说。

168
00:16:45.730 --> 00:16:51.420
Norris: 这个真的不重要，就是我还是那句话，世界都是假的，你何必较真呢？

169
00:16:52.460 --> 00:17:09.339
Norris: 我们无非就是过来经历了，经历完了，体验好坏也没有差别。戴了这个虚拟眼镜，你又重新开一句新的游戏，还是戴上之后忘却前世喝了孟婆汤，整个这个过程无非就是重启，重启再重启对吧？

170
00:17:10.730 --> 00:17:20.640
Fan Yunjie: 那你觉得你现在的很大程度上还有跟他拜托五轮的能力，那未来的小朋友有跟他拜托五轮的能力吗？

171
00:17:21.079 --> 00:17:27.969
Norris: 我明白你的意思了。就是说，如果他们都raise from这个aier，他们就没有这个experience去。

172
00:17:28.019 --> 00:17:32.609
Fan Yunjie: 那不对的，因为首先ai训练，它并不是只通过一个数据员。

173
00:17:33.469 --> 00:17:38.909
Norris: 而且ai和ai之间也是有区别的，未来肯定是多智能体协作时代，

174
00:17:39.559 --> 00:17:46.399
Norris: 你不可能就一个产品里边就一个ai agent吧，它肯定是多个agent嘛，多个agent也不可能只有一个api吧，

175
00:17:46.809 --> 00:18:01.329
Norris: 他肯定是有gpt四，有可能gpt五，然后又加上一个jamai，然后又加上一个通10,000,000就是他们的训练源头都不一样，可能说八个里面有四个是对的四个是错的，但是总会有八es的。

176
00:18:02.710 --> 00:18:07.310
Fan Yunjie: 但本身其实ai它是一个概率论出来的。这种算法。

177
00:18:07.310 --> 00:18:09.280
Norris: 所以就是问题就是概率啊。

178
00:18:09.460 --> 00:18:23.440
Fan Yunjie: 对啊，但是这种东西它很大程度上是没有确切真正正的那种就是data analyses的这样子很确切的过程，你还是需要你本身自己去纠错的呀。在我认为对。

179
00:18:29.550 --> 00:18:32.420
Norris: 这不好意思，我又没跟上你的问题。Sorry。

180
00:18:32.910 --> 00:18:44.320
Fan Yunjie: 对啊，就是就是这样。我是觉得ai它本身是概率生成文字的一个模型，对吧？它不是数据分析的一个模型。

181
00:18:44.320 --> 00:18:44.960
Norris: 对。

182
00:18:44.960 --> 00:18:55.090
Fan Yunjie: 所以说，如果带著这样子的话，我会，我会有一个，我会有一个。这种这种担忧在于比方说现在

183
00:18:55.190 --> 00:19:10.630
Fan Yunjie: 大概率的，大家的靠ai使用，那你ai是通过比较大的这种信息流来给你总结，那如果说，它就是在一个错误的基础上，你如果还在存在很多很较真的行业。

184
00:19:11.280 --> 00:19:23.910
Fan Yunjie: 是在这个问题上去去迭代，而不是说在一个正确的方向之上，这个就是之前可能没有ai的过程当中，大家不会去那么大程度上犯错。

185
00:19:24.300 --> 00:19:30.080
Norris: 不会啊，怎么不会，难道是没有ai的时候，你是怎么查资料的呢？翻图书馆看谷歌

186
00:19:30.320 --> 00:19:35.410
Norris: google scholar就一定是verified source吗？所有的paper就一定是对的嘛。

187
00:19:37.140 --> 00:19:47.330
Fan Yunjie: 也是存在这个问题，但是你肯定还是会有比较多的critical thinking在你自己个人，而不是说完全就是依赖它的成果。这一类的。

188
00:19:47.560 --> 00:20:00.370
Norris: 对你想啊，你就把谷歌或者是ai，作为两个对立面，一个是你读了之后自己思考一个是，你读了之后就不思考了。但问题是为啥我读了ai，我就会不思考呢？

189
00:20:01.450 --> 00:20:09.080
Fan Yunjie: 当然啊，这个就是我们在探讨的问题啊，就是很明显，肯定还是会思考的呀，你是一个独立个体啊，你是生物啊，你有应急反应啊。

190
00:20:10.230 --> 00:20:30.230
Fan Yunjie: 可是现在很多的像你是绝对带着自自己的问题去看他给你解决的问题，但是现在很多的小朋友，他可能就是或者说用户，我不能说是小朋友，现在即使是很很有experience的这些用户，他就是喂给ai，然后直接就利用了，直接就这样子走了。

191
00:20:30.500 --> 00:20:34.320
Norris: 那没办法，社会就是分层的，它这么使用它，说明就在这个层次。

192
00:20:35.070 --> 00:20:37.220
Fan Yunjie: 你认为这是完完全全在

193
00:20:37.980 --> 00:20:43.640
Fan Yunjie: 在这个个人的思维上的问题，而不是说在ai的迭代上的问题。

194
00:20:43.640 --> 00:20:47.440
Norris: 没错，这是人怎么使用工具，而不是工具怎么驯化人。

195
00:20:48.200 --> 00:20:52.030
Fan Yunjie: 但你不会说觉得ai会加剧这个问题吗？

196
00:20:52.030 --> 00:20:57.090
Norris: 加剧，就加剧了世界永远在朝着二级化方面发展。马太效应永远在加深。

197
00:20:58.320 --> 00:21:01.760
Fan Yunjie: 你有没有觉得说想过

198
00:21:01.870 --> 00:21:06.690
Fan Yunjie: 去解决人类的这个问题，ai应该去怎么样的优化。

199
00:21:07.280 --> 00:21:19.290
Norris: 我想过啊，我不是从做了一个基数的infer，就是说做这个什么分布式计算，目的就是把个人的信息留在个人身边，不让它成为一个公众数据吗？

200
00:21:21.080 --> 00:21:21.960
Fan Yunjie: 但是肉粽。

201
00:21:21.960 --> 00:21:27.960
Norris: 更好的使用它，但是如何训练出更好的模型，这个不在我能力范围以内，对吧？

202
00:21:29.160 --> 00:21:42.490
Fan Yunjie: 这样子的，它本身okay，它是把自己的，我可以把把它变成理解为一个区块链的形式吗？就所有大家的数据都以一个block的形式单独的存储了，是这个意思吗？

203
00:21:42.760 --> 00:21:52.670
Norris: 稍微区别一点的就是区块链，它还是在链上的，我这个东西更多的是一个本地化的方案，就相当于是你买了个路由器一样。

204
00:21:53.220 --> 00:21:57.140
Fan Yunjie: 所以也没有将来的一些ethical的一些困扰了，对吧？

205
00:21:57.300 --> 00:22:03.690
Norris: 没有什么就是privacy什么的都不存在的，因为别人也不知道你的信息，aa，大脑也不知道你的信息。

206
00:22:04.310 --> 00:22:07.390
Fan Yunjie: 那他还会被有效的训练吗？

207
00:22:07.800 --> 00:22:14.100
Norris: 有啊，因为我们会通过模糊的这些参数来就是进行语音更新

208
00:22:14.260 --> 00:22:18.030
Norris: 就是我只给你更新parameters，但是我不更新数据，

209
00:22:18.140 --> 00:22:24.450
Norris: 数据只依赖于你现在本机有什么数据，比如说你照过的照片说过的话，去过的地方。

210
00:22:25.940 --> 00:22:29.190
Norris: 他就更更加变成了一个personal的system，

211
00:22:29.500 --> 00:22:30.690
Norris: 对对对，对。

212
00:22:31.360 --> 00:22:38.960
Fan Yunjie: 所以你觉得这是一个最好的。对于ai的定位嘛，就是将来来说的话，从通过人就是人机协作最好的一个方向吗？

213
00:22:39.540 --> 00:22:42.110
Norris: 对于C端用户来说应该是的，

214
00:22:42.610 --> 00:22:49.070
Norris: 对于个体用户来说，每一个人都会有一个独立的服务器，或者是租用一个专属的服务器，

215
00:22:49.620 --> 00:22:52.950
Norris: 怎么干，怎么安全，怎么来呗，数据只会越来越多。

216
00:22:53.600 --> 00:23:00.560
Fan Yunjie: 你觉得说比方现在比较理想的，这种劳动劳工分配的话，你觉得应该大概是一个什么样子的，

217
00:23:00.840 --> 00:23:11.130
Fan Yunjie: 因为现在我觉得已经很大程度之上，ai能帮助大家解决很很很多的工作了，或者说是在比方说在教育这个领域。

218
00:23:11.180 --> 00:23:18.870
Norris: 就招生来看的话，就已经ai就可以帮你去把很多的candidates都已经分配好了，然后你可能做的就是很。

219
00:23:19.070 --> 00:23:23.590
Fan Yunjie: 很personal的工作就是你跟人家面个事，谈个话，聊个天，这样子。

220
00:23:23.930 --> 00:23:29.620
Fan Yunjie: 基本上剩下的一些之前很重量型的工作都可以交给ai来做了。

221
00:23:30.240 --> 00:23:38.920
Fan Yunjie: 将来说，如果这种劳工分配，你就怎样分配，是一个比较好的一个理想的状态，然后也保留了人类的这些

222
00:23:39.490 --> 00:23:46.800
Fan Yunjie: 就是不不不不不去依赖ai的问题，或者说不去降低自己批判性思考的问题。

223
00:23:47.310 --> 00:23:51.380
Norris: 好好问题，这个绝对可以出一篇paper了。

224
00:23:51.830 --> 00:23:53.190
Fan Yunjie: 哎呀。

225
00:23:53.190 --> 00:23:58.880
Norris: 让我想一想，如何能保持自己的主观独立性，

226
00:24:00.640 --> 00:24:01.940
Norris: 然后

227
00:24:02.330 --> 00:24:04.550
Norris: 还能更好的跟ai协作。

228
00:24:04.750 --> 00:24:05.400
Fan Yunjie: 二喔。

229
00:24:07.740 --> 00:24:10.240
Norris: 如何能保持主观独立性，

230
00:24:15.190 --> 00:24:24.610
Norris: 我感觉是肯定可以保持的，因为本身ai的算法虽然是在模拟脑神经元嘛，但是

231
00:24:24.790 --> 00:24:33.020
Norris: 他毕竟没有这么多的神经元，如果真的模拟人脑的多少万亿个神经元，全世界的龟都不够用了。

232
00:24:34.900 --> 00:24:37.440
Norris: 你倒是不用担心什么

233
00:24:37.880 --> 00:24:41.500
Norris: 算法的同质化，它肯定是不一样的，

234
00:24:42.030 --> 00:24:45.000
Norris: 他还是停留在零，一的二金质层面上的，

235
00:24:45.390 --> 00:24:51.900
Norris: 我们还是停留在脑电波概率论和量子态的一个状态，

236
00:24:52.100 --> 00:24:53.170
Norris: 所以

237
00:24:53.480 --> 00:24:54.810
Norris: 我们

238
00:24:55.050 --> 00:24:58.140
Norris: 就是很难说被同化，

239
00:24:58.400 --> 00:25:00.600
Norris: 它的确是一种配合关系，

240
00:25:01.080 --> 00:25:04.020
Norris: 有一点像你养了一个小狗，

241
00:25:04.370 --> 00:25:08.540
Norris: 这个小狗时间长了，你好像感觉它挺通人性的。

242
00:25:09.350 --> 00:25:10.800
Norris: 但实际上他就是个狗，

243
00:25:12.650 --> 00:25:16.760
Norris: 他永远不会张嘴跟你聊天，他表达的方式只能是汪汪汪。

244
00:25:19.370 --> 00:25:25.490
Fan Yunjie: 但是他也会伴随着你的训练越来越像人啊，越来越像你啊，你这个主人。

245
00:25:25.740 --> 00:25:29.930
Norris: 对呀，它只是长得像你皮像肉，像骨，像可能皮像有点像。

246
00:25:29.930 --> 00:25:32.480
Fan Yunjie: 谁说他长得像那么的红。

247
00:25:32.480 --> 00:25:33.810
Norris: 谁养傻狗？我跟你说。

248
00:25:34.050 --> 00:25:34.850
Fan Yunjie: 怎么样。

249
00:25:34.850 --> 00:25:38.360
Norris: 人养什么狗真的长得有点像的。

250
00:25:38.360 --> 00:25:41.840
Fan Yunjie: 这是什么？这这

251
00:25:41.840 --> 00:25:52.380
Norris: 现吗？养沙皮狗的都是比较，就是脸比较垮的人，然后养那个面目的稍微人就会至少脑子会活一点。

252
00:25:53.200 --> 00:26:06.280
Fan Yunjie: 这个是这个原因，我的认为，是啊，人家所有人都会有自恋的倾向，所以他喜欢去养跟自己长得像的东西，而不是说这个东西因为被他养，所以跟他长得像。

253
00:26:06.720 --> 00:26:08.670
Norris: 对，对，对也是有可能的，

254
00:26:09.210 --> 00:26:15.370
Norris: 那你去用ai的时候也会尽量去选择说说话顺你意义的ai咯。

255
00:26:15.480 --> 00:26:28.650
Norris: 比如说这个他就会有情绪化的反馈，说你真棒什么的ppt就更加的专家化，就是就事论事，对吧，那肯定切的主题，用户方向就不一样了。

256
00:26:29.720 --> 00:26:36.940
Fan Yunjie: 那你觉得说你现在目前还会给每一个ai去分工吗？按照他们的。

257
00:26:37.360 --> 00:26:43.530
Norris: 不会啊，我就是从成本角度考虑，在使用过程中，哪一个成本低，我就用哪一个。

258
00:26:44.780 --> 00:26:48.170
Fan Yunjie: 你现在最最多使用的还是gpt，对吗？

259
00:26:48.340 --> 00:26:49.340
Norris: 对对，对，

260
00:26:50.120 --> 00:26:51.600
Norris: 因为包月了嘛。

261
00:26:51.600 --> 00:26:54.540
Fan Yunjie: 因为花钱了。

262
00:26:54.940 --> 00:26:55.530
Fan Yunjie: Ok.

263
00:26:55.530 --> 00:27:11.970
Norris: 另外几个呢，都是按照token算的，所以就token哪个低。比如说我们用那个flash模型就去process所有的邮件，那你看一个邮件的html，如果他是广告邮件的话剧场五笔非常费token，那你肯定不会用全模型去process他嘛，

264
00:27:12.220 --> 00:27:14.060
Norris: 你就用就好了。

265
00:27:15.040 --> 00:27:18.790
Fan Yunjie: 哎，可是我们弹回刚刚那个问题，那个狗的问题

266
00:27:18.910 --> 00:27:25.220
Fan Yunjie: 就是，但是用户，在很多情况下，他会自动化的去信任ai啊，对吧。

267
00:27:25.220 --> 00:27:25.940
Norris: 哼。

268
00:27:26.460 --> 00:27:33.010
Fan Yunjie: 对啊。但是这种信任机制逐渐形成之后就变成ai去反向训练人类了呀。

269
00:27:34.360 --> 00:27:38.040
Norris: Aa没有这个主观意识，去反向训练人类，这是我想表达的。

270
00:27:38.260 --> 00:27:39.540
Fan Yunjie: 当然，对。

271
00:27:39.710 --> 00:27:43.320
Norris: 他没有这个意图，他也不在乎他，也不在乎人类。

272
00:27:45.610 --> 00:27:56.060
Fan Yunjie: 对啊，但是人类它就会慢慢的会变成了一种有奴性的动物，然后就类似于人类对于某种熟悉刺激的这种习惯化。

273
00:27:56.210 --> 00:27:57.000
Norris: 哼。

274
00:27:57.000 --> 00:27:59.130
Fan Yunjie: 然后隐性形成。

275
00:27:59.300 --> 00:28:00.030
Norris: 哼，

276
00:28:00.370 --> 00:28:01.240
Norris: 没问题啊。

277
00:28:01.350 --> 00:28:09.500
Fan Yunjie: 这种自动化的信任就会导致可能更大程度之上人类本身它的这种credibility。

278
00:28:09.500 --> 00:28:10.330
Norris: 哼。

279
00:28:10.710 --> 00:28:11.370
Fan Yunjie: 就会。

280
00:28:11.740 --> 00:28:12.290
Norris: 明白。

281
00:28:12.290 --> 00:28:14.380
Fan Yunjie: 会受到很大的影响。

282
00:28:14.640 --> 00:28:17.040
Norris: 你喜欢吃米饭或者面条吗？

283
00:28:18.560 --> 00:28:20.280
Fan Yunjie: 我喜欢吃面条。

284
00:28:20.280 --> 00:28:21.040
Norris: Ok,

285
00:28:21.550 --> 00:28:25.670
Norris: 那么小麦驯化人类，这个事情你也接受了呀。

286
00:28:27.520 --> 00:28:35.970
Fan Yunjie: 那你是伴随自己since的判断的，呀。他也没有说，比方说现在我们的这一个世界上，小麦消失了。

287
00:28:36.350 --> 00:28:38.320
Fan Yunjie: 那我也没有办法呀，对吧？

288
00:28:38.320 --> 00:28:41.400
Norris: 对啊，今天ai主机关掉了，我也没有办法呀。

289
00:28:42.560 --> 00:28:46.360
Fan Yunjie: 但你仍然会接受。我喜欢面条这个事情。

290
00:28:46.360 --> 00:28:48.790
Norris: 当然了，当然了。

291
00:28:48.790 --> 00:28:59.300
Fan Yunjie: 而且我确实是喜欢面条这个事情。然而现在的问题是，即使ai关掉它，给到很多用户的已经既定的结果，它仍然是相信的对。

292
00:28:59.560 --> 00:29:00.680
Norris: 对对，对，

293
00:29:02.180 --> 00:29:02.980
Norris: 是啊。

294
00:29:03.940 --> 00:29:08.230
Fan Yunjie: 所以你觉得如何避免这种错误信任的这种情况，或者说是。

295
00:29:08.230 --> 00:29:18.500
Norris: 避免，我还是这个主观就是我认为能用好ai的人自然就能用好用不好的被驯化了，也就被驯化了。这是一种自然选择，这是进化对

296
00:29:18.890 --> 00:29:19.980
Norris: 不是训话。

297
00:29:21.430 --> 00:29:27.570
Fan Yunjie: 但是我觉得我们通你也是做产品呢？我也是做很多无用的工作的人，

298
00:29:27.780 --> 00:29:35.170
Fan Yunjie: 然后应该还是要思考一下怎么样帮人类变得更好，而不是说就是适者生存的问题吧。

299
00:29:35.300 --> 00:29:38.820
Norris: 人类本身能活到今天就是适者生存的结果。

300
00:29:40.040 --> 00:29:47.260
Fan Yunjie: 但是我觉得他是需要后边的更强大的人去去帮助他们的，就是说他们。

301
00:29:47.260 --> 00:29:50.060
Norris: 我跟你说，爱因斯坦也没有真的想写相对论，

302
00:29:50.840 --> 00:29:59.450
Norris: 马克思维，也没有真的想发明方城主，他们都只是兴趣使然，这个世界就是被快乐催化的吗？

303
00:29:59.660 --> 00:30:06.390
Norris: 你爱干什么就干什么，不爱干什么就不干什么？真的哪天世界灭亡了，大家一整眼发现都在母体里边重生了，

304
00:30:06.500 --> 00:30:08.340
Norris: 也没谁真的没有了。

305
00:30:09.610 --> 00:30:16.610
Fan Yunjie: 但是有这样一很多的机制可以去避免这个问题啊。比方说，你现在去设计这个clarity ai。

306
00:30:16.610 --> 00:30:17.590
Norris: 对吧。

307
00:30:17.590 --> 00:30:22.770
Fan Yunjie: 你就是比方设计一个什么认知预警的一个机制。

308
00:30:23.130 --> 00:30:26.810
Fan Yunjie: 让就让用户去激活一下他们本身的这种。

309
00:30:26.810 --> 00:30:28.220
Norris: 思考能力也是ok的，

310
00:30:28.520 --> 00:30:30.540
Norris: 我不会的，因为对我来说是成本。

311
00:30:33.060 --> 00:30:36.340
Fan Yunjie: 如果说现在都我们现在都是很有钱了。

312
00:30:36.630 --> 00:30:37.260
Norris: 嗨，

313
00:30:37.490 --> 00:30:38.040
Norris: 我觉得。

314
00:30:38.040 --> 00:30:40.730
Fan Yunjie: 化了这个当然。

315
00:30:40.730 --> 00:30:46.410
Norris: 如果说钱的无限多的话，我就不开这个产品了。好吧，我开这产品不是为了赚钱吗？现在。

316
00:30:46.410 --> 00:30:48.940
Fan Yunjie: 就是说我们要怎么去。

317
00:30:48.940 --> 00:30:56.590
Norris: 我知道你的意思，但是我真的劝你一下子这个话题真的太无解了，因为你真的改变不了什么。

318
00:30:57.230 --> 00:31:00.780
Fan Yunjie: 这个东西，写写论文还行，你真的要做点什么事，太难了，

319
00:31:01.370 --> 00:31:10.870
Fan Yunjie: 那我们就是跟他重新想知道你们这些产品研发的人员是不是有一些就是trigger，maybe可以去解决这个问题。

320
00:31:11.100 --> 00:31:16.450
Norris: 让我想一想，如果社会给予足够的奖励的话，我可以解决这个问题啊。

321
00:31:16.860 --> 00:31:26.330
Fan Yunjie: 你可以详细说一说，比方说就把它跟block team，maybe，maybe结合一下，然后通过一些token机制，什么，我觉得也是很很有趣。

322
00:31:26.930 --> 00:31:28.030
Norris: En

323
00:31:28.520 --> 00:31:41.820
Norris: 倒，不会有这么复杂的机制了。单纯就是说，如果有一个基金会它的使命，愿景就是避免人类去这个，

324
00:31:42.590 --> 00:31:44.540
Norris: 那么它将会。

325
00:31:44.540 --> 00:31:45.270
Fan Yunjie: Ye

326
00:31:45.270 --> 00:31:50.410
Norris: 直接给开发过程中的产品，或者是可以用pmf的产品就打钱嘛。

327
00:31:50.750 --> 00:31:53.880
Fan Yunjie: 就是你达到什么样的一个阶段，我就给你多少钱吧。

328
00:31:55.280 --> 00:32:00.480
Norris: 比如说你能通过什么什么测试这个产品通过什么什么反图龄测试，我就给你多少钱。

329
00:32:01.670 --> 00:32:10.460
Fan Yunjie: 你说的是一种，这是一种就是比较正向的反馈。你如果说现在就是regulation，他就是这样子约束你的话呢？你怎么样。

330
00:32:10.460 --> 00:32:18.080
Norris: 那就干呗，就是如果已经有regulation了，然后他要求说，哎，你必须有这么一个东西，不然不让你上线，那我就做呀，那怎么办？

331
00:32:18.080 --> 00:32:20.540
Fan Yunjie: 好怎么做。

332
00:32:20.540 --> 00:32:24.610
Norris: 怎么做？我想想啊，怎么去规范它，

333
00:32:25.440 --> 00:32:26.130
Norris: 我们

334
00:32:26.500 --> 00:32:29.850
Norris: 激活用户的自我认知，

335
00:32:31.000 --> 00:32:35.940
Norris: 这个他们有点反产品啊，因为产品最最核心的就是要是易用性嘛，

336
00:32:36.380 --> 00:32:40.290
Norris: 你用户用的时候，你突然打断它，就很不易用。

337
00:32:41.710 --> 00:32:46.990
Fan Yunjie: 这个。当然我跟你说，学术跟商业最大的产，这个这个

338
00:32:47.220 --> 00:32:51.320
Fan Yunjie: 差别就是在于这两个相左的方向拉扯，

339
00:32:51.470 --> 00:32:58.320
Fan Yunjie: 你就发现，好像我之前提过一个一个课题，然后都没有人来接盘的问题就是我在

340
00:32:58.470 --> 00:33:01.800
Fan Yunjie: 去探讨这个gating app上映性的问题。

341
00:33:02.440 --> 00:33:21.810
Fan Yunjie: 对，然后包括。但是你对于产产品来说的话，人家人家就是希望你上瘾啊，人家不希望你本身因为一些算法给你推荐你喜欢的人，然后你去去形成一些不良性的关系处理态度，他们才不care这个，但是我想解决的是这种对于关系的

342
00:33:21.900 --> 00:33:23.770
Fan Yunjie: 这种不认真的态度。

343
00:33:23.770 --> 00:33:26.070
Norris: 这两个相左的方向。

344
00:33:26.070 --> 00:33:28.280
Fan Yunjie: 所以我是想现在让你去。

345
00:33:28.280 --> 00:33:33.060
Norris: 去，maybe你现在是就乌托邦了，就是个学术人。

346
00:33:33.060 --> 00:33:40.520
Fan Yunjie: 对，然后现在你也是有被约束到这种情况，想知道你要怎么解决。如果说

347
00:33:40.640 --> 00:33:43.700
Fan Yunjie: 可能是你认为现在

348
00:33:43.850 --> 00:33:53.810
Fan Yunjie: 你的这个产品也通过一些系统设计去去防护。现在现在就是regulation，就是两个月之后你就要去执行了。

349
00:33:56.040 --> 00:33:57.090
Norris: 好难啊。

350
00:34:00.980 --> 00:34:03.090
Norris: 怎么做呀？我操

351
00:34:04.150 --> 00:34:05.220
Norris: en

352
00:34:08.290 --> 00:34:09.010
Norris: 就是

353
00:34:09.659 --> 00:34:10.810
Norris: en.

354
00:34:11.489 --> 00:34:13.159
Norris: 我能说我想不出来吗？

355
00:34:13.489 --> 00:34:15.030
Norris: 一时间想不出来。

356
00:34:15.239 --> 00:34:17.099
Fan Yunjie: 可以？没关系。

357
00:34:17.099 --> 00:34:19.809
Norris: 真的有点有悖于我正常思考模型。

358
00:34:20.530 --> 00:34:35.909
Fan Yunjie: 可以，没关系，所以你觉得说，就我们通过日常来讲嘛，你的你的同事啊，或者说你的下属，大家跟ai的互动方式大概是怎么样的，或者说是ai参与的程度大概怎么样？

359
00:34:36.330 --> 00:34:41.100
Norris: 如果是在我们这个项目的工作过程中，可以说是每个人都在用ai，

360
00:34:41.440 --> 00:34:47.239
Norris: 然后基本上只不过用的ai的工具不同，比如说那些后端工程师或者是

361
00:34:47.350 --> 00:34:51.650
Norris: 就不说后端工程师吧，前端工程师也好，他们

362
00:34:52.070 --> 00:34:56.440
Norris: 开了一个cursor，然后在cursor里边去管理整个的github

363
00:34:56.710 --> 00:34:57.840
Norris: 的这个，

364
00:34:57.950 --> 00:34:59.530
Norris: 然后这个技术战。

365
00:35:00.870 --> 00:35:08.460
Norris: 一个模块，一个模块的，改一行一行的就是他改一个地方，然后ai就可以帮他把其他的类似的地方都改掉嘛，就很方便。

366
00:35:10.640 --> 00:35:14.210
Fan Yunjie: 你会去认可他们多大程度之上

367
00:35:14.650 --> 00:35:17.450
Fan Yunjie: 去依赖我知道你十分认可。

368
00:35:17.450 --> 00:35:18.500
Norris: 哼。

369
00:35:20.520 --> 00:35:22.380
Fan Yunjie: 你不介意这个问题吗？

370
00:35:22.380 --> 00:35:24.210
Norris: 而且是我要求他们这么做的。

371
00:35:24.460 --> 00:35:33.510
Fan Yunjie: 你会介你也不会介意，比方说你的小朋友，他现在已经95%的工作都是ai来来做了，你也不会去介意这个事情。

372
00:35:33.720 --> 00:35:36.700
Norris: 不会，他只需要report就可以了。

373
00:35:37.120 --> 00:35:38.570
Norris: 我只看结果。

374
00:35:39.430 --> 00:35:46.890
Fan Yunjie: 明白明白，所以你觉得这样怎么去论证一个人本身他的专业能力呢？

375
00:35:47.400 --> 00:35:48.630
Norris: okay,

376
00:35:48.740 --> 00:35:53.640
Norris: 看看看报酬呗，社会赋予他多大的回报。

377
00:35:58.040 --> 00:35:59.640
Fan Yunjie: 你是指什么意思？

378
00:35:59.640 --> 00:36:06.160
Norris: 比如说名望啊，比如说金钱啊，比如说地位啊，政治头衔啊，就是所有的这些东西。

379
00:36:06.160 --> 00:36:11.840
Fan Yunjie: 不是我的意思是比方说，现在我们我们不看那么高维度的人，我们就看candidates

380
00:36:12.040 --> 00:36:16.050
Fan Yunjie: 很多的这些，这些这个候选人，他可能。

381
00:36:16.050 --> 00:36:16.490
Norris: 那啥。

382
00:36:16.490 --> 00:36:31.170
Fan Yunjie: 强的专业知识，但是他处理工作完完全全是不相信ai。我遇到过很多这样的人，他们真的完全不相信ai，特别是搞学术的人，他不相信ai，他说的话，他一定要去自己再去clarify一下。

383
00:36:31.170 --> 00:36:32.450
Norris: 那。

384
00:36:32.720 --> 00:36:34.390
Fan Yunjie: 还有一部分人可能。

385
00:36:34.390 --> 00:36:34.820
Norris: 尽量吧。

386
00:36:34.820 --> 00:36:40.160
Fan Yunjie: 专业知识并没有那么强，但他处理工作却处理到你的结果上了。

387
00:36:40.600 --> 00:36:43.830
Fan Yunjie: 在衡。量candidates方向的时候，你怎么去衡量。

388
00:36:43.830 --> 00:36:50.080
Norris: 对我个人来说就是看，结果还是一样的，他给我deliver的结果好，我就说你能力强。

389
00:36:50.830 --> 00:37:02.870
Norris: 当然你本身这个人不能有毛病，你别说话都说不明白，然后ai给你的结果你也解释不清楚，那是不行的就是你得懂得最终的结果你得解释得清楚，

390
00:37:03.510 --> 00:37:05.190
Norris: 然后这个结果还要好。

391
00:37:08.390 --> 00:37:15.070
Fan Yunjie: 那你对于效率性呢？那肯定这两个人他的效率是不一样的ai的那一个肯定会很快啊，对吧。

392
00:37:15.070 --> 00:37:16.820
Norris: 对呀，那就那就快的呗，

393
00:37:17.400 --> 00:37:20.710
Norris: 你都已经结果好的情况下了，那当然择优录取啊。

394
00:37:21.380 --> 00:37:27.820
Fan Yunjie: 这种你会在你的这个应聘过程当中会去做这方面的吗？

395
00:37:27.980 --> 00:37:34.930
Norris: 没有我，在应聘过程中我就非常简单，就是第一聊一下这个人是不是个正常人。

396
00:37:35.590 --> 00:37:45.010
Norris: 第二就是看一下它的历史都做过什么项目，因为这个时候吧，就是很难做。我们在招聘的时候还没有ai呢。

397
00:37:45.610 --> 00:37:46.260
Fan Yunjie: En.

398
00:37:47.090 --> 00:37:57.550
Norris: 就因为我们这个团队也不是就是为了ai这个项目组的，所以当时大家都只是简简单单的程序员，所以那个时候就看历史项目。

399
00:38:00.160 --> 00:38:03.740
Fan Yunjie: 所以就是完全看这个人的经验，判断能力。

400
00:38:03.920 --> 00:38:07.580
Norris: 对。至于说ai出了之后，我还真没招过什么新人。

401
00:38:07.970 --> 00:38:11.120
Fan Yunjie: 有。哼，

402
00:38:11.430 --> 00:38:15.930
Fan Yunjie: 这是是因为工作流都被aic试掉了吗？还是说。

403
00:38:16.490 --> 00:38:21.040
Norris: 对就是他们学的也挺快的，然后他们这个

404
00:38:21.260 --> 00:38:22.760
Norris: 也挺快的，

405
00:38:23.710 --> 00:38:29.660
Norris: 也不用再招一个特别会ai的人了，我还反而还开了一个会ai的人。

406
00:38:30.070 --> 00:38:31.100
Fan Yunjie: 为什么。

407
00:38:31.910 --> 00:38:34.860
Norris: 我开了这个人，是因为他个人的态度不好，

408
00:38:35.060 --> 00:38:36.800
Norris: 两次早会没来。

409
00:38:38.550 --> 00:38:40.540
Fan Yunjie: 好吧，okay。

410
00:38:40.740 --> 00:38:58.610
Fan Yunjie: 那你觉得说你再去进行ai的这个使用过程当中，我觉得因为你本身是有很强的思考能力的你，为了去让你的ai，就咱们说你训练一条狗更更符合你的调性，跟你训练一个ai，我觉得有异曲同工之处吧？对吧？

411
00:38:58.610 --> 00:39:01.030
Norris: 你在这个过程当中。

412
00:39:01.030 --> 00:39:07.210
Fan Yunjie: 我相信你去让他一轮一轮的校正，也是希望他可以变成你更好的assistant。

413
00:39:07.210 --> 00:39:07.670
Norris: 对。

414
00:39:07.670 --> 00:39:19.750
Fan Yunjie: 你会去让他。比方说，第一，我让你去给我多出几个结果，我去比较选择，或者说我让你给我出一个正好，相反的结果会有这种过程吗？

415
00:39:20.800 --> 00:39:21.660
Norris: 没有

416
00:39:21.780 --> 00:39:30.170
Norris: 没有我，我，我还是那句话，我知道我大概要的结果是怎么样子的，所以我就是看他给我的，是不是符合我的期待的。

417
00:39:32.520 --> 00:39:35.770
Fan Yunjie: 所以你在这个过程当中，你觉得prong engineering

418
00:39:35.950 --> 00:39:39.350
Fan Yunjie: 是会有帮助吗？即使你没有学过。

419
00:39:39.350 --> 00:39:41.180
Norris: 有帮助，但是帮助不大，

420
00:39:41.430 --> 00:39:46.510
Norris: 因为我也看了很多其他人的提示词就是大长串大长串的。

421
00:39:46.960 --> 00:39:56.530
Norris: 你又怎么样呢？你要扮演什么角色呀？你要想识怎么样，其实没有必要这么去prompt，你就把你要做的事情说清楚就可以了。

422
00:39:59.250 --> 00:40:01.890
Fan Yunjie: 所以你觉得在这个过程当中啊，

423
00:40:02.380 --> 00:40:07.990
Fan Yunjie: 你当然是ai开始有了这样子的一个分工跟你的团队，

424
00:40:08.130 --> 00:40:24.400
Fan Yunjie: 那你觉得多大程度之上，你的你的工作领域变成你指定这个事情是人做，而不是说这个事情变成了一个一个一个一个ai可以去分担的事情，或者说哪些岗位你觉得将来就是应该应该

425
00:40:24.530 --> 00:40:26.730
Fan Yunjie: damage掉，就是没没有了。

426
00:40:28.130 --> 00:40:29.180
Norris: 程序员。

427
00:40:31.040 --> 00:40:32.440
Fan Yunjie: 这么直接吗？

428
00:40:32.570 --> 00:40:33.180
Norris: 哼。

429
00:40:34.520 --> 00:40:37.610
Fan Yunjie: 因为我不写代码，所以我不知道你为什么这么negative。

430
00:40:38.280 --> 00:40:44.230
Norris: 不不是connective，就是单纯是因为现在ai在代码能力的进化上是最快的，

431
00:40:45.570 --> 00:40:47.230
Norris: 因为它有最多的数据。

432
00:40:47.790 --> 00:40:50.470
Fan Yunjie: 那这样会有一个很大的问题啊。

433
00:40:50.860 --> 00:40:55.600
Fan Yunjie: 那你最终ai，它背后不也是人去写的程序吗？

434
00:40:55.600 --> 00:40:56.650
Norris: 最开始是的。

435
00:40:59.710 --> 00:41:00.350
Fan Yunjie: 那现在你。

436
00:41:00.350 --> 00:41:04.840
Norris: 他们就变成了比如说十个ai写一个新A就可以自己造自己了。

437
00:41:07.020 --> 00:41:12.370
Fan Yunjie: 那也是仍仍然需要人类在上面去驯化它呀，人类去给他下指令啊。

438
00:41:13.840 --> 00:41:20.920
Norris: 不是训练和推理是两个过程嘛，对吧，咱们说，训练还是说推理

439
00:41:21.230 --> 00:41:22.600
Norris: 推理的话，你需要用这个。

440
00:41:22.600 --> 00:41:27.750
Fan Yunjie: 我这样子觉得我是这样觉得你不论是训练还是推理，都要有experience，

441
00:41:27.870 --> 00:41:33.020
Fan Yunjie: 你不能是不理解，或者说是你对于这个东西没有认知。

442
00:41:33.020 --> 00:41:36.390
Norris: 然后去进行，这个东西就变得。

443
00:41:36.390 --> 00:41:37.610
Fan Yunjie: 空中楼阁。

444
00:41:39.930 --> 00:41:46.830
Norris: 对，是的，空中落格这个词很好，就是ai在空中打转是经常有的事情，自己把自己给说服了。

445
00:41:50.230 --> 00:41:56.190
Fan Yunjie: 但是这样的问题其实会不好啊，因为你长期以来就会变成就像我刚刚说的，

446
00:41:56.230 --> 00:42:00.220
Fan Yunjie: 就是像吸了大麻一样，对于普通用户啊，

447
00:42:00.220 --> 00:42:16.500
Fan Yunjie: 我就是被ai遛着转ai，它的思维混乱，我也不知道他在思维混乱，我就正在享受在他给我的制造的假象当中里面。当然，这些人类也是你所说的可能会被

448
00:42:16.500 --> 00:42:22.450
Fan Yunjie: 适者生存淘汰掉的这些人类，但是很大程度之上，我觉得对于

449
00:42:22.660 --> 00:42:26.880
Fan Yunjie: 人工协就是人机协作来说的话。

450
00:42:27.660 --> 00:42:34.220
Fan Yunjie: 应该还是要有一个training的一个process，去让大家去更好程度上的保留认知吧。

451
00:42:36.970 --> 00:42:42.800
Norris: 有可能这个也是一个过程嘛，你不可能说，突然有一天，所有程序员都被开除了，

452
00:42:43.110 --> 00:43:02.070
Norris: 就算被开除了，这帮人还要回到90多岁，他还会有这个技能就是一直伴随他，然后学校也不可能说，我完全就不教这个编程了，因为你不需要学编程，你像难道人会说话就不教语文了吗？对吧，还是还是在教学的吗？就这个学科肯定还是存在的。

453
00:43:04.420 --> 00:43:11.890
Fan Yunjie: 你本身在对于ai的这个未来来说的话，你是一个非常非常positive的一个状态，是吗？

454
00:43:12.220 --> 00:43:18.590
Norris: 就是我是一个没有办法的positive，因为这个是一个趋势，趋势不可逆，

455
00:43:19.680 --> 00:43:27.580
Norris: 全世界没有像ai这次革命一样花过这么大的全人类的力量去做一件事情。

456
00:43:28.700 --> 00:43:29.410
Fan Yunjie: 可是

457
00:43:30.500 --> 00:43:31.010
Norris: 你说。

458
00:43:31.010 --> 00:43:35.840
Fan Yunjie: 可可是像像你所说的就是ai之前也有搜索引擎啊。

459
00:43:36.030 --> 00:43:37.550
Norris: 哼，是的。

460
00:43:37.550 --> 00:43:43.030
Fan Yunjie: 这个目前也是一个非常大的，革命型的这种事件啊，对吧。

461
00:43:43.860 --> 00:43:53.390
Fan Yunjie: 我觉得ai某种程度上来说的话，其实对于某一些用户，它只是一个集合型的一个搜索引擎，一个非常

462
00:43:54.160 --> 00:43:56.080
Fan Yunjie: ordinary的一个tool。

463
00:44:01.790 --> 00:44:04.580
Norris: 对啊，我一直认为它就是一个工具啊。

464
00:44:06.050 --> 00:44:13.400
Fan Yunjie: 但是目前但是未来的趋势，你是认为ai是跟人类会有很强烈的协作，还是说它仍然是这个脱欧。

465
00:44:13.590 --> 00:44:15.000
Norris: 他是一直都是个吐，

466
00:44:16.900 --> 00:44:19.000
Norris: 他永远的是辅助角色。

467
00:44:19.930 --> 00:44:28.260
Fan Yunjie: 你也不会去在tms这个角度上去想到将来ai跟人类的真正意义上的场景化分工。

468
00:44:29.290 --> 00:44:42.320
Norris: 场景化分工是顺其自然发生的。如果有一天扫大街的都是机器人了，那前提一定是社会福利综合性的提高，并不需要扫大街这个岗位了，而不是机器人能扫大街。

469
00:44:42.830 --> 00:44:48.010
Norris: 就算每一天程序员都消失了，不是因为他们不会编程了，是他们不需要编程了。

470
00:44:51.780 --> 00:44:55.510
Norris: 一定是基于整体社会福利去思考这个社会构架问题。

471
00:44:57.080 --> 00:45:07.380
Fan Yunjie: 但是你现在所有的这些产品定位，你都是把它当成一个图，你从来也没有想说我要把它变成personal assistant，但是你曾经在底层的时候，你就是这样思考的呀。

472
00:45:08.280 --> 00:45:12.900
Norris: 不他是personal assistant，但他就难道他就不是个two吗？

473
00:45:13.690 --> 00:45:20.880
Fan Yunjie: 的一个工具跟一个personalism。我觉得他对于一个人的印象还是不太一样的。

474
00:45:21.130 --> 00:45:25.700
Norris: 这的确是嘛，就是我不可能说我就不要人类助理了，

475
00:45:25.820 --> 00:45:30.480
Norris: 我不要人类助理了，一定是因为我秘密太多了，我需要一个ai来帮我保守秘密。

476
00:45:32.120 --> 00:45:34.800
Fan Yunjie: 那倒也不是从通过这个

477
00:45:35.620 --> 00:45:47.280
Fan Yunjie: 本身一些一些这个数据安全或者其他问题上，我觉得其实大部分人都不care这个问题就真正意义上自己的这个数据安全是不是就是在。

478
00:45:47.280 --> 00:45:47.610
Norris: 还可以。

479
00:45:47.610 --> 00:46:05.520
Fan Yunjie: 当中可以完全的就有一个好的保护，甚至对于伦理性的问题，他们也不是很在乎，他们只是在乎这个东西好不好用，有多大情况的就是有效性，我觉得这个是大部分去去去，可能更大程度上跟ai协作的一个主要原因。

480
00:46:05.640 --> 00:46:22.860
Fan Yunjie: 但是我想要就是更大程度上，在这里的产品上，我想要知道你们的方向，就仍然会把它想要定位成未来人类的使用的工具，还是说人机在未来真的程度上可以变成这种劳动分配，

481
00:46:22.860 --> 00:46:28.860
Fan Yunjie: 然后很大程度之上变成了一个团队，里面，他也变成了一种女人化personalized。

482
00:46:30.140 --> 00:46:33.050
Norris: 我们没有想过personalize这个事情。

483
00:46:35.500 --> 00:46:37.250
Fan Yunjie: 这不是好事情吗。

484
00:46:37.600 --> 00:46:38.750
Norris: 没想过而已，

485
00:46:39.070 --> 00:46:41.660
Norris: 好不好的没想过呀。

486
00:46:42.900 --> 00:46:54.410
Fan Yunjie: 因为你是处理这种这种比较底层的工作，但是很多比方其实很多market你知道吧，就做事销的人。

487
00:46:54.410 --> 00:47:00.210
Norris: 他们很依赖ai啊，去去写一些这个copy post呀，去输出一些poster啊。

488
00:47:00.460 --> 00:47:01.300
Norris: 对。

489
00:47:01.390 --> 00:47:17.600
Fan Yunjie: 这种的话，就很需要这种匿人化，然后你会认为就是比方说artist或者说是creator这这一类的工作人员，他们将来会面临着比较大的这个就业的问题吗？或者说是会被替代吗？

490
00:47:21.870 --> 00:47:25.350
Norris: 哎，这个他们这个好问题啊，因为

491
00:47:26.620 --> 00:47:34.860
Norris: 首先你得看他写这个东西是为了干什么？他是为了su嘛？为了blog嘛为了宣传这个产品嘛？为了霍克嘛，就是

492
00:47:35.180 --> 00:47:38.830
Norris: 它有个目的嘛。假设说未来

493
00:47:38.960 --> 00:47:43.840
Norris: 这个所有的数据，ai都能搜索到，是不是就不需要seo了。

494
00:47:44.890 --> 00:47:48.010
Norris: 那么假如说你的产品足够好，

495
00:47:48.190 --> 00:47:51.840
Norris: 那么是不是ai也可以搜索到？即使你并没有做过生产有效。

496
00:47:53.870 --> 00:47:58.140
Fan Yunjie: 但你通过一个branding的角度上来说，

497
00:47:58.320 --> 00:48:02.380
Fan Yunjie: 这样的东西它也没有办法帮你去品牌化呀。

498
00:48:02.680 --> 00:48:08.670
Norris: 是的，这样品牌这个东西跟ai只能是ai辅助于品牌建设。

499
00:48:09.370 --> 00:48:15.200
Fan Yunjie: 当然，那你就需要人类去在后边去，怎么样去处理了。

500
00:48:15.360 --> 00:48:27.630
Norris: 对的就是需要人类去做这个风格化的筛选，就是说ai给我出了80个版本，我就喜欢这个版本。Ai说我推荐另外这个我不，我就喜欢这个，对吧？人类拍了板说了算的。

501
00:48:28.830 --> 00:48:40.100
Fan Yunjie: 人类确实是有这个判断能力，但是它也解决不了，可能80个都是在同质化的角度上产出的结果呀，然后本身的创意啊，包括很多的，

502
00:48:40.620 --> 00:48:46.410
Fan Yunjie: 我们说人性，或者说这个东西有人味的问题，就我觉得慢慢就就已经我也。

503
00:48:46.410 --> 00:48:50.360
Norris: 也是我现在刷短视频就是看到那种ai上的视频，我直接跳过的。

504
00:48:52.910 --> 00:49:02.640
Fan Yunjie: 你会觉得说，所以你会觉得说这种创意型的岗位或者创意型的，这种输出的这些职能还是永远不会被替代的，对吗？

505
00:49:02.810 --> 00:49:08.400
Norris: 我认为创意不能说永远吧，很长一段时间是很难被替代的，

506
00:49:08.610 --> 00:49:10.880
Norris: 但是ai早晚会起来的。

507
00:49:13.130 --> 00:49:21.590
Fan Yunjie: 为什么，它怎么会能替代到人类本身的创意想法，或者说它本身的这种调性，这种调性这个东西我就很难替代啊。

508
00:49:21.730 --> 00:49:25.610
Norris: 药性，它也是有个总数的吧，你就算有68,000多种

509
00:49:25.810 --> 00:49:29.430
Norris: 调性，你最终也是会是一个数字嘛。

510
00:49:29.720 --> 00:49:33.920
Norris: 那ai永远有数字的边缘可以被触碰到的。

511
00:49:34.700 --> 00:49:38.070
Fan Yunjie: 那这很有趣。我想问你平常看电影吗？

512
00:49:39.710 --> 00:49:51.700
Fan Yunjie: 你每一个电影的这个导演，他的风格都不同，你觉得ai可以通过学习之后就是可以拍出跟这个导演的调性，一模一样的产作品，或者说同调性的作品出来，是吗？

513
00:49:51.850 --> 00:49:52.570
Norris: 可以的，

514
00:49:52.780 --> 00:49:57.010
Norris: 你要是说他能不能拍出不同调性的，我要思考一下，拍同调性绝对没问题。

515
00:49:59.390 --> 00:50:11.020
Fan Yunjie: 那这样很可怕耶对于我来讲很可怕，我还是希望可以吃一点创意的饭呢？虽然我也是没靠这个赚到钱。

516
00:50:11.310 --> 00:50:19.030
Norris: 我这个我感觉啊，咱们的有生之年，在我们的这个硬件还能支撑的，这么几十年里面，

517
00:50:19.190 --> 00:50:20.860
Norris: 你不用担心这个问题，

518
00:50:21.910 --> 00:50:23.030
Norris: 数据不够，

519
00:50:23.150 --> 00:50:25.420
Norris: ai也是要量变导致质变的。

520
00:50:26.730 --> 00:50:35.730
Fan Yunjie: 可是现在的这个educational的这个level其实很大的一个问题，我在于现在小朋友很多学习的东西，他将来会完全没有用。

521
00:50:38.030 --> 00:50:47.850
Norris: 但是他得知道吧，就是好像你说为什么水在往下流，他没有飘到天上去，他得知道是因为地球有引力吧。

522
00:50:49.650 --> 00:50:52.530
Norris: 就是如果这个都不知道，那他也没法在社会生存呀。

523
00:50:54.060 --> 00:50:58.670
Fan Yunjie: 你，你如你现在是一个单身无阿的状态，对吗？

524
00:50:58.670 --> 00:50:59.290
Norris: 是的。

525
00:50:59.880 --> 00:51:06.800
Fan Yunjie: 你将来如果有小孩的话，现在面临着就是选方向。选专业的角度，你会有什么样的建议给他。

526
00:51:07.140 --> 00:51:09.690
Norris: 我会有什么建议给他呀，我想想啊，

527
00:51:10.100 --> 00:51:11.850
Norris: 假如说你是我的小孩，

528
00:51:11.970 --> 00:51:15.120
Norris: 然后，现在你要上大学了，要选个专业。

529
00:51:16.070 --> 00:51:22.910
Norris: 我会说，我会观察你过去的18年去看你最喜欢什么，做什么最开心。

530
00:51:24.980 --> 00:51:28.710
Norris: 你如果喜欢玩，我就看你什么东西玩的最好。

531
00:51:29.990 --> 00:51:33.550
Fan Yunjie: 但他玩的东西他也不一定可以去支持他的生活呀。

532
00:51:33.550 --> 00:51:36.670
Norris: 生活不是问题，都是我的小孩怎么会有生活问题呢？

533
00:51:37.200 --> 00:51:40.490
Fan Yunjie: 开玩笑话下一个话题，

534
00:51:40.490 --> 00:51:48.730
Norris: 开个玩笑。如果说是这么说啊，就是ai如果已经把这个服务领域都充斥掉了，

535
00:51:48.800 --> 00:52:07.380
Norris: 因为ai肯定是先上来服务于人来作为工具辅助角色嘛，那么肯定会给社会创造巨大的剩余价值，这些剩余价值一定会通过一个比较平稳的方式传导给普罗大众。虽然99%还是被截留在顶层社会。

536
00:52:08.820 --> 00:52:12.050
Fan Yunjie: 你觉得呢？我想听一下你的意见，就是。

537
00:52:12.050 --> 00:52:13.100
Norris: 所以成。

538
00:52:13.100 --> 00:52:13.480
Fan Yunjie: Left.

539
00:52:13.480 --> 00:52:15.380
Norris: 也会非常的乌托邦。

540
00:52:17.670 --> 00:52:25.500
Fan Yunjie: 你可以给我几大类吗？你可以给我几大类吗？大概是你认为无法被被替代的。那些遗留的。

541
00:52:25.810 --> 00:52:26.880
Norris: 一流的专业吗？

542
00:52:26.880 --> 00:52:27.540
Fan Yunjie: 对啊。

543
00:52:29.200 --> 00:52:30.590
Norris: 前沿物理。

544
00:52:34.170 --> 00:52:38.090
Norris: 前沿物理就包括了理论和那个实验的话，就。

545
00:52:38.090 --> 00:52:38.610
Fan Yunjie: 然后。

546
00:52:39.350 --> 00:52:40.400
Norris: En

547
00:52:41.370 --> 00:52:42.560
Norris: 体育运动，

548
00:52:44.640 --> 00:52:45.610
Norris: 然后

549
00:52:47.990 --> 00:52:49.990
Norris: 娱乐吧，娱乐，艺术。

550
00:52:54.280 --> 00:52:56.300
Norris: 反正就是anything cues time。

551
00:52:58.020 --> 00:52:59.020
Fan Yunjie: 明白，

552
00:53:00.150 --> 00:53:13.620
Fan Yunjie: 所以本身的话你，你认为如今的这个这个社会上来讲，因为我我认为不论是人际啊，人跟人也是我觉得现在这个社会已经发展到了一个

553
00:53:13.920 --> 00:53:17.530
Fan Yunjie: 你如果想长线发展一个关系的话，其实

554
00:53:17.800 --> 00:53:27.890
Fan Yunjie: 我觉得最最核心，或者说最重要就是坦诚，就是比聪明啊，比什么都重要。如果说你因为长线的关系，

555
00:53:28.050 --> 00:53:33.840
Fan Yunjie: 那你觉得通过你自己的产品，你怎么去跟用户建立这种信任关系？

556
00:53:34.700 --> 00:53:37.000
Norris: 就是怎么让用户感觉，

557
00:53:37.550 --> 00:53:40.790
Norris: 但不是，但是你这个只是一方面呀，

558
00:53:41.160 --> 00:53:47.460
Norris: 就是坦诚，只是你打动用户的这方面，但如果我对你特别好，掏心掏肺，

559
00:53:47.780 --> 00:53:50.790
Norris: 但是我又没钱又丑，你也不会喜欢我啊，

560
00:53:54.460 --> 00:53:58.060
Norris: 首先你得产品好才行，其次才是真诚。

561
00:53:59.340 --> 00:54:09.070
Fan Yunjie: 那信任都很是是一个很重要的点啊，你当然产品好是每一个产品，我相信你们技术出身的这些，这些大佬们

562
00:54:09.150 --> 00:54:22.670
Fan Yunjie: 首先priority的事情是把自己的产品打磨的够漂亮，然后才去有信心让他去面对普罗大众这个事情，我把我把它放到第一个我不去讲，但是信任这个关系，你怎么去跟用户建立。

563
00:54:24.220 --> 00:54:29.810
Norris: 不是我就要把产品做得多好的问题是用户接不接受我的产品

564
00:54:30.010 --> 00:54:32.880
Norris: 就是我的理念，是不是跟用户能match

565
00:54:34.120 --> 00:54:44.580
Norris: 对吧？然后至于说如果卖去了之后他会不会。喜欢我这个产品，他又见不到我的人，我有什么好，跟他真诚的。

566
00:54:44.750 --> 00:54:49.310
Fan Yunjie: 没有我的意思，就是现在把比方说我们现在把clarity。

567
00:54:50.030 --> 00:54:52.000
Fan Yunjie: 把它变成了一个拟人化。

568
00:54:52.040 --> 00:54:57.000
Norris: Ok，我们现在就是做marketing给他出去，让他跟audience去建立connection。

569
00:54:57.940 --> 00:55:00.200
Fan Yunjie: 他的这个信任机制是怎么形成的？

570
00:55:00.840 --> 00:55:12.850
Norris: 你做了一个我不会去做的假设哈，首先我，我会回答你这个问题，但是我先澄清一点，就是我们不会去给搞一张脸出来。

571
00:55:13.000 --> 00:55:15.270
Fan Yunjie: 当然，当然，当然我只是这样。

572
00:55:15.270 --> 00:55:15.830
Norris: 太吓人了。

573
00:55:15.830 --> 00:55:18.100
Fan Yunjie: 对，当然不会这样啦。

574
00:55:18.100 --> 00:55:20.340
Norris: So creepy就是

575
00:55:21.250 --> 00:55:23.450
Norris: 如果

576
00:55:23.820 --> 00:55:30.920
Norris: 建立这个形象就是这么说吧，你首先要信任的是一个你已经认知的物种，

577
00:55:31.150 --> 00:55:37.800
Norris: 你不可能信任一个你不知道是什么东西的东西，你不可能信任一团烟，你可能会信任皮卡丘，

578
00:55:39.640 --> 00:55:41.990
Norris: 所以你必须要有一个实体形象，

579
00:55:42.490 --> 00:55:45.120
Norris: 这个实体形象可以是一个二维的动画，

580
00:55:45.750 --> 00:55:47.780
Norris: 也可以真的就是一张脸。

581
00:55:50.250 --> 00:56:09.890
Fan Yunjie: 我们说的东西现在有点太抽象了，我们现在就是要说本身对于产品来讲，你比方说比方说它是不是要去处理模糊性，他是不是要跟用户解释它的推理，它是不是要要就是比方说这一类对。

582
00:56:10.290 --> 00:56:13.810
Norris: Okay，首先，他不需要跟用户去解释，推理，

583
00:56:14.250 --> 00:56:17.250
Norris: 他，直接给用户答案就好了，用户要的是便捷性。

584
00:56:18.840 --> 00:56:34.770
Fan Yunjie: 但这样子伴随著ai的这些产品层出不迭。这样出现，你觉不觉得也许或许啊，跟用户解释，推理会是一个更好程度上打消用户疑虑的，一个一个好的一个切入点。

585
00:56:35.000 --> 00:56:36.430
Norris: 用户没那么多想法，

586
00:56:36.610 --> 00:56:47.950
Norris: 用户不会去看你的社交软件的mac算法是怎么写的，或者是机智讲解，他也不想听他要看的是youtube视频打开我就想看我最想最想看的频道，

587
00:56:48.600 --> 00:56:52.710
Norris: 他看的是抖音的推荐算法给我最喜欢的视频博主。

588
00:56:53.560 --> 00:56:57.110
Fan Yunjie: 你更大程度上认为大家还是

589
00:56:58.060 --> 00:57:07.710
Fan Yunjie: 在依赖ai，它变成一种一种拐棍，一种认知型的拐棍，而不是说去靠它去锻炼我们的思维，对吗？

590
00:57:07.710 --> 00:57:11.650
Norris: 没有人想成长，所有人都想留一辈子当一个baby。

591
00:57:14.330 --> 00:57:16.130
Fan Yunjie: 但也这也是问题啊，

592
00:57:16.290 --> 00:57:29.030
Fan Yunjie: 就像你所说的，你希望你的员工再去用ai处理出来数据，他可以知道这个数据本身的这个详细的意义，他可以说出来，123，四五万。

593
00:57:29.430 --> 00:57:43.050
Norris: 是的，我当然希望他是这个样子的，但是这是我的筛选标准，你来我公司上班，或者是咱们合作，那咱们要有一个基础嘛，但是你不能要求全社会的人都有这个基础呀。

594
00:57:44.430 --> 00:57:45.200
Fan Yunjie: 那我们就是。

595
00:57:45.200 --> 00:57:51.370
Norris: 大部分人，或者是我们把社会改造成这个方向。我跟你说，成年人了，不做改变，只做筛选。

596
00:57:52.070 --> 00:57:59.570
Fan Yunjie: 当然，但是我们还是要去做一些这种可以去批判性的

597
00:58:00.230 --> 00:58:02.330
Fan Yunjie: 这种动作吧，对吧，我觉得。

598
00:58:02.330 --> 00:58:03.350
Norris: 看性的。

599
00:58:06.400 --> 00:58:12.950
Norris: 哎呀，这个就跟那个那个那个alia一样嘛，就是说super lines

600
00:58:13.380 --> 00:58:14.790
Norris: 对吧，超级队齐。

601
00:58:16.660 --> 00:58:19.050
Fan Yunjie: 超级队齐对袭的是什么

602
00:58:20.230 --> 00:58:29.320
Fan Yunjie: 你这样子的话，我觉得你就要去去去挑战自己了，你就要挑战自己当初的假设，然后你还要去考虑你的一些

603
00:58:29.320 --> 00:58:48.550
Fan Yunjie: 反方的论点，才能真正意义上构成一种批判性思维的一种，很很具象的一种，一种一种轮廓在，但是不要说那些有没有悲哀淘汰掉的人了，我觉得可能2%的人，他们都没有这样子批判性思维这样强的能力。

604
00:58:48.550 --> 00:58:53.770
Norris: 你说的太对了，人本身就是一个概率。的

605
00:58:53.990 --> 00:58:58.930
Norris: 你现在在新加坡生活。我在美国生活都是一个概率，

606
00:58:59.180 --> 00:59:00.550
Norris: 这些概率

607
00:59:00.690 --> 00:59:03.380
Norris: 是过去的一系列的巧合导致的

608
00:59:04.420 --> 00:59:05.460
Norris: 就是

609
00:59:07.010 --> 00:59:14.330
Norris: 就是他们有没有批判性思维，他们所在的那个位置并不重要，有可能一个贫民窟的百万富翁，他最有批判性思维，

610
00:59:14.890 --> 00:59:17.510
Norris: 但是他可能连用ai的机会都没有。

611
00:59:19.500 --> 00:59:28.230
Fan Yunjie: 然后你这个，你这个点很有趣，就是你认为即使全人类，他都失去了批判性思维，但是这不是一个问题。

612
00:59:30.010 --> 00:59:34.370
Norris: 这本身这个思维都是人类给定义的，人类，

613
00:59:34.610 --> 00:59:37.340
Norris: 对吧？也没有那么清晰的对。

614
00:59:38.520 --> 00:59:46.350
Fan Yunjie: Okay，这个很有趣，所以说，你觉得你觉得就是人类的最大不可替代性还是在哪里？

615
00:59:46.980 --> 00:59:48.860
Norris: 人类最大的不可替代性

616
00:59:50.060 --> 00:59:51.130
Norris: en

617
00:59:51.800 --> 00:59:55.970
Norris: 人类的不可替代性。人类存在就是个巧合，没什么不可替代的。

618
00:59:57.840 --> 01:00:03.950
Fan Yunjie: 哇，你这个人我不行，我觉得人类还是完全完全。

619
01:00:03.950 --> 01:00:15.490
Norris: 保持人类文明的火种，但是实际上人类就只是一种巧合呀，你说自然进化论，它是个巧合或者是可造论，它是个神创造的一些有机体对，

620
01:00:15.730 --> 01:00:17.650
Norris: 或者，我们就是一堆数据，

621
01:00:17.900 --> 01:00:20.270
Norris: 这个真的重要吗？

622
01:00:22.630 --> 01:00:28.130
Fan Yunjie: 如果这么抽象来说的话当然不重要，那世界都可以爆炸的对吧？

623
01:00:28.130 --> 01:00:28.870
Norris: 还可以。

624
01:00:29.660 --> 01:00:35.410
Fan Yunjie: 那那你如果说再去具象的说的话，你我，你觉得不可替代吗？

625
01:00:35.770 --> 01:00:37.080
Norris: 非常可替代。

626
01:00:39.670 --> 01:00:46.160
Fan Yunjie: 你是通过这种你本身你的程序员想法来说的，但是你如果感性一点来说呢。

627
01:00:46.310 --> 01:00:50.920
Norris: 不是你这么说吧，没有人想死你让我现在去跳楼，我不想跳，

628
01:00:52.370 --> 01:00:53.120
Norris: 对吧，这个。

629
01:00:53.940 --> 01:01:01.790
Norris: 这个是一个最基础的就是人活著，就是为了生存基因，活著是为了生存，所有人都是为了生存。

630
01:01:04.350 --> 01:01:07.150
Fan Yunjie: 对呀，外形思维并不是生存的一部分呀。

631
01:01:08.290 --> 01:01:19.810
Fan Yunjie: 确实，所以我们现在来说的话，你认为人类它是本身可替代的这个问题，我觉得它其实是值得被拜托的比方说，我现在在跟你聊天，我觉得很开心。

632
01:01:19.810 --> 01:01:21.440
Norris: 你也不会是。

633
01:01:21.610 --> 01:01:26.280
Fan Yunjie: 可替代的呀。我可能这一个礼拜，我跟谁聊天，都没有跟你聊天，开心。

634
01:01:26.280 --> 01:01:26.860
Norris: 对。

635
01:01:29.720 --> 01:01:30.620
Fan Yunjie: 是不是？是？

636
01:01:30.620 --> 01:01:32.760
Norris: 这个我没有空跟你说，呵。

637
01:01:32.760 --> 01:01:36.580
Fan Yunjie: 没有我在最喜欢别人跟我唱反调，真的。

638
01:01:36.580 --> 01:01:38.270
Norris: Okay. Good.

639
01:01:38.270 --> 01:01:41.430
Fan Yunjie: 只是谈恋爱，不要跟我唱反调就可以了。

640
01:01:41.780 --> 01:01:42.510
Norris: 啊啊。

641
01:01:43.510 --> 01:01:48.110
Fan Yunjie: Okay。所以其实你是认为首先，第一个

642
01:01:48.320 --> 01:02:03.260
Fan Yunjie: 人类本身它也它就不是一个完全不可替代的生物，即使将来我们说的什么？Ai与人类道德伦理战真的开始了，人类可能变成了一个弱势，你也是在心里welcome这个状态的，对吗？

643
01:02:05.630 --> 01:02:13.960
Norris: 我不敢说我welcome这个状态，就是说，我如果作为一个个体来说，没有想被统治的欲望。

644
01:02:16.780 --> 01:02:18.840
Fan Yunjie: 对呀，当然了。

645
01:02:18.840 --> 01:02:19.660
Norris: 二，至少。

646
01:02:20.600 --> 01:02:27.360
Fan Yunjie: 当然了，这个就是我们觉得我们现在讨论这个课题的存在的很大的意义吧？对吧？

647
01:02:28.500 --> 01:02:29.580
Norris: En

648
01:02:29.860 --> 01:02:38.920
Norris: 对你我这么说吧，这个你们现在的这个thesis非常有意义，就是你能尽量延缓人类被灭绝的时间。

649
01:02:41.240 --> 01:02:43.950
Fan Yunjie: 你是不是延缓了这个词。

650
01:02:43.950 --> 01:02:50.470
Norris: 对，但是从事实上来讲就是这个人类被灭绝有好多好多好多种可能性，

651
01:02:50.850 --> 01:02:54.650
Norris: 但是最可能的反而是小行星坐地球这种

652
01:02:54.770 --> 01:02:58.570
Norris: 我们完全不是通过地球上做的努力，能改变的事情。

653
01:03:00.800 --> 01:03:02.960
Fan Yunjie: 但是你会觉得说，

654
01:03:03.970 --> 01:03:11.920
Fan Yunjie: 只要人类不master ai，只要人类不去，不去限制它的发展，它就

655
01:03:12.040 --> 01:03:18.270
Fan Yunjie: 他就真的不会真正意义上的去去去去存在这种很大程度上的

656
01:03:18.400 --> 01:03:21.150
Fan Yunjie: 争议性的结果，对吗？

657
01:03:25.120 --> 01:03:27.940
Norris: 如果人类不master的话

658
01:03:28.400 --> 01:03:32.080
Norris: 还是master人类啊？没没太搞搞清楚这个逻辑。

659
01:03:32.600 --> 01:03:33.760
Fan Yunjie: 就是说

660
01:03:34.090 --> 01:03:41.870
Fan Yunjie: 现在我们都要谈论的是ai发展，那我觉得很大程度上是限制ai发展，对吧？怎么样去

661
01:03:42.050 --> 01:03:45.090
Fan Yunjie: 操控他，怎么样的去他。

662
01:03:45.660 --> 01:03:51.740
Fan Yunjie: 如果说我们现在就是把这个问题就抛开，不看我们去完全让它发展，

663
01:03:52.410 --> 01:03:55.470
Fan Yunjie: 你觉得也不会存在最终的那个问题，对吗？

664
01:03:56.780 --> 01:03:58.280
Norris: 完全让它发展，

665
01:03:58.610 --> 01:04:07.980
Norris: 完全让他发展，就是我们给他足够的算力，然后足够的资料，然后他就可以发展成超级人工智能。

666
01:04:08.210 --> 01:04:12.430
Fan Yunjie: 就是也不不不不去care什么道德伦理学这些全部都不care。

667
01:04:12.430 --> 01:04:16.220
Norris: Okay，okay。然后你想说它会发展成什么样子。

668
01:04:16.830 --> 01:04:18.810
Fan Yunjie: 就我们最最去

669
01:04:19.210 --> 01:04:23.790
Fan Yunjie: 担心的普罗大众最会担心的人类被ip那种事情。

670
01:04:25.310 --> 01:04:26.060
Norris: Ok

671
01:04:26.460 --> 01:04:27.640
Norris: 会被替代啊

672
01:04:29.730 --> 01:04:31.640
Norris: 肯定啊？因为

673
01:04:33.970 --> 01:04:39.100
Norris: 超级人工智能就像人类治愈蚂蚁嘛，你不在乎一个医学崩不崩溃，

674
01:04:39.230 --> 01:04:43.110
Norris: 你也不在乎人类生不生存，你不会有意的去毁掉一个医学。

675
01:04:43.420 --> 01:04:47.780
Norris: 但是如果他真的打了你的路，你交一罐立星上去铺个了路，

676
01:04:48.060 --> 01:04:49.550
Norris: 也是很正常的事情。

677
01:04:54.480 --> 01:04:56.750
Fan Yunjie: 你是觉得这种你可以接受吗？

678
01:04:57.180 --> 01:05:00.920
Norris: 我不接受这个世界上没有什么我的意愿。

679
01:05:04.080 --> 01:05:11.890
Fan Yunjie: 但是这就是变成比较相悖的了。像像学术者为什么要出去做这个问题，就是因为他不接受啊，他要去。

680
01:05:12.740 --> 01:05:15.470
Norris: 对啊，那他去努力嘛，那他去fight嘛。

681
01:05:16.600 --> 01:05:28.940
Fan Yunjie: 所以你觉得本身ai的存在，它是为了去节省一些生产力的指标，还是说它本身是辅助人类去去有一个更好的一个输出，对。

682
01:05:29.350 --> 01:05:37.160
Norris: 应该boss吧，就是首先，社会生产力总就是在下降嘛，不然也不会出现经济萎缩的问题，

683
01:05:37.970 --> 01:05:47.690
Norris: 然后人口也在减少，所以需求在减少，需求在减少，所以市场在萎缩，市场在萎缩，所以生产在萎缩，就是整体都在下行嘛。

684
01:05:48.110 --> 01:05:54.730
Norris: 那你说需要什么样的资源来输入呢？需要外部资源，一个

685
01:05:55.070 --> 01:06:05.310
Norris: 一个内部系统，它的商混乱是由于只有内部系统能量消耗造成的，它唯一的解决办法就是输入外部能量，那外部能量来自于哪里？

686
01:06:05.470 --> 01:06:10.450
Norris: 来自于更多的人口，或者来自于外太空的资源。

687
01:06:11.510 --> 01:06:16.020
Norris: 那么这两个都导致说，我们必须要由aa来

688
01:06:16.150 --> 01:06:18.140
Norris: 就是帮助人类吧。

689
01:06:18.530 --> 01:06:22.580
Fan Yunjie: 人类发展ai肯定是希望能解放生产力的。

690
01:06:22.750 --> 01:06:27.070
Norris: 就像把这个妇女平权也是为了解放生产力是一样的。

691
01:06:29.310 --> 01:06:38.360
Fan Yunjie: 那有，那那就存在问题了呀，那又存在真正意义上让ai变成更好的ai，人类变成更好的人类的这个问题了呀。

692
01:06:38.570 --> 01:06:44.100
Norris: Ai会变成更好的ai，但是一旦这个轮子已经启动了，它是没有刹车的，

693
01:06:45.450 --> 01:06:47.370
Norris: 而且你人加不上刹车。

694
01:06:48.290 --> 01:06:50.620
Fan Yunjie: 我想知道这个壁垒在哪里，真正的壁垒。

695
01:06:50.620 --> 01:06:58.640
Norris: 人类加不上砂石的壁垒在于它的底层逻辑。Ai有两种算法，一个是diffusion模型，一个是transformer模型对吧？对。

696
01:06:59.350 --> 01:07:02.750
Norris: 你可以理解为你现在有一盘军司，

697
01:07:03.100 --> 01:07:09.230
Norris: 然后你在A点放了一个食物，他一定会找到最短的路径，把这个军思蔓延到那个a，那个地方去。

698
01:07:11.130 --> 01:07:16.510
Norris: 这个就是。这个defeat模型就是它先假设所有的路径，然后再收拢。

699
01:07:17.980 --> 01:07:26.470
Norris: 然后呢？Transformer模型是说我不断的预测下一个字节是什么，只有足够的电力，我就必然能预测出最优解的概率。

700
01:07:28.860 --> 01:07:33.340
Norris: Okay，那么这两种模型你看得到有任何的

701
01:07:33.530 --> 01:07:35.160
Norris: 拐弯的可能吗？

702
01:07:35.980 --> 01:07:39.250
Norris: 就是如果能量充足的情况下，他是不会拐弯的。

703
01:07:42.040 --> 01:07:45.100
Fan Yunjie: 能量充足就是一个先决条件啊。

704
01:07:45.540 --> 01:07:47.390
Norris: 能量充足是必然的呀。

705
01:07:48.830 --> 01:07:55.520
Fan Yunjie: 怎么说能让政治变？那现在本身你要说顺利的消耗现在已经也是很大的一个问题啊。

706
01:07:55.520 --> 01:08:05.410
Norris: 是的，但是你单从这个晶圆厂或者是需要产能的这个角度来说，能量其实是无限的。由于太阳能的供给对。

707
01:08:06.940 --> 01:08:15.720
Fan Yunjie: 你如果说一定要这样子说的话，那大家肯定会认为它是无限的，但是人类会相信别人告诉它的呀。

708
01:08:16.420 --> 01:08:20.090
Norris: 没没没听懂这个怎么跟相信又扯上关系了。

709
01:08:20.670 --> 01:08:32.870
Fan Yunjie: I mean。你现在告诉所有人现在是这个地球面临了能源危机，大家就会相信他有能源危机，其实他的能量他是很荣誉的。

710
01:08:32.870 --> 01:08:35.250
Norris: 对吧？所以你看人多傻，要人干嘛。

711
01:08:38.000 --> 01:08:41.580
Fan Yunjie: Ok，以后就我们两个，这种存在在世界上就可以了。是不是。

712
01:08:41.580 --> 01:08:46.620
Norris: 太聪明不好，太聪明，活累。

713
01:08:46.620 --> 01:08:51.060
Fan Yunjie: 我不聪明，我这个人很笨，我这个人的脑子都不拐，弯我跟原来一样的

714
01:08:51.520 --> 01:08:52.170
Fan Yunjie: 对。

715
01:08:52.170 --> 01:08:56.830
Norris: 你看你还不聪明？So so，so good，so great pong。

716
01:08:57.450 --> 01:09:05.560
Fan Yunjie: 你你看我的那个网名都是那个oic。所有人问我，那个osc是什么意思？我说我就是个operation systems。

717
01:09:05.609 --> 01:09:08.410
Norris: 这个意思吗？我以为是oversee。

718
01:09:08.880 --> 01:09:09.649
Fan Yunjie: 对，对，

719
01:09:09.859 --> 01:09:12.290
Fan Yunjie: all standings.

720
01:09:12.960 --> 01:09:14.210
Fan Yunjie: 哈哈哈。

721
01:09:14.580 --> 01:09:15.630
Norris: Outstanding.

722
01:09:15.630 --> 01:09:24.540
Fan Yunjie: Okay，anyway。我觉得我觉得你的，你的想法都很有趣，我真的觉得跟你聊天挺好有意思的。

723
01:09:24.720 --> 01:09:26.810
Norris: 好吧，是不是？

724
01:09:26.810 --> 01:09:33.250
Fan Yunjie: 多聊吧，多聊吧。没想到这样子，跟跟你聊天，我就先把它stop recording一下吧。

725
01:09:34.590 --> 01:09:36.220
Norris: 我都忘了还在record。

726
01:09:36.760 --> 01:09:41.790
Fan Yunjie: 对啊，你，你都回头回头，我都要给你打码，说了好几次脏话。

727
01:09:41.979 --> 01:09:43.579
Norris: 是吗？这个还在乎吗？