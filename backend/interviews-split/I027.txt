受访人27:
WEBVTT

1
00:00:00.570 --> 00:00:01.820
Chin Ee Moon: Okay?

2
00:00:02.140 --> 00:00:12.740
yunjie.fan@ntu.edu.sg: 所以整体来说的话，这个研究的目的还是在聚焦于我们ai，还有我们本身的这个人类的critical thinking这一块，

3
00:00:12.740 --> 00:00:23.960
yunjie.fan@ntu.edu.sg: 听说有看到你刚刚在这个面试之前有看到你的这questionnaire，所以你本身是做research相关工作的，是吗？对。

4
00:00:23.960 --> 00:00:31.620
Chin Ee Moon: 是的，我是做比较多相关的那个工作，然后目前是在工作。

5
00:00:32.880 --> 00:00:39.670
yunjie.fan@ntu.edu.sg: 那你的这个研究的主要的一些方向。还有一些具体的课题，你方便介绍一下吗？

6
00:00:39.670 --> 00:00:55.570
Chin Ee Moon: 我的研究方向是，现在我主要是在做那个就是就是一个要。然后他想要把一个把一个protein

7
00:00:55.570 --> 00:01:11.050
Chin Ee Moon: 和一个拉在一起，然后要那个不好的啊。大致上的意义就是这样，就是就是一个，然后把两个东西黏在一起，然后让那个让那个让一个

8
00:01:11.150 --> 00:01:17.090
Chin Ee Moon: 好的好的去吃掉另外一个不好的，大致大概是这样的，方向。

9
00:01:17.580 --> 00:01:28.360
yunjie.fan@ntu.edu.sg: 好的，好的我能理解。所以你觉得你在你的这个研究或者说你的相关的工作过程当中使用ai比较多吗？

10
00:01:28.810 --> 00:01:39.600
Chin Ee Moon: 主要使用的还是这种ai会比较多，但是我觉得我是不会完全依赖在

11
00:01:39.600 --> 00:01:53.650
Chin Ee Moon: 就是他们给我的答案，然后我就直接拿来用，这样就是我还是会去跟我做的那个literary核对一下，然后看怎么样再选择，要不要就是接受那个ai的建议。

12
00:01:54.130 --> 00:01:58.000
yunjie.fan@ntu.edu.sg: 好的那你觉得一般什么样的场景会使用到ai啊？

13
00:01:58.550 --> 00:02:20.640
Chin Ee Moon: 就是比如说我对一个一个一个方向，或者是一个课题不太了解的时候，初步的那个我就会通过ai来缩小范围，然后后面我会再去google找相关的一些文章来看，因为我主要是我发现到那个ai给我的文章很多都不太靠谱

14
00:02:20.660 --> 00:02:38.350
Chin Ee Moon: 就是，就是他给那个对我来说是还行的。但是就是说，如果是在更深或者是更的问题，上面他们的答案还是就是跟还有相当大的差距。

15
00:02:39.120 --> 00:02:45.790
yunjie.fan@ntu.edu.sg: 那你是如何发现他的这个这个弊端是你直接通过经验吗？还是。

16
00:02:45.790 --> 00:03:03.790
Chin Ee Moon: 就是可能有时候我就会直接在那边开点开那个。就是让他也去搜那些网站，然后我就说，给我一些，然后我发现到他给我的跟我要找的资料，其实差距还蛮大的，就是等于说他乱给这样。

17
00:03:04.650 --> 00:03:11.930
yunjie.fan@ntu.edu.sg: 那你觉得如果这样子的情况之下，它有多大的可信性，或是有多大的可应用性，它的结果对。

18
00:03:12.240 --> 00:03:13.810
Chin Ee Moon: 它的结果吗？

19
00:03:13.980 --> 00:03:20.730
Chin Ee Moon: 直接应用的话，我觉得少于50。50%吧，就是现在来看的话。

20
00:03:21.880 --> 00:03:26.200
yunjie.fan@ntu.edu.sg: 这样来看的话，那你仍然还会有很高的使用频率吗？

21
00:03:26.200 --> 00:03:45.010
Chin Ee Moon: 我还是会有比较高的使用频率，就是我说我会先去一些了之后，然后我再去google上面核对，或者是更。问google一些更直接的问题，这样就是还是一个很好的工具来帮我，缩小我的范围。

22
00:03:45.970 --> 00:03:56.540
yunjie.fan@ntu.edu.sg: 请问你在这个过程当中是用你的母语，中文还是用英文去进行相关的这个疑问，或者说是相关的，这个这个信息给予啊。

23
00:03:56.740 --> 00:04:11.660
Chin Ee Moon: 我用的都是英文，因为就是我的教育背景和我的。就是做研究的那些研究所都是主要主要还是用英语比较多，华语就是口语还行，但是研究上不怎么用华语。

24
00:04:11.660 --> 00:04:28.450
yunjie.fan@ntu.edu.sg: 好的，好的，好的，那我了解。那面对这样的问题，你有没有去想过说去question ai，或者说是想要通过另外一个ai模型去跟它相辅相成的，去论证或者验证这样的事情呢？

25
00:04:28.660 --> 00:04:32.440
Chin Ee Moon: 有说法吗？在

26
00:04:32.440 --> 00:04:55.180
Chin Ee Moon: chatgpt我就会使用几个不一样的模式来来对比他们的答案就是那个O3，然后那个四，然后现在最近还有一个五，就是我会对比他们的答案。然后现在最近我也会用那个的那个来来来，就是进行一些一些对比。然后我觉得那个google的ai mode会相对来讲，给到我比较

27
00:04:55.180 --> 00:05:03.110
Chin Ee Moon: 我，我比较可可信任的答案就是，他会直接在google上面收一堆的那个给我这样子。

28
00:05:03.110 --> 00:05:08.740
yunjie.fan@ntu.edu.sg: 明白明白你有没有接受过。engineering的这个培训呀。

29
00:05:09.100 --> 00:05:10.060
Chin Ee Moon: 没有。

30
00:05:10.060 --> 00:05:10.930
yunjie.fan@ntu.edu.sg: 没有了好的。

31
00:05:10.930 --> 00:05:11.680
Chin Ee Moon: 没有。

32
00:05:11.830 --> 00:05:18.640
yunjie.fan@ntu.edu.sg: 好的。那现在你会不会有那种场景去刻意的不用ai。

33
00:05:19.130 --> 00:05:26.480
yunjie.fan@ntu.edu.sg: 比方说某一些研究工作或者某一些的这个特定的解决方式的时候，你会刻意的不用去ai。

34
00:05:27.220 --> 00:05:28.960
Chin Ee Moon: 刻意的不用ai。

35
00:05:28.960 --> 00:05:29.690
yunjie.fan@ntu.edu.sg: 对。

36
00:05:29.690 --> 00:05:45.390
Chin Ee Moon: 可能就是说，当那些资讯就是我要找的东西比较的情况下，我还是会靠自己的思考和就是google搜一些关键词比较多，不会直接把那个我的问题，到ai去

37
00:05:45.650 --> 00:05:49.610
Chin Ee Moon: 对，就是主要还是那个information safety的问题。

38
00:05:50.300 --> 00:05:55.220
yunjie.fan@ntu.edu.sg: 那您觉得就是比方说，您刚刚有提到的这种信息误差性，

39
00:05:55.220 --> 00:06:19.420
yunjie.fan@ntu.edu.sg: 你觉得多大程度之上可以可以避免这种错误存在。如果说你作为一个产品设计的人员，或者说是如果你在这个ai应用当中，不是通过自己来发现问题，你是希望如何能够看出来整个ai给你呈现的结果可以更直观的这种出错性，你有没有这种这种相关的想法呢？

40
00:06:20.030 --> 00:06:25.100
Chin Ee Moon: 就是怎么设计那个ai，它会它的那个对的几率会更大，对吧？

41
00:06:25.100 --> 00:06:36.240
yunjie.fan@ntu.edu.sg: 一个是这方面，另外一个是你认为ai这个，因为我们其实看到ai，它目前还是一个对话形式的存在，你给它一个什么信息，它可能很直观的给你看到一些信息，你其实

42
00:06:36.240 --> 00:06:55.830
yunjie.fan@ntu.edu.sg: 像你来说的话是比较有变质性思考的，人，比方比较有批判性的，这些思考的，但是很多可能并没有这一种能力的一些受众，他并不能绝对的分辨ai，的它的这个信息，它的这个准确性或者说是它的可用性，

43
00:06:55.830 --> 00:06:56.440
yunjie.fan@ntu.edu.sg: 你觉得

44
00:06:56.440 --> 00:07:05.130
yunjie.fan@ntu.edu.sg: 有哪些？一个是从直观上来看，或者说是从它的，它的这个功能上来看，可以会更好的去避免这样的薄弱环节。

45
00:07:05.530 --> 00:07:09.790
Chin Ee Moon: 我感觉就是，

46
00:07:10.360 --> 00:07:13.220
Chin Ee Moon: 把那个ai跟google

47
00:07:13.220 --> 00:07:30.480
Chin Ee Moon: 做更好的会很好，会会会更好，就是所有ai说过的话，如果都有reference去去帮助，就是去提升他说的那句话的那个意义的话，我感觉可能就能够更能够看出它的准确性有多少。

48
00:07:30.860 --> 00:07:31.810
yunjie.fan@ntu.edu.sg: 明白，

49
00:07:32.320 --> 00:07:38.670
yunjie.fan@ntu.edu.sg: 所以你在使用的时候，你大概是一个以什么样的口吻去

50
00:07:38.810 --> 00:07:54.560
yunjie.fan@ntu.edu.sg: 去费它呢？是是你扮演某种角色，还是说，你去让ai去尝试扮演某种角色，比方说你的，你的这个。你的这个peer或者说是你的一些reviewer，你会不会有这样的一些环节？

51
00:07:54.990 --> 00:08:19.870
Chin Ee Moon: 就是我会跟他说，现在我要我要做的task主要是什么，然后我需要他给我怎样的？就是。然后我会说，可能比较比较的一个方向就是多少个字啊，然后要限制在怎么样啊？然后我的观众会是什么样的群体的人，我需要他帮我写的有多professional多严谨，这些我都会先提前给他，然后之后他就给我一个，

52
00:08:19.870 --> 00:08:30.650
Chin Ee Moon: 我觉得就是把那个方向缩小了之后，他给的答案会更加的准确一些。比起你就很的问他一个很的问题，这样。

53
00:08:31.510 --> 00:08:44.980
yunjie.fan@ntu.edu.sg: 那一般ai给出的这些答案，有一些它是有reference的，你可以看出它的这个漏洞。那如果说直接性的答案，你怎么能辨证它的这个真实性或者可用性呢？

54
00:08:44.980 --> 00:08:49.540
Chin Ee Moon: 那如果有一些是

55
00:08:49.540 --> 00:09:13.030
Chin Ee Moon: 比较属于那种的问题，就是没有那么多相关的问题的话，那我觉得我还是会直接使用的，但是如果他说到了一些跟相关，我需要用一些去支持他的这种说法的时候，可能我就会把那他说的那一句话，整个看下有没有相关的资料，就是我能不能够在那些上面

56
00:09:13.030 --> 00:09:23.670
Chin Ee Moon: 到跟他同样跟ai写出同样的观点来来这样辨认，就是我能不能够把那个句子这样写下去，或者是应用去我想用的地方。

57
00:09:23.960 --> 00:09:35.600
yunjie.fan@ntu.edu.sg: Okay。所以说你还是会很大程度上需要搜索引擎的支持，然后去辅助你去验证这个ai的内容是否有足够的。这个背书性，对吧？

58
00:09:35.600 --> 00:09:36.120
Chin Ee Moon: 对对。

59
00:09:36.120 --> 00:09:44.680
yunjie.fan@ntu.edu.sg: 那在这个过程当中的话，你有没有去把你已经返补回来的资料，再重新给到ai去让它优化之类的。

60
00:09:44.870 --> 00:10:06.400
Chin Ee Moon: 我会的就是。就是我，我找了一遍之后，然后我就会收集所有的那些资料就是不管是ai给的还是我透过google找到的，我都会写下来，然后过后我再进去，然后叫他帮我improve那个内容，然后在这个过程中，其实在写作方面，我觉得他帮到我是挺多的，

61
00:10:06.400 --> 00:10:13.290
Chin Ee Moon: 就是他真的能够帮我。improve我的整个我的整个flow。那个写作方面对

62
00:10:13.560 --> 00:10:14.970
Chin Ee Moon: 这样的帮助还是挺大的。

63
00:10:14.970 --> 00:10:15.500
yunjie.fan@ntu.edu.sg: 为什么

64
00:10:15.610 --> 00:10:30.340
yunjie.fan@ntu.edu.sg: 对在你整个去进行research这个环节，我方便问一下，整个在这个流程当中，ai充当了一个大概什么样的角色吗？它只是帮你去润色你的内容吗？还是说给您提供一个相对的思路。

65
00:10:30.700 --> 00:10:31.170
Chin Ee Moon: 对对。

66
00:10:31.170 --> 00:10:34.960
yunjie.fan@ntu.edu.sg: 大概的这个，这个总体的角色会是怎样的。

67
00:10:35.350 --> 00:10:56.460
Chin Ee Moon: 我觉得它给我一个思路之后，然后我会再从那个大框架那里做出一些调整，跟自己的思考，然后过后把我的内容先编辑出来，然后之后呢，再利用它去优化，所以我觉得它就是充当一个前面跟一个后面这样的角色，然后，中间的那一段就是由我自己来补充。

68
00:10:56.680 --> 00:10:59.680
yunjie.fan@ntu.edu.sg: 明白那有没有这种情况，ai

69
00:10:59.820 --> 00:11:14.930
yunjie.fan@ntu.edu.sg: 在最一开始其实就给您误导到了一个错误的方向，然后当然，因为他本身，你认为他很聪明，他的可信性很强，你伴随着他的这个误导，走了很长的时间。有没有这种情况？

70
00:11:14.930 --> 00:11:19.900
Chin Ee Moon: 有时有时候也是也是会有的。但是。

71
00:11:19.900 --> 00:11:43.450
Chin Ee Moon: 就可能之前这样，这样就是有发生这样的情况，我没发现的时候，最后还是通过，就是问了别人比较一点的那些啊，或者还是我的这样，然后他们才给我说其实有时候A那个ai讲的不太可信，你从一开始就就就不太能相信了，这样就是还是有，但是我觉得还是比较小的情况，因为通常如果

72
00:11:43.450 --> 00:11:46.600
Chin Ee Moon: 我需要用ai来帮我制造一些

73
00:11:46.660 --> 00:12:00.380
Chin Ee Moon: 开头的思路的时候，我给他的问题都会偏比较general一些，然后我之前也说了，就是通常general没那么高的的时候，ai的那个信任度还是蛮高的。

74
00:12:00.620 --> 00:12:10.210
yunjie.fan@ntu.edu.sg: 明白你方便就是大概给我一个什么样的方向，它是相对来说依据较少可能会导致出错的吗？

75
00:12:10.370 --> 00:12:31.570
Chin Ee Moon: 因为我做的工作比较多。是，跟嘛。然后这个在化学上面，有时候我就问他说，为什么我的这一步实验会会出错呢？那个我就给他了我的。然后我就问他，那个出错的原因在哪里，为什么我的那个又会那么低，然后他就会开始给我乱说一通。

76
00:12:31.570 --> 00:12:39.610
yunjie.fan@ntu.edu.sg: 所以这种这种错误答案，您是通过什么样的思维发现它是它是错误的。

77
00:12:39.990 --> 00:12:40.580
Chin Ee Moon: 就是你。

78
00:12:40.780 --> 00:12:43.140
yunjie.fan@ntu.edu.sg: 尝试着去纠正它吗。

79
00:12:43.420 --> 00:13:08.250
Chin Ee Moon: 我就会先去询问一些比我有经验的人问他们说，就是。我发现有这样的问题，然后。然后。有没有可能我的问题，我，我的这个实验出现了这样的问题，是因为一，二，三原因，which is那个ai跟我讲的答案，然后他们就说啊，好像不太正确，好像不太是这样的，这样我就发现了，然后我就会再去google

80
00:13:08.250 --> 00:13:13.600
Chin Ee Moon: 再去找其他的有可能的那个，那个问题是是在哪里？这样

81
00:13:13.600 --> 00:13:20.620
Chin Ee Moon: 对，就是在这种比较特定的一个场景之下，可能ai给的答案会没有的那么

82
00:13:20.690 --> 00:13:22.430
Chin Ee Moon: 准确。对。

83
00:13:22.770 --> 00:13:36.710
yunjie.fan@ntu.edu.sg: 了解了解。所以你在发现问题之后，你是更大程度上依靠你自己去进行这个问题的解决，还是说我在发现问题之后，我告诉他问题在哪，然后再让他去给我另外一个答案呢？

84
00:13:36.870 --> 00:13:48.640
Chin Ee Moon: 没有比较少，就是通常我发现他错了，我就不会再问他。就是相关的问题，我就会直接去找其他的地方就是就是答的答案对。

85
00:13:49.060 --> 00:14:08.640
yunjie.fan@ntu.edu.sg: 明白。所以说正常来讲，我们整个一个一个research project过来之后。你是大概去怎么样去跟他进行沟通互动的是，你首先会告诉他我整个的这一个实验他的一个前提，然后你们会在一个对话框里面去完成你整样整个的这个协作过程是吗？

86
00:14:08.640 --> 00:14:14.570
Chin Ee Moon: 对对，对，通常我就会针对一个问题，然后我就开一个这样。

87
00:14:14.570 --> 00:14:32.840
Chin Ee Moon: 然后如果是下一个问题了，我就会去下一个，因为有时候那个ai它会一直延续下去，就是可能你到不一样的问题了，但是它，因为还在同一个那个框里面，然后它就会再refer回之前你问的问题有时候已经是没有相关的问题了，但它硬硬会跟你扯关系。这样。

88
00:14:33.490 --> 00:14:37.700
yunjie.fan@ntu.edu.sg: 方便问一下您现在用的是gpt的哪一个模型呢？

89
00:14:38.060 --> 00:14:44.210
Chin Ee Moon: 好等一下，我看一下，好像是那个gpt5吧，就是我有那个subscription。

90
00:14:44.400 --> 00:14:46.600
yunjie.fan@ntu.edu.sg: 有有到plus版本是吧？

91
00:14:47.520 --> 00:14:50.760
Chin Ee Moon: 对，就是对，就是那个plus版本。

92
00:14:50.760 --> 00:15:00.880
yunjie.fan@ntu.edu.sg: 了解了解，就您刚刚提到的问题，其实很多人会会有同样的这个，这个发现这个弊端，就是说ai它其实会。

93
00:15:00.880 --> 00:15:03.740
Chin Ee Moon: 思路混乱，而且他会记忆混乱。

94
00:15:03.740 --> 00:15:18.570
yunjie.fan@ntu.edu.sg: 可能在您继续下一个subject的时候，在某一个特性点的时候，他仍然会提起之前的答案。所以你觉得如果这种答案框定的这种这种问题你觉得有没有可能避免呢？

95
00:15:19.350 --> 00:15:44.200
Chin Ee Moon: 有没有可能避免？就是在我跟他的沟通过程中，可能我就会在开始一个新的的时候，我就先提前的跟他说，就是这已经跟上面的那个问题不一样了。所以现在是一个。我们要开启一个新的一个，然后这个是关于什么什么，然后再给他一个新的。我觉得这样或许会比较好，但我还是会比较prefer，就是开一个新的那个

96
00:15:44.200 --> 00:15:44.980
Chin Ee Moon: check boy.

97
00:15:45.320 --> 00:15:59.960
yunjie.fan@ntu.edu.sg: 了解，了解，所以您会不会去，比方说就某一个问题让他给您like a，b test2个答案，然后你自己去辩证的去给到相应的这个继续的执行应用，这样。

98
00:16:00.450 --> 00:16:19.120
Chin Ee Moon: 会的，它有时候会就会弹出来，就是给了response a跟response b。然后他问你，你比较喜欢哪一个？但是我觉得内容上来说的话，其实两个response的那个内容会差不多，一样，可能只是呈现的方式没有太大的差距，所以其实我也不太介意这个东西。

99
00:16:19.320 --> 00:16:37.420
yunjie.fan@ntu.edu.sg: 了解，但是就ai出错的这个问题，因为您是也是比较敏感度的可以发现ai的问题有没有可能就是在这个过程当中，您可能设立一些对立性的问题，让ai去提一些反问的，反对的。这些论证，这个过程有吗？对。

100
00:16:37.610 --> 00:16:39.850
Chin Ee Moon: 没有，暂时还没尝试过。

101
00:16:40.230 --> 00:16:48.230
yunjie.fan@ntu.edu.sg: 了解了解，那您觉得如果说就激发ai它本身的一种批判性思维的话，可以通过哪些方式啊？

102
00:16:50.140 --> 00:17:11.099
Chin Ee Moon: 就是跟他说就是。比如说他给了一个答案，然后就跟他说，你错了，你可以再想想嘛，然后可能他又会想一个新的，但是我觉得这个。我不确定没试过，但是我不知道这样的方法会不会让他那个思考变得更加的混乱，然后后面给的答案就越来越偏离那个那个，那个topic。这样。

103
00:17:11.490 --> 00:17:20.310
yunjie.fan@ntu.edu.sg: 明白，所以其实因为我看您年纪还小了，所以您在这个ai使用的时候是从哪一年开始的。

104
00:17:21.109 --> 00:17:37.889
Chin Ee Moon: 就是那个刚刚开始的时候吧，2023年吧，但是我就是很很时常的真的是使用它应该也是从2024年大概七，8月左右，对。

105
00:17:38.650 --> 00:17:51.480
yunjie.fan@ntu.edu.sg: 明白，那您觉得在此前跟之后您自己的这个思考有很大的变化吗？或者说针对于一些这个您本身工作，还有专业领域的这种思路。

106
00:17:51.990 --> 00:18:01.060
Chin Ee Moon: 我觉得他确实是帮我省了很多的时间，但是同时我也觉得可能我的思考会比以前

107
00:18:01.060 --> 00:18:10.560
Chin Ee Moon: 浅了挺多，因为就是我会对他有一定的那个依赖性，就是遇到了什么不知道的，我就会想要去问他，而不是先自己去思考一遍，

108
00:18:10.560 --> 00:18:21.220
Chin Ee Moon: 这样，除非我真的从他那里问不出问题，然后我才会想办法自己去再去思考，那可能就跟以前我还没开始用ai的时候，就有一一点点不一样了。

109
00:18:21.780 --> 00:18:33.240
yunjie.fan@ntu.edu.sg: 了解。所以说你觉得这种其实是对于学者或说是对于研究员来说，他是更多的prone才是更多的。

110
00:18:35.330 --> 00:18:58.130
Chin Ee Moon: 我觉得还是主要是看个人的那个使用方式吧，就是如果你有意识到，就是他可能会阻拦你一些思想，一些思考，那你就还是的去跟自己说，我一定要去想那个问题在哪里，是是因为怎么样，然后要去把自己的那个思考，投入在自己的工作里面，那我觉得他其实pro

111
00:18:58.130 --> 00:19:06.100
Chin Ee Moon: 挺多的，就是在协作上面，它帮你节省了时间，然后可能一开始的那个构架上也会帮助你，就是

112
00:19:06.130 --> 00:19:18.510
Chin Ee Moon: 只要配合的好，我觉得其实它是好的，但是如果是过度的使用，或者是就让它压过了你所有的思考，那可能就不好了。所以还是个人原因比较多。

113
00:19:19.300 --> 00:19:27.910
yunjie.fan@ntu.edu.sg: 了解，所以您现在的整个工作的协作方式，你觉得更大程度上是跟ai还是说跟您个人的团队啊。

114
00:19:31.710 --> 00:19:36.980
Chin Ee Moon: 就是写作嘛，就是在在在写东西上面，我觉得跟ai会比较多。

115
00:19:38.420 --> 00:19:56.220
yunjie.fan@ntu.edu.sg: 明白了解您，您觉得如果就您的专业领域去给到未来的研究人员跟ai协作的建议的话，如果说你要提出一个这个相关的guideline，你觉得如果说你去keep三点，你觉得哪三点是比较重要的。就您的专业来讲。

116
00:19:57.730 --> 00:20:02.890
Chin Ee Moon: 就是怎么让人跟ai有更好的合作，在写作上面。

117
00:20:02.890 --> 00:20:03.940
yunjie.fan@ntu.edu.sg: 对对。

118
00:20:06.060 --> 00:20:07.510
Chin Ee Moon: 我感觉

119
00:20:07.710 --> 00:20:10.910
Chin Ee Moon: 第一点肯定就是。

120
00:20:11.520 --> 00:20:35.430
Chin Ee Moon: 可能说吧，就是有时候ai可能两个人问他一样的问题，他就会很相似的答案，那就很容易出现那种的问题啊，或者是那个ai ai写作的那个感觉太重了。我觉得如果能够让他写作的更自然，更像人一些的话，我觉得那那会更好。

121
00:20:35.580 --> 00:20:39.690
Chin Ee Moon: 对，这是这是第一点，就是可能他也能够根据。

122
00:20:39.970 --> 00:20:49.540
Chin Ee Moon: 每个人的一些需求，或者是研究的一些不一样的地方，上面做出一些写作上面的调整。

123
00:20:49.540 --> 00:20:50.110
yunjie.fan@ntu.edu.sg: 那个。

124
00:20:50.110 --> 00:20:53.880
Chin Ee Moon: 那那就会比较好。这是第一点吧，我觉得。

125
00:20:55.990 --> 00:21:03.590
Chin Ee Moon: 这也是主要我现在看到的一点对，就是有时候它真的出来，你就很明显的看得出它就是一个ai写的。

126
00:21:05.230 --> 00:21:13.750
Chin Ee Moon: 是它的惯用词吧，对它的惯用词跟它的句子构造对这两点是主要还是比较明显的。

127
00:21:14.750 --> 00:21:29.190
yunjie.fan@ntu.edu.sg: 那本身对于这种问题，你觉得它更大程度上的给你带来的这这这个这个壁垒在于它的形式还是在于它的内容啊，因为其实如果说就研究来讲的话，

128
00:21:29.190 --> 00:21:43.700
yunjie.fan@ntu.edu.sg: 它更大程度上要看这个内容的严谨性啊，或者说是启发性啊等等的。其实ai如果说就您来讲的话，它本身在内容框架上可能更偏了一种一种这个形式或者主流，

129
00:21:43.700 --> 00:21:49.310
yunjie.fan@ntu.edu.sg: 您觉得他在于这个启发性的严谨性，上面也给到您一些比较大的困扰吗？

130
00:21:49.630 --> 00:22:14.510
Chin Ee Moon: 目前是没这么觉得因为主要的内容跟我就是我产出的内容，我是不会依赖那个ai给的答案来写。通常我都会把所有的我想要加加入的内容都写好了之后，我只是拿它来做一个修饰，所以我主要还是看他的那些啊，或者是他，他他是怎么怎么让我的整个的

131
00:22:14.510 --> 00:22:16.980
Chin Ee Moon: 的storytelling更好。

132
00:22:16.980 --> 00:22:26.400
Chin Ee Moon: 这样的一个一个功能，对，就是内容来说，我还是会比较严谨一些，就是自己自己的想法，跟自己的产出比较多。

133
00:22:27.340 --> 00:22:32.400
yunjie.fan@ntu.edu.sg: 那有没有说您比较担心的问题。比方说，

134
00:22:32.400 --> 00:22:51.360
yunjie.fan@ntu.edu.sg: 不论是合规性啊，一些伦理合规性，包括是它本身的一些数据安全性。还有像您刚刚提到的一些假文献，他可能自己去去杜撰出来的一些一些内容和文献装入是是是一些比较比较比较有用，可靠的引用

135
00:22:51.360 --> 00:22:58.540
yunjie.fan@ntu.edu.sg: 这些问题，你觉得哪？如果说对于你来你的这个领域的风险，你觉得哪些是比较比较严重的。

136
00:22:59.370 --> 00:23:02.800
Chin Ee Moon: 风险。我觉得假假文献的风险最大吧

137
00:23:02.880 --> 00:23:21.120
Chin Ee Moon: 就是如果有些人他们不去了，之后再拿来用的话，那那就一团糟了，对那那写出来的所有东西都是没有依据的，然后然后也就不这么可靠，然后可用性也不大了。

138
00:23:21.310 --> 00:23:25.090
Chin Ee Moon: 对，我觉得假文献的这一方面会比较严重。

139
00:23:25.660 --> 00:23:44.650
yunjie.fan@ntu.edu.sg: 因为我们之前在采访一些，比方说去做这个人力资源相关的一些，一些这个候选人的时候，他们可能会面临到这个ai的这个伦理性，他们可能在一些大数据的整理过程当中已经被一些bias所洗脑，比方说。

140
00:23:44.650 --> 00:23:45.570
Chin Ee Moon: 女性嘛。

141
00:23:45.570 --> 00:24:05.310
yunjie.fan@ntu.edu.sg: 这个求职方向啊。比方说，女性在某一些岗位上的她的最佳年龄啊等等的。对于您的这个医学领域来讲的话，是否也会有这样的一些歧视，或者说是一些一些不好的偏好，导致您本身会带来一定的困扰，或者带来一定的漏洞。

142
00:24:08.030 --> 00:24:14.670
Chin Ee Moon: 暂时来说，我是没有给他特别那种subjective的问题就是

143
00:24:14.670 --> 00:24:25.420
Chin Ee Moon: 会给出这种这种的答案，我主要还是依靠它来寻找fact比较多，所以那个fact来说的话我就能够直接的就是参考别的，

144
00:24:25.420 --> 00:24:35.250
Chin Ee Moon: 别别别的那个网站，然后我就知道了他是对还是错这样。所以关于这一类的问题，我还是没没怎么没怎么接触过。

145
00:24:35.750 --> 00:24:51.590
yunjie.fan@ntu.edu.sg: 所以您之前也总结出来了，您大概对于ai给您带来的这种风险困扰。所以你觉得ai系统你觉得最大的。对于你来讲，它的这个改进点在哪里？是支持这种医学类的研究，对。

146
00:24:52.880 --> 00:24:54.520
Chin Ee Moon: 哪里的thing？

147
00:24:54.730 --> 00:24:58.020
Chin Ee Moon: Ok

148
00:24:59.630 --> 00:25:04.770
Chin Ee Moon: 我觉得除了我刚刚说的那个就是他的，是要对

149
00:25:04.860 --> 00:25:23.810
Chin Ee Moon: 以外，还有就是可能他在一些还是一些上面的进步，我觉得也是可以有的就是有时候我会让他一些比较简单的出来就是增强我的那个理解。但是呢，他

150
00:25:23.810 --> 00:25:26.140
Chin Ee Moon: 通常january出来的都

151
00:25:26.170 --> 00:25:43.260
Chin Ee Moon: 不太像是我需要找的东西。对，就是它可能会复杂化，就是可能，我要的东西就是一个X1个，然后怎么样，他们的那个那个关系是怎么样，但是它就整理出了一堆非常复杂的东西，然后那个能够那个可用性就变低了。

152
00:25:43.770 --> 00:25:50.790
yunjie.fan@ntu.edu.sg: 明白，对，我觉得您觉得在这个ai的这个版本迭代的过程当中，你有没有觉得说？

153
00:25:51.330 --> 00:26:05.330
yunjie.fan@ntu.edu.sg: 其实伴随着版本迭代，他的思考可能越来越复杂，或者说是越来越可能没有像3.5或者说四的那个阶段，他给出的那个答案好像更加直接，您会不会有这种感觉呢？

154
00:26:05.860 --> 00:26:15.180
Chin Ee Moon: 我，我的确是有觉得跟著他的这个的进步，他给出的答案也的确是有一些改进，也的确是有一些进步的。

155
00:26:15.740 --> 00:26:32.710
yunjie.fan@ntu.edu.sg: 明白，那你有没有比方说，进一步的，我就是在做这个研究的时候，你要给我相关的reference dui一些department的id啊，或者一些数据库的解锁日期，这样去帮助您去更好的，有直接应用型的这种结果。

156
00:26:32.710 --> 00:26:44.920
Chin Ee Moon: 有的，有的就是他。很多时候他给的那些啊，他给的那些所有的那些资料其实都是对的，只是和我问的问题没有太大的关系。

157
00:26:44.920 --> 00:26:45.810
yunjie.fan@ntu.edu.sg: 了解没有。

158
00:26:45.810 --> 00:26:46.410
Chin Ee Moon: 对对。

159
00:26:48.140 --> 00:26:53.770
yunjie.fan@ntu.edu.sg: 好的。所以整个在您的这个使用过程当中，如果说

160
00:26:53.780 --> 00:27:18.750
yunjie.fan@ntu.edu.sg: 面临未来ai可视化的一个问题，因为现在只是一个对话的一个形式，那对话，一个形式。其实最简单的一种人机交互的一个形式，或者说是最直观的人机交互形式。如果说将来你觉得比较好的一种情况。就在这种realization的方向，你觉得比较好的一种一种协调，是更好的，是一种什么样的形式呢？如果说除了对话，或者说你认为对话，也可以去进步一下，

161
00:27:18.750 --> 00:27:19.450
yunjie.fan@ntu.edu.sg: 谢谢。

162
00:27:20.210 --> 00:27:22.240
Chin Ee Moon: 对话，

163
00:27:29.150 --> 00:27:34.050
Chin Ee Moon: 我暂时觉得对话应该是最

164
00:27:34.080 --> 00:27:46.420
Chin Ee Moon: 最有效的那个，那个方式就是它能够通过退化来进行更多的那个，那个改进和调整，可能我觉得有一点可能需要比较的就是

165
00:27:46.420 --> 00:28:09.250
Chin Ee Moon: 如果能进步的话，那就是你在问他问题的时候，他就很能能很精准的就是到你，你，你想要问的到底是什么东西，因为有时候你问了一个问题，然后他就给了你很的答案。然后你要再问多几回，他才会真的是能够get到。你原来想问的是这个啊，这样子就是可能这个是

166
00:28:09.250 --> 00:28:14.240
Chin Ee Moon: 他的sensitivity，如果能再进步一些的话，我觉得是会更好的。

167
00:28:14.780 --> 00:28:17.450
Chin Ee Moon: 对问题的敏感度对。

168
00:28:18.190 --> 00:28:19.220
yunjie.fan@ntu.edu.sg: 好的。

169
00:28:19.440 --> 00:28:29.140
yunjie.fan@ntu.edu.sg: 那本身在医学当中的话，其实它是有比较大的严谨，在我理解它是有非常大的严谨性的，

170
00:28:29.140 --> 00:28:43.710
yunjie.fan@ntu.edu.sg: 那你觉得就是因为我在采访很多的这个，这个受访人当中，其实我觉得像您本身认为百50%，它的可信性，这个其实是在大多数的受访者当中，它是偏低的，您觉得。

171
00:28:44.080 --> 00:28:46.140
yunjie.fan@ntu.edu.sg: 学科是否有关呢？

172
00:28:46.140 --> 00:28:47.050
Chin Ee Moon: 跟什么。

173
00:28:47.050 --> 00:28:49.320
yunjie.fan@ntu.edu.sg: 您的学科，您的领域。

174
00:28:49.780 --> 00:29:08.440
Chin Ee Moon: 我，我觉得是吧，因为我们需要非常精准的一些答案跟观点就是如果有稍微一些不严谨的都会被被被问这样，所以我们需要一个很准确的答案，但是ai很多时候还是会摇摆或者是它的答案还是没有说到重点

175
00:29:08.570 --> 00:29:13.810
Chin Ee Moon: 这样，所以如果是参考内容的话，我会比较少用ai。

176
00:29:14.790 --> 00:29:21.490
yunjie.fan@ntu.edu.sg: 对您更好的把ai定义成它是您的assistant还是他的，还是你的collaborator？

177
00:29:22.290 --> 00:29:25.220
Chin Ee Moon: Assistant，现阶段还是assistant。

178
00:29:25.220 --> 00:29:26.520
yunjie.fan@ntu.edu.sg: 好的好的。

179
00:29:26.650 --> 00:29:30.720
yunjie.fan@ntu.edu.sg: 那您更大情况上希望他扮演什么样的角色。

180
00:29:33.050 --> 00:29:39.510
Chin Ee Moon: 更大情况我觉得能进步，能当个collaborator，其实也还是

181
00:29:39.510 --> 00:29:53.180
Chin Ee Moon: 一个不错的方向，就是如果他能给他给出的答案就是有一定的那个思考性跟一定的那个参考程度的话，那肯定对于我们来说也会是很大的帮助。

182
00:29:53.230 --> 00:29:59.120
Chin Ee Moon: 只是现现阶段我觉得他还不太具备这种相关的能力。

183
00:30:00.260 --> 00:30:18.000
yunjie.fan@ntu.edu.sg: 那如果我们回到科研的这个层面上来说，其实大部分的目前的这个researcher，或者说是去做相关学术相关领域的这些工作人员还是会比较大量或者重量使用ai的一部分人群。但是

184
00:30:18.000 --> 00:30:26.890
yunjie.fan@ntu.edu.sg: 你如何看待这个ai协作跟学术不端就是这两个事情，它之间的这个边界呢？

185
00:30:31.380 --> 00:30:36.720
Chin Ee Moon: 就是说滥用了ai，然后导致导致学术上的一些退步，这样。

186
00:30:36.910 --> 00:30:39.630
yunjie.fan@ntu.edu.sg: 就是like academic misconduct。

187
00:30:40.010 --> 00:30:41.350
Chin Ee Moon: 喔喔

188
00:30:46.370 --> 00:30:48.080
Chin Ee Moon: 就是

189
00:30:48.190 --> 00:31:13.140
Chin Ee Moon: 所以我觉得还是就是要谨慎的使用那个ai，就是你要知道在什么情况下，那个ai是可以帮助你的，但是在什么情况下还是最好不要用ai会比较好，或者是现在就很像我们的那个公司，他也他们也出了一些比较比较严谨的一些policy，关于我们使用ai啊然后如果我们要发p

190
00:31:13.140 --> 00:31:28.870
Chin Ee Moon: paper要交paper的话，我们需要通过那个层层的那个ai的，就是把我们的paper交上去，然后一定要过了那些关之后，我们才可以交到那个的手上，就是他们也有在做这样的工作就是避免我们有这个。

191
00:31:28.870 --> 00:31:31.160
Chin Ee Moon: misconduct的这个风险吧。

192
00:31:31.500 --> 00:31:42.100
yunjie.fan@ntu.edu.sg: 那它这一方面是misconda，另一方面，您觉得有多大程度上是ai会影响医学研究的这种批判性的思维。

193
00:31:47.160 --> 00:31:49.750
Chin Ee Moon: 就是哪一种程度上，它会影响。

194
00:31:49.750 --> 00:31:53.430
yunjie.fan@ntu.edu.sg: 对，或者说是您认为多大程度上它会影响。

195
00:31:54.930 --> 00:31:57.460
Chin Ee Moon: 多大程度上它会影响，

196
00:32:00.010 --> 00:32:04.180
Chin Ee Moon: 我感觉就是因为每一个研究它都有一个

197
00:32:04.180 --> 00:32:21.530
Chin Ee Moon: 就是它主要的那个。如果你把你主要的那个，上面大部分的绝大部分的内容都依靠在ai给你的建议和那些指导上，可能那它就会非常的严重的影响这一个

198
00:32:21.650 --> 00:32:35.150
Chin Ee Moon: 这个研究的那个角度，但是如果你只是把它拿它当成一个辅助辅助的工具帮助你的，只是那些旁的一些工作的话，其实我觉得是还好的，

199
00:32:35.320 --> 00:32:36.240
Chin Ee Moon: 对。

200
00:32:36.470 --> 00:32:39.520
yunjie.fan@ntu.edu.sg: 明白，好的，那从

201
00:32:39.660 --> 00:32:48.470
yunjie.fan@ntu.edu.sg: 这里来看我，基本上我的问题就差不多了，你有没有什么要补充的，或者说是对于我们的这个课题有相关的疑问。

202
00:32:51.900 --> 00:33:06.170
Chin Ee Moon: 还还好还好吧，就是你们现在是你们的这个研究，你们是想要就是做一个新的一个网站，是像chat这样的吗？还是你们就只是要

203
00:33:06.480 --> 00:33:12.230
Chin Ee Moon: 就是粉饰这个的对我们人的这个影响。

204
00:33:12.800 --> 00:33:35.960
yunjie.fan@ntu.edu.sg: 一方面是会审视对于我们人的影响。另外一方面呢，很大程度之上，我们也需要去去optimize它整个的这个流程在后续的话，我们的这个研究人员呢？会有这样的一个步骤，就是说把我们可能已经做出来的比较demo的这些这个实验出来的。不论是

205
00:33:35.960 --> 00:33:48.670
yunjie.fan@ntu.edu.sg: ui也好，还是说是针对于直接可以反馈出数据的这种模型也好，请到你们到这个线下的这个实验室里面去体验。如果说这一方面的你，你会觉得愿意来参加吗？对。

206
00:33:48.950 --> 00:33:51.770
Chin Ee Moon: 我觉得我觉得是是愿意的。

207
00:33:52.010 --> 00:33:52.960
yunjie.fan@ntu.edu.sg: 好的，好的。

208
00:33:52.960 --> 00:33:53.390
Chin Ee Moon: 就是。

209
00:33:53.390 --> 00:34:05.350
yunjie.fan@ntu.edu.sg: 那我大概差不多啦，差不多了解了，然后在此的话还是有一个表格，因为你是没有填的，我们我需要您在场，然后开着摄像头把它填完。您是local吗？

210
00:34:05.560 --> 00:34:09.120
Chin Ee Moon: 哪哪哪一哪一个我没填，可以。

211
00:34:09.120 --> 00:34:12.400
yunjie.fan@ntu.edu.sg: 有一个我们的concent flow。

212
00:34:13.130 --> 00:34:14.260
Chin Ee Moon: 哎，我填了。

213
00:34:14.429 --> 00:34:15.419
yunjie.fan@ntu.edu.sg: 有甜吗？

214
00:34:15.690 --> 00:34:19.120
Chin Ee Moon: 我已经发回去了那个consent form。

215
00:34:20.230 --> 00:34:22.739
Chin Ee Moon: 现在挺早，对。

216
00:34:23.030 --> 00:34:43.320
yunjie.fan@ntu.edu.sg: Okok，好的，那我知道了，那就谢谢您今天的时间，然后后边的这个相关的feedback，我们会去应用到实验当中，所以提前跟您打招呼。然后如果说后续需要任何的协助，或者说是实验上的协助或者进一步采访的协助，我都会通过邮件去通知您的好吗？

217
00:34:43.320 --> 00:34:44.710
Chin Ee Moon: 好好好谢谢。

218
00:34:44.719 --> 00:34:47.499
yunjie.fan@ntu.edu.sg: 谢谢您时间再一次感谢拜拜。

219
00:34:47.500 --> 00:34:48.449
Chin Ee Moon: 掰掰。